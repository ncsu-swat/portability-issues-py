% Ensure xcolor loads with table option before the class loads it.
\PassOptionsToPackage{table}{xcolor}
\documentclass[sigconf,review,anonymous]{acmart}

\usepackage{tabularx}   % flexible X column
\usepackage{booktabs}   % rules
\usepackage{array}      % for m{} column type

\input{macros}

\begin{document}

\fancyfoot{}

\title{\PaperTitle}

\begin{abstract}
Python is widely regarded as cross-platform, yet our cross-OS re-execution of real project tests reveals frequent portability gaps. We study \NumProjects{} repositories (currently an estimated 500 analyzed), execute 440{,}728 tests across operating systems, and observe 9{,}508 tests with divergent outcomes; 2{,}421 differences have been analyzed so far. We identify 11 actionable root-cause categories (e.g., unavailable \texttt{os.*} methods on Windows, terminal/GUI capability gaps in CI, path/line-ending mismatches, dynamic loading issues) and curate concrete repair patterns (guards and fallbacks, cross-platform APIs, normalization). These results provide early empirical evidence of the prevalence, causes, and fix patterns of Python portability issues and establish baselines for future tools and guidelines.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011074.10011099</concept_id>
       <concept_desc>Software and its engineering~Software verification and validation</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10011007.10010940.10010992.10010993.10010994</concept_id>
       <concept_desc>Software and its engineering~Functionality</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software verification and validation}
\ccsdesc[500]{Software and its engineering~Functionality}

\keywords{Mining Software Repositories, Python, Cross-Platform, Portability, Test Re-execution, Repair Patterns}

\maketitle

\section{Introduction}
\label{sec:intro}
Python is widely regarded as cross-platform, powering applications
from diverse domains (e.g., data science pipelines, web applications,
etc.) and consistently ranks among the most popular programming
languages of
today~\cite{stackoverflow2025,githuboctoverse2024,pypl2025}. Yet,
platform portability is not a guarantee. For example, Mince \etal{}
found that 40\% of ML library functionality does not perform
consistently across different hardware (\eg{}, CPU and
TPU)~\cite{mince2023}. Similar problem exists at the OS level.
%% While
%% Python's design philosophy emphasizes portability through standardized
%% interfaces in modules like \texttt{os}~\cite{pymotwos,geeksos2025},
%% real-world codebases routinely contain platform-specific assumptions
%% that render them fragile across operating systems.
For example, the Python function \texttt{os.geteuid()} is unavailable
on Python distributions for Windows. Overall, non-portable code
results in failed deployments and disrupts develpment
productivity~\cite{mince2023,kiuwancodeportability,peng2024,job2024}. Container
technology (e.g., Docker and Singularity) can mitigate this problem
but we find that most open source projects are not containerized,
i.e., they lack Dockerfile or Singularity recipe to create
images. Only 19.9\% of \NumProjects{} \github projects in Python we
analyze contain Dockerfile or Docker compose
configurations.\Mar{Ali/Denini, please check/revise!}

%% (e.g., broken
%% continuous integration, increased maintenance overhead, and broader
%% implications like 
%% These failures manifest in
%% predictable ways: missing system calls on Windows
%% (e.g., \texttt{os.geteuid()}) \cite{pythonos315}, GUI-dependent code
%% failing in headless environments, filesystem path incompatibilities,
%% and dynamic library loading issues that vary by
%% platform \Fix{Cite}. What appears to be a language-level guarantee of
%% portability dissolves under the weight of actual usage patterns and
%% environmental dependencies~\cite{peng2024,job2024}.


%%% This is way too long (distracting) to add in an introduction.
%%%
%% Our analysis of \NumProjects{} Python projects reveals significant insights regarding containerization adoption in the open-source ecosystem. Among these projects, only 345 utilize Docker through either Dockerfiles or Docker Compose configurations, representing approximately 16.9\% of the total dataset. This relatively low adoption rate highlights a critical concern: the vast majority of Python projects (83.1\%) remain potentially vulnerable to portability issues. Projects without containerization face inherent challenges in maintaining consistent execution environments across different systems, dependency management complications, and deployment reproducibility problems. The absence of Docker containerization means that these projects rely heavily on local environment configurations, Python version compatibility, and system-specific dependencies, which can lead to the infamous ``it works on my machine'' syndrome. This portability gap becomes particularly problematic when projects need to be deployed across different operating systems, cloud platforms, or when team members work with varying development setups, ultimately hindering collaboration and increasing the likelihood of environment-related bugs and deployment failures.

%% This fragility is not merely a theoretical concern. Modern software
%% development increasingly relies on cross-platform deployment, from
%% containerized applications that must run consistently across
%% different host systems to open-source projects that serve diverse
%% user communities. When portability breaks, it creates cascading
%% problems: delayed releases, increased maintenance burden, and
%% reduced software accessibility.

This paper reports on a study about portability issues in Python
tests\Mar{is it tests or code? considering the text above...why
tests?}. More precisely, we analyze prevalence, root causes of
failures, their fixes, and the reaction of developers toward the pull
requests we opened to fix portability issues in their code. We
conclude with a set of lessons we learn, including \Mar{try to
integrate with next paragraph...}

Our analysis reveals that these portability failures, while widespread, are not chaotic or unpredictable. Instead, they cluster around a small set of well-defined categories—path handling, character encoding, platform-specific APIs, and environment assumptions. This systematic nature suggests that targeted solutions are possible, transforming what appears to be an intractable problem into a manageable engineering challenge.

This paper contributes an empirical study that (i) re-executes tests across operating systems for a large set of open-source projects, (ii) categorizes the root causes behind cross-OS differences, and (iii) derives concrete repair patterns.

\paragraph{Contributions.}
\begin{itemize}
  \item Cross-OS re-execution of Python tests across a large sample (\NumProjects{} repositories, with results reported so far from an estimated 500).
  \item A taxonomy of 11 actionable root-cause categories (Table~\ref{tab:categories}) with ready-to-apply repair patterns.
\end{itemize}


\label{sec:sources}
We define an \emph{OS portability issue} as a difference in test outcome across operating systems attributable to platform-specific APIs, environment/terminal capabilities, filesystem semantics, encodings, or native libraries. 

\section{Methodology}
\label{sec:methodology}
\textbf{Dataset.} We sampled \NumProjects{} Python repositories; the analyses reported here reflect the first $\sim$500 analyzed.  



\begin{table}[H]

\centering
\caption{Stats on \NumProjects{} evaluated projects: no. of tests (\#Tests), size (SLOC), no. of commits (\#Sha), age in years (Age), no. of GitHub stars ($\star$).\label{tab:project-stats}}
\begin{tabular}{lrrrrr}
\hline
Statistic & \#Tests & SLOC & \#Sha & Age & $\star$ \\
\hline
Mean & 583.94 & 42,467.26 & 2,883.25 & 6.24 & 6,147.88 \\
Median & 72 & 7,478 & 426.50 & 5.87 & 83.50 \\
Min & 1 & 2 & 2 & 0.3 & 2 \\
Max & 36,588 & 3,237,895 & 194,153 & 17.27 & 366,472 \\
Sum & 1,169,056 & 85,019,457 & - & - & - \\
\hline
\end{tabular}
\end{table}


Table~\ref{tab:project-stats} summarizes key characteristics of the \NumProjects{} evaluated projects. 
On average, each project comprises approximately 584 tests and 42K lines of Python code (SLOC), with a median of 72 tests and 7.5K SLOC, indicating a distribution skewed by a few very large systems. Regarding repository activity, projects have on average 2.9K commits, a median of 427, and span a mean lifetime of 6.2 years. The number of GitHub stars varies widely, with an average of 6.1K, a median of only 84, and a maximum exceeding 366K, reflecting the presence of both niche and highly popular projects. 

Overall, the dataset covers a diverse range of software systems, from small-scale repositories with only 
a handful of commits to large, long-lived, and widely adopted projects.




\textbf{Cross-OS re-execution.} For each project we run the test suite on multiple operating systems and record divergent outcomes. We then triage failures via logs and small code inspections, mapping each instance to a concrete category.  

\textbf{Mining portability-related issues.} To complement test-based findings, we also mine GitHub issues and pull requests. We built a “candidate finder” combining multiple keyword axes:  
(a) OS/platform indicators (e.g., Windows, Linux, macOS, specific distros/architectures),  
(b) failure/fix language (e.g., fails, error, bug, fix, workaround),  
(c) testing/CI context (pytest, CI, GitHub Actions), and  
(d) common portability causes (e.g., path separators, chmod/permissions, encodings/UTF-8, dynamic libraries like \texttt{.dll}/\texttt{.so}).  

We applied AND across axes and OR within each axis, so matches typically required an OS reference plus a failure/fix cue, optionally reinforced by test or cause terms. We searched titles, bodies, and comments, with extra weight to title and close-proximity matches. Light triage (brief summaries and negative filters for off-topic mentions) kept the set precise. Candidate issues were normalized into consistent records (project, link, date, summary, compact tags like OS=, FIX=, TEST=, CAUSE=), duplicates removed, and full text (title, description, comments) archived for analysis.  

\section{Evaluation}
\label{sec:evaluation}
% This section presents the empirical results of our study,
% organized around the research questions (RQ1–RQ4.2).
% For each RQ, we restate the rationale, present the findings,
% and provide a concise quick answer.

\subsection{RQ1 — Prevalence}
\label{rq1}
\textbf{Rationale.} With this question, we aim to measure how often portability problems actually occur in practice. Although Python is advertised as a cross-platform language, our preliminary results suggest that many tests fail when run on different operating systems. By quantifying the frequency of such issues across a representative set of projects and applying statistical tests, we can provide stronger evidence on whether portability is a widespread concern in the Python ecosystem or only affects a small subset of projects.  

% add here: statistics, tables, and plots showing the frequency and distribution of portability issues
% for example: Table 1 (Projects analyzed, tests executed, failures observed), Figure 1 (Distribution of failures by OS)

\textbf{Research Question.} How prevalent are portability issues in Python tests when executed across different operating systems?  

\textbf{Snapshot.}
\begin{table}[h]
\centering
\caption{Dataset snapshot (in progress).}
\label{tab:dataset}
\begin{tabular}{l r}
\toprule
Projects analyzed (of \NumProjects{}) & $\sim$500 \\
Total tests executed & 440{,}728 \\
Tests with cross-OS differences & 9{,}508 \\
Differences analyzed/categorized & 2{,}421 \\
\bottomrule
\end{tabular}
\end{table}

\noindent\textbf{Quick Answer.} Portability issues are widespread across Python projects, affecting a significant fraction of test suites.

\subsection{RQ2 — Causes, Symptoms, and Fixes}
\subsubsection{RQ2.1 — Causes}
\label{rq2-1}
\textbf{Research Question.} What are the typical causes of portability problems?  
\textbf{Rationale.} This question seeks to identify and categorize the underlying reasons why portability issues arise. Our observations indicate that differences in modules such as os, file system conventions, path handling, and environment-specific assumptions may contribute to failures. In addition, some dependencies work properly on one operating system but not on another, further aggravating the problem. Understanding these root causes is essential not only to characterize the problem but also to guide the development of best practices and tools to prevent such issues in the future.   

\textbf{Findings.} Table~\ref{tab:categories} reports root-cause categories with sub-categories (bulleted). Numbers are aggregated per category (tests, issues, and totals).  

% ========================= CATEGORIES TABLE =========================
\begin{table*}[t]
    \centering
    \caption{Each row lists a root-cause category with representative sub-categories. Columns report: (i) distinct projects per sub-category observed in mined issues and in test outcomes; and (ii) the overall total (issues + tests).}
    \label{tab:categories}
    \scriptsize
    \setlength{\tabcolsep}{3pt}
    \renewcommand{\arraystretch}{1.15}
    \begin{tabularx}{\textwidth}{
    >{\raggedright\arraybackslash}p{0.25\textwidth}
    >{\raggedright\arraybackslash}X
    >{\centering\arraybackslash}m{0.12\textwidth}
    >{\centering\arraybackslash}m{0.12\textwidth}
    >{\centering\arraybackslash}m{0.08\textwidth}
}
    \toprule
    \textbf{Root-cause category} & \textbf{Sub-categories} & \textbf{Sub-category projects (tests)} & \textbf{Sub-category projects (issues)} & \textbf{Total projects} \\
    \midrule
    
    \multirow[t]{3}{*}{File and Directories} 
    & Path name convention & 27 & 7 & \multirow{3}{*}{41} \\
    & Line ending          & 4  & 1 &                      \\
    & Case sensitivity     & 1  & 1 &                      \\
    \midrule
    
    \multirow[t]{3}{*}{Process execution / signal mismatch}
    & Command execution mismatch (os.system / subprocess / popen) & 14 & 5 & \multirow{3}{*}{25} \\
    & Signal handling (e.g., SIGKILL, SIGHUP)                     & 2  & 3 &                      \\
    & Address already in use                                      & 0  & 1 &                      \\
    \midrule
    
    \multirow[t]{3}{*}{Library availability / dynamic loading mismatch}
    & lib fcntl dont works in windows  & 1  & 2 & \multirow{3}{*}{21} \\
    & Missing a lib                    & 11 & 2 &                      \\
    & Dynamic library loading mismatch & 5  & 0 &                      \\
    \midrule
    
    \multirow[t]{4}{*}{Environment / terminal capability mismatch}
    & no display avaliable in github ci for linux & 2 & 3 & \multirow{4}{*}{14} \\
    & GUI / window-manager differences            & 4 & 0 &                      \\
    & Curses maybe not avaliable Win              & 0 & 1 &                      \\
    & Terminal capability mismatch                & 2 & 2 &                      \\
    \midrule
    
    Opened file locking & - & 4 & 5 & 9 \\
    \midrule
    
    Encoding mismatch & - & 7 & 2 & 9 \\
    \midrule
    
    \multirow[t]{6}{*}{Method or module unavailable}
    & method does not exist: os.uname()   & 1 & 2 & \multirow{6}{*}{8} \\
    & method does not exist: os.geteuid() & 0 & 1 &                      \\
    & method does not exist: os.getuid()  & 0 & 1 &                      \\
    & Method does not exist: os.getpgid() & 1 & 0 &                      \\
    & module does not exist: readline     & 1 & 0 &                      \\
    & module does not exist: resource     & 1 & 0 &                      \\
    \midrule
    
    \multirow[t]{3}{*}{System permission / limits mismatch}
    & Permission mismatch           & 1 & 3 & \multirow{3}{*}{7} \\
    & File descriptor limits        & 1 & 0 &                     \\
    & Symlink privilege restrictions& 1 & 1 &                     \\
    \midrule
    
    Binary wheel architecture mismatch & - & 3 & 0 & 3 \\
    \midrule
    
    \multirow[t]{2}{*}{System info source mismatch}
    & /proc not available on macOS         & 0 & 1 & \multirow{2}{*}{2} \\
    & Timezone database missing on Windows & 0 & 1 &                      \\
    \midrule
    
    File system block size mismatch & - & 1 & 0 & 1 \\
    \midrule
    \textbf{Total} & - & \textbf{95} & \textbf{45} & \textbf{140} \\
    \bottomrule
    \end{tabularx}
\end{table*}

% ====================================================================

\textbf{Concrete example (PR fix).} \Fix{We will insert a specific PR and code diff illustrating the applied fix.}


\noindent\textbf{Symptom signatures.} For subcategories with concrete signals, we list verbatim exceptions or OS-specific error texts that reliably identify the underlying cause in test/CI logs. Not every subcategory has a unique signature; we include only those with specific, reproducible messages (e.g., \texttt{AttributeError: module 'os' has no attribute 'uname'}, \texttt{UnicodeDecodeError: 'utf-8' codec can't decode byte 0x..}). These signatures are meant to speed triage and help developers and tools quickly recognize portability issues; they are not exhaustive of all possible manifestations (see Table~\ref{tab:symptoms}).  

Across projects, we find that portability issues are often resolved through a small set of recurring strategies. Figure~\ref{fig:patterns} illustrates representative examples of these fixes, drawn from real-world projects.

Specifically, we observe four general patterns. (i)  \textbf{Defensive checks}, which add guards, fallbacks, \texttt{try/except}, or conditional imports to avoid crashes when APIs or modules are missing (see Listing~\ref{lst:defensive}); 
(ii) \textbf{Portable APIs}, which replace OS-specific constructs with cross-platform abstractions (e.g., \texttt{pathlib}, \texttt{platform}, \texttt{psutil}) and safer subprocess usage (Listing~\ref{lst:portable});
(iii) \textbf{Normalization} which standardizes encodings, line endings, paths, or filenames before comparison to prevent spurious differences (Listing~\ref{lst:normalization}); 
and \textbf{Environment handling}, which adapts code and tests to runtime conditions (e.g., skipping tests in restricted environments, handling permissions/dependencies, or using containers/virtual environments) is illustrated in Listing~\ref{lst:environment}. These patterns are simple, teachable, and cover the majority of fixes we observed.

\begin{figure*}[!ht]
\centering

% Defensive Checks
\begin{subfigure}{0.48\textwidth}
\begin{lstlisting}[caption={Defensive check for optional library}, label={lst:defensive}]
- import readline
+ try:
+     import readline
+ except ImportError:
+     readline = None  # Fallback if readline is unavailable
\end{lstlisting}
\end{subfigure}
\hfill
% Portable APIs
\begin{subfigure}{0.48\textwidth}
\begin{lstlisting}[caption={Portable APIs: Using pathlib for cross-platform paths}, label={lst:portable}]
- import os
- home_dir = os.path.expanduser('~')
+ from pathlib import Path
+ home_dir = Path.home()  # Cross-platform home directory
\end{lstlisting}
\end{subfigure}

\vspace{0.2cm}

% Normalization
\begin{subfigure}{0.48\textwidth}
\begin{lstlisting}[caption={Normalization: Normalizing file paths}, label={lst:normalization}]
- file_path = "data/file.txt"
+ import os
+ file_path = "data" + os.path.sep + "file.txt"  # Consistent path
\end{lstlisting}
\end{subfigure}
\hfill
% Environment Handling
\begin{subfigure}{0.48\textwidth}
\begin{lstlisting}[caption={Environment handling: Using pytest skip}, label={lst:environment}]
+ import platform
+ @pytest.mark.skipif(platform.system() == "Darwin", reason="Not supported on macOS")
def test_special_case():
    ...
\end{lstlisting}
\end{subfigure}

\caption{Representative fix patterns for portability issues: (a) Defensive checks, (b) Portable APIs, (c) Normalization, and (d) Environment handling.}
\label{fig:patterns}
\end{figure*}


\begin{table*}[t]
    \centering
    \caption{Diagnostic symptom signatures and general fix patterns for portability sub-categories.}
    \label{tab:symptoms}
    \scriptsize
    \setlength{\tabcolsep}{3pt}
    \renewcommand{\arraystretch}{1.15}
    \begin{tabularx}{\textwidth}{
        >{\raggedright\arraybackslash}m{0.20\textwidth}
        >{\raggedright\arraybackslash}m{0.28\textwidth}
        >{\raggedright\arraybackslash}X
        >{\raggedright\arraybackslash}m{0.22\textwidth}
    }
    \toprule
    \textbf{Root-cause category} & \textbf{Sub-categories} & \textbf{Symptom (verbatim error / message)} & \textbf{General Fix Pattern} \\
    \midrule
    File and Directories &
    Path name convention & No specific signature & Normalization — normalize paths with pathlib \\
    & Line ending & \texttt{AssertionError: '\textbackslash r\textbackslash n' != '\textbackslash n'} & Normalization — normalize line endings \\
    & Case sensitivity & \texttt{FileNotFoundError: [Errno 2] No such file or directory: 'Readme.md'} & Normalization — case-normalize filenames \\
    \midrule
    Process execution / signal mismatch &
    Command execution mismatch (os.system / subprocess / popen) & No specific signature & Portable APIs — use subprocess.run shell=False \\
    & Signal handling (e.g., SIGKILL, SIGHUP) & \texttt{AttributeError: module 'signal' has no attribute 'SIGHUP'} & Defensive checks — guard signals per platform \\
    & Address already in use & \texttt{OSError: [Errno 98] Address already in use} (Linux) \; / \; \texttt{OSError: [WinError 10048] Only one usage of each socket address (protocol/network address/port) is normally permitted} (Windows) & Environment handling — retry or bind ephemeral port \\
    \midrule
    Library availability / dynamic loading mismatch &
    lib fcntl dont works in windows & \texttt{ModuleNotFoundError: No module named 'fcntl'} & Defensive checks — conditional import provide fallback \\
    & Missing a lib & \texttt{ModuleNotFoundError: No module named '...'} & Environment handling — conditionally install optional dependency \\
    & Dynamic library loading mismatch & \texttt{OSError: [WinError 126] The specified module could not be found} & Defensive checks — guard loading prefer pure-Python \\
    \midrule
    Environment / terminal capability mismatch &
    no display avaliable in github ci for linux & \texttt{TclError: no display name and no \$DISPLAY environment variable} & Environment handling — skip GUI tests in CI \\
    & GUI / window-manager differences & No specific signature & Environment handling — skip or mock GUI features \\
    & Curses maybe not avaliable Win & \texttt{ModuleNotFoundError: No module named '\_curses'} & Defensive checks — conditional import with stub fallback \\
    & Terminal capability mismatch & No specific signature & Environment handling — detect capabilities degrade gracefully \\
    \midrule
    Opened file locking &
    - & \texttt{PermissionError: [Errno 13] Permission denied: 'filename'} & Environment handling — close files retry on Windows \\
    \midrule
    Encoding mismatch &
    - & \texttt{UnicodeDecodeError: 'utf-8' codec can't decode byte 0x\{XX\} in position N: invalid start byte} & Normalization — set explicit UTF-8 encoding \\
    \midrule
    Method or module unavailable &
    method does not exist: os.uname() &
    \texttt{AttributeError: module 'os' has no attribute 'uname'} & Portable APIs — use platform.uname instead \\
    & method does not exist: os.geteuid() & \texttt{AttributeError: module 'os' has no attribute 'geteuid'} & Defensive checks — guard call provide fallback \\
    & method does not exist: os.getuid() & \texttt{AttributeError: module 'os' has no attribute 'getuid'} & Defensive checks — guard call provide fallback \\
    & Method does not exist: os.getpgid() & \texttt{AttributeError: module 'os' has no attribute 'getpgid'} & Defensive checks — guard call provide fallback \\
    & module does not exist: readline & \texttt{ModuleNotFoundError: No module named 'readline'} & Defensive checks — guard import use alternatives \\
    & module does not exist: resource & \texttt{ModuleNotFoundError: No module named 'resource'} & Defensive checks — guard import use alternatives \\
    \midrule
    System permission / limits mismatch &
    Permission mismatch & \texttt{PermissionError: [Errno 13] Permission denied: '...'} & Environment handling — handle permissions document requirements \\
    & File descriptor limits & \texttt{OSError: [Errno 24] Too many open files} & Environment handling — close files adjust ulimit \\
    & Symlink privilege restrictions & \texttt{OSError: symbolic link privilege not held: 'src' -> 'dst'} & Environment handling — detect support and use copies \\
    \midrule
    Binary wheel architecture mismatch &
    - & No specific signature & Environment handling — install platform-appropriate wheel \\
    \midrule
    System info source mismatch &
    /proc not available on macOS & \texttt{FileNotFoundError: [Errno 2] No such file or directory: '/proc/...'} & Portable APIs — use psutil or platform APIs \\
    & Timezone database missing on Windows & \texttt{zoneinfo.\_common.ZoneInfoNotFoundError: 'No time zone found with key ...'} & Normalization — install tzdata or fallback \\
    \midrule
    File system block size mismatch &
    - & No specific signature & Normalization — normalize computations avoid assumptions \\
    \bottomrule
    \end{tabularx}
    \end{table*}   

\noindent\textbf{OS-specific pathing.}
Differences in how operating systems handle file paths are a common source of portability issues. Code that hardcodes separators (e.g., \texttt{/} in Unix or \texttt{\textbackslash} in Windows), assumes case-insensitive filesystems, or relies on drive letters often behaves inconsistently, leading to broken comparisons, globbing, or even basic file discovery failures. Such problems frequently remain unnoticed because development and testing usually take place in a single environment, where these assumptions appear valid, and because standard utilities such as \texttt{os.path} tend to hide subtle discrepancies. A more reliable practice is to rely on \texttt{pathlib.Path} for path composition and traversal, normalize paths before comparison, and prefer temporary directories or relative paths over embedded absolute ones. When comparing file names across platforms, developers should also explicitly account for case sensitivity.


\noindent\textbf{Unavailable methods.}
Portability problems also arise when platform-specific functions are invoked without safeguards. A typical example is \texttt{os.uname()}, which is widely available on Unix-like systems but not implemented on Windows. If called unguarded, it raises an \texttt{AttributeError}, immediately breaking execution. These errors often escape detection because the code is authored and tested exclusively on Unix-like platforms, without continuous integration that includes Windows. Robust solutions involve feature detection (e.g., checking whether the attribute exists or catching exceptions) and, whenever possible, adopting portable APIs such as \texttt{platform.uname()}, which provide similar information consistently across systems.


\noindent\textbf{Command execution differences.}
Executing external commands is another area where cross-platform discrepancies are pronounced. Shell syntax, quoting rules, environment resolution, and executable extensions differ substantially, so scripts that assume a POSIX shell—such as those relying on pipelines, redirection, or calls via \texttt{sh -c}—typically fail on Windows. Even when commands execute, signal handling and exit-code semantics may diverge. These issues are often masked in developer environments, where required tools are already installed, or in tests that mock subprocess calls. A recommended approach is to use \texttt{subprocess.run} with explicit argument lists and \texttt{shell=False}, thereby avoiding shell-specific parsing. When possible, external calls should be replaced by equivalent Python libraries. If invoking external tools is unavoidable, paths should be resolved with \texttt{shutil.which}, encoding specified explicitly, and OS-specific branches introduced only when strictly necessary.

\subsubsection{RQ2.2 — Fixes}
\label{rq2-2}
\textbf{Research Question.} What are the typical fixes to address portability issues in Python?  
\textbf{Rationale.} This question focuses on how developers address portability issues once identified. Strategies include avoiding direct use of OS-specific APIs by relying on higher-level cross-platform libraries, normalizing filesystem paths with utilities such as pathlib, and handling encoding explicitly. Dependency-related problems are often mitigated by using containerization (e.g., Docker) or virtualization to ensure reproducible environments. Clear documentation of system-specific requirements also plays a key role. By analyzing these practices, we can highlight patterns that effectively reduce portability problems and help inform guidelines for the broader community.  

\textbf{Quick Answer.} Common fixes include relying on cross-platform libraries, normalizing paths, explicitly handling encodings, using containerized or virtualized environments, and documenting system requirements.  


\begin{itemize}
  \item \textbf{Defensive checks:} add guards, fallbacks, \texttt{try/except}, or conditional imports to avoid crashes when APIs or modules are missing.
  \item \textbf{Portable APIs:} prefer cross-platform abstractions such as \texttt{pathlib}, \texttt{platform}, or \texttt{psutil}, and use safer subprocess calls with explicit arguments.
  \item \textbf{Normalization:} standardize encodings, line endings, paths, or filenames before comparison to prevent spurious differences.
  \item \textbf{Environment handling:} adapt code and tests to runtime conditions (e.g., skip GUI in headless CI, manage permissions/dependencies, or use containers/virtual environments).
\end{itemize}


\subsection{RQ3 — Tool Effectiveness}
\label{rq3}
\textbf{Research Question.} How good are existing tools at detecting portability issues in Python projects?  
\textbf{Rationale.} This question investigates to what extent existing tools help identify portability problems. Static tools, including linters, type checkers, CodeQL queries, and even large language models, can flag certain categories of issues, such as OS-dependent APIs or unsafe path handling. However, many portability issues only manifest at runtime, for example due to environment assumptions, platform-specific encodings, or dependency incompatibilities. To complement static checks, we employ a dynamic strategy—our ``rerun'' approach—that executes tests across multiple operating systems to reveal failures that static analysis cannot anticipate. By comparing the coverage of static and dynamic techniques, we can assess their effectiveness and identify opportunities for improving tool support in cross-platform Python development. 

\textbf{Analysis of Static Tools.} To evaluate the effectiveness of static analysis tools in detecting Python portability issues, we systematically examined the rules and capabilities of several widely used linters and type checkers. Specifically, we considered Ruff~\cite{ruffref}, Flake8~\cite{flake8}, Pylint~\cite{pylint}, MyPy~\cite{mypy}, Bandit~\cite{bandit}, and Pyright~\cite{pyright}.

Our analysis revealed that, among these tools, only Ruff provides a rule that directly addresses potential portability concerns, namely the \texttt{unspecified-encoding} check~\cite{encruff}. This rule helps identify cases where files are opened without explicitly specifying an encoding, which may lead to cross-platform inconsistencies. The remaining tools focus primarily on style, type correctness, security, and general code quality, without offering rules specifically designed to detect operating-system-dependent behaviors or other portability issues.

\textbf{Motivation for LLM Evaluation.} Given the limited support for portability checks in existing static tools, we extended our evaluation to large language models (LLMs). We designed an experiment in which LLMs are presented with real Python code snippets exhibiting both portable and non-portable behavior. By comparing the models' predictions against the known classification of each snippet, we can quantitatively assess their ability to detect portability issues. Specifically, we calculate precision and recall for each model, providing an objective measure of performance in identifying cross-platform hazards that traditional static tools largely fail to capture. This approach allows us to explore whether LLMs can serve as a complementary analysis layer for improving cross-platform Python development practices.

% results LLMs:

Table~\ref{tab:llm_performance} presents the quantitative results for the four evaluated LLMs across 51 Python code samples, of which 17 were portable and 34 non-portable. Overall, the models exhibit moderate yet encouraging ability to identify cross-platform issues, suggesting potential for complementary use alongside traditional static analyzers.

\begin{table}[h!]
\centering
\caption{Performance metrics for each evaluated LLM in detecting Python portability issues.}
\label{tab:llm_performance}
\begin{tabular}{lrrrrr}
\toprule
Model & Precision & Recall & F1-Score & Accuracy & Samples \\
\midrule
\texttt{llama-3.3-70b-instruct} & 0.545 & 0.353 & 0.429 & 0.686 & 51 \\
\texttt{grok-4-fast} & 0.548 & 1.000 & 0.708 & 0.725 & 51 \\
\texttt{gpt-4o-mini} & 0.818 & 0.529 & 0.643 & 0.804 & 51 \\
\texttt{deepseek-chat-v3.1} & 0.684 & 0.765 & 0.722 & 0.804 & 51 \\
\bottomrule
\end{tabular}
\end{table}

The \texttt{Llama-3.3-70B-Instruct} model achieved the lowest recall (0.353) and F1-score (0.429), indicating that while it correctly identified some portability violations, it frequently failed to recognize others. This suggests limited sensitivity to OS-specific patterns or file-handling idiosyncrasies. The confusion matrix confirms that it misclassified many truly portable samples as non-portable, evidencing a tendency toward conservative classification.

\texttt{Grok-4-Fast} reached perfect recall (1.000) but comparatively low precision (0.548), meaning it successfully captured all truly portable cases but also misclassified a considerable number of non-portable snippets as portable. This overgeneralization may stem from the model’s heuristic bias toward permissive judgments when uncertain, emphasizing recall at the expense of specificity. Its behavior mirrors an “inclusive” classification approach that prioritizes coverage rather than accuracy.

In contrast, \texttt{GPT-4o-Mini} exhibited the highest precision (0.818) and overall accuracy (0.804), indicating that when it predicted portability, it was usually correct. However, its recall (0.529) reveals that it missed several truly portable instances. This conservative profile suggests that the model prefers to err on the side of caution, labeling code as non-portable unless the evidence of portability is explicit. Such behavior could be advantageous in a practical context, where false negatives (i.e., missed portability) are less problematic than false positives.

The \texttt{DeepSeek-Chat-v3.1} model achieved the best F1-score (0.722), representing the most balanced trade-off between precision (0.684) and recall (0.765). Its performance suggests an ability to generalize across various forms of cross-platform behavior while maintaining a reasonable error rate. Unlike Grok-4-Fast, DeepSeek did not rely on excessive positive classification but instead demonstrated a nuanced understanding of environment-dependent operations.

Comparing these outcomes reveals different inferential ``styles'' among LLMs. Llama tended toward under-detection, Grok favored over-detection, GPT-4o adopted a cautious filtering approach, and DeepSeek found a middle ground. This behavioral diversity highlights that LLMs are not homogeneous analyzers but systems that exhibit distinct operational trade-offs shaped by their alignment, instruction-tuning, and domain exposure.

From a broader perspective, all models demonstrated at least partial awareness of portability-related concepts such as file I/O encodings, OS-specific path separators, and dependency on environment variables. However, they struggled to identify more subtle forms of non-portability, including locale-dependent string handling, subprocess calls, or reliance on unavailable system commands. This limitation underscores that while LLMs possess broad reasoning capabilities, their implicit software knowledge remains probabilistic and incomplete.

Interestingly, the models occasionally provided correct justifications for their decisions—even when the final classification was wrong. For example, some outputs identified potential encoding issues or noted that “file paths might differ across systems” but still concluded with “Portable!!!”. This pattern suggests that LLMs can reason about the relevant dimension but fail to consistently map that reasoning to the correct final label, a challenge that could be addressed through targeted prompt calibration or reinforcement feedback.

In aggregate, these findings indicate that LLMs are capable of performing basic reasoning about cross-platform hazards, achieving results that surpass random guessing by a substantial margin. Although no model reached expert-level reliability, the observed F1-scores between 0.43 and 0.72 demonstrate meaningful signal extraction from the task, validating the experimental premise that LLMs can detect portability-related cues even without explicit static rules.

Finally, the results highlight a promising research avenue: integrating LLMs as auxiliary components within static analysis pipelines. Their probabilistic insights could help prioritize inspection of code segments with higher cross-platform risk, complementing deterministic linters that lack such reasoning capacity. Thus, while current performance remains limited, the overall evidence supports the view that LLM-assisted analysis represents a viable, if early-stage, approach toward improving Python portability assessment.


\textbf{Quick Answer.} \Fix{turn this into a box anwser:}Static tools (linters, CodeQL, LLM prompts) can detect some issues but miss runtime failures. Dynamic re-execution across OSes (our approach) provides broader coverage.


% Add here: quantitative comparison (coverage, precision, recall)
% For example: Figure 3 (Venn diagram of issues detected by static vs dynamic approaches)

\subsection{RQ4. How Do Developers Respond to Reported Portability Issues?}
\label{sec:pr-discussion}
\subsubsection{RQ4.1 — Applied Fixes}
\label{rq4-1}
\textbf{Research Question.} What types of fixes are most commonly applied to resolve portability issues, and what solution patterns can be identified?  
textbf{Rationale.} This question investigates the concrete changes developers make when addressing portability problems. For example, replacing operating-system-specific API calls, adopting cross-platform libraries, or adding conditional code that adapts to the environment. By categorizing and analyzing these changes, we can uncover recurring solution patterns that may be generalized into practical guidelines for writing more portable Python software.  

% Add here: commit/PR analysis, with representative examples of code changes
% For example: Table 4 (Common fix patterns with frequency and examples)


\textbf{Quick Answer.} Fixes often involve replacing OS-specific APIs, adding conditional checks, or adopting cross-platform libraries.  

\subsubsection{RQ4.2 — Community Reaction}
\label{rq4-2}
\textbf{Research Question.} How do open-source developers and project communities respond to reported portability issues (issues, pull requests, or discussions)?  
\textbf{Rationale.} This question examines how the OSS community treats portability fixes by analyzing issue trackers, pull requests, and related discussions. We investigate whether such problems are seen as important, how quickly they are addressed, and the engagement level (comments, reviews, merges) these fixes receive, helping us understand real-world awareness and prioritization of portability.  

% Add here: time-to-fix statistics, issue/PR comment analysis
% For example: Figure 4 (Distribution of time-to-fix), Table 5 (Engagement metrics per project)

To validate our findings and contribute back to the open-source ecosystem, we submitted pull requests addressing the portability issues uncovered in our study. Our contributions focused on libraries where our analysis revealed systematic portability challenges, proposing fixes to improve cross-platform compatibility and robustness.

Table~\ref{tab:prs} summarizes the results of this effort. We submitted a total of \TotalPROpened{} pull requests (PRs) spanning 8 categories of portability issues, of which \TotalPRAccepted{} had been accepted at the time of writing. The overall acceptance rate of 27\%, along with a zero rejection rate, indicates that the issues we identified correspond to genuine portability problems recognized by library maintainers. Notably, ``Method or module unavailable'' issues were the most frequent (11 PRs), reflecting substantial API inconsistencies across platforms. Conversely, ``Process execution / signal mismatch'' issues exhibited the highest acceptance rate (100\%), likely due to their critical impact on application functionality.

\begin{table}[h!]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{2pt}
    \caption{Summary of Pull Requests (PRs) by issue type, showing opened, accepted, and rejected. The last row shows totals.}
    \label{tab:prs}
    % Creating a compact tabular environment
    \begin{tabular}{lccc}
        \toprule
        \textbf{Category} & \textbf{Opened} & \textbf{Accepted} & \textbf{Rejected} \\
        \midrule
        Method or module unavailable & 11 & 0 & 0 \\
        Opened file locking & 4 & 2 & 0 \\
        File/path representation mismatch & 8 & 3 & 0 \\
        Encoding mismatch & 4 & 1 & 0 \\
        Environment / terminal capability mismatch & 1 & 0 & 0 \\
        Process execution / signal mismatch & 2 & 2 & 0 \\
        System info source mismatch & 2 & 1 & 0 \\
        Library availability / dynamic loading mismatch & 1 & 0 & 0 \\
        \midrule
        \textbf{Total} & \textbf{\TotalPROpened} & \textbf{\TotalPRAccepted} & \textbf{\TotalPRRejected} \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Quick Answer.} Developers typically acknowledge portability problems but often give them lower priority than functional bugs, resulting in varied resolution times.  



\Den{Please review.}
To better understand how open-source developers and project communities react when portability issues are reported, we submitted pull requests (PRs) across different projects. The responses we received illustrate a spectrum of engagement patterns, from prompt acceptance to rejection or non-merge due to project inactivity.

% \cite{breds_pr_40}
In one case, the proposed change addressed a test portability issue arising from platform-specific file handling. The original test used Python's \texttt{tempfile.NamedTemporaryFile} without disabling automatic deletion, which led to failures on Windows due to how temporary files are closed and reopened. Our fix stored the path before closing the file and explicitly removed it after the test execution. The maintainer accepted the contribution, commenting: \emph{``Thanks for this.''}, 
in another PR, the maintainer expressed appreciation for the contribution and promptly merged the fix, stating: \emph{``Thanks a lot for this contribution! Good catch.''}
% \cite{python_doit_api_pr_9}. 
These responses highlight a positive and collaborative stance, where developers value external input and promptly integrate fixes that improve cross-platform compatibility.

% \cite{frequenz_pr_1261}
A different outcome emerged in a PR where we attempted to resolve a dependency portability issue. Specifically, the tests relied on the \texttt{zoneinfo} and \texttt{tzdata} modules, which are not consistently available across platforms. Our proposed patch included conditional installation of the dependency when running on Windows. However, the maintainer argued that the better solution was to avoid reliance on the library altogether, regardless of platform. As a result, the PR was reduced to removing the dependency and related tests. This case illustrates that communities sometimes prefer simplifying the codebase rather than maintaining platform-specific workarounds.

Finally, in another project, the response was shaped less by technical considerations and more by the project's state of maintenance. The maintainer acknowledged the validity of the patch but declined to merge it, noting: \emph{``I do somewhat view that project as archived and inactive... My concern about merging is that an updated last commit date would make the project seem more active than it really is.''} This case illustrates how project activity levels and community sustainability strongly influence whether portability improvements are accepted, even if the technical changes are sound.

Overall, these three cases demonstrate that responses to portability contributions vary significantly: active projects may readily adopt fixes, maintainers may prefer design simplifications over platform-specific adjustments, and inactive projects may resist merging altogether to avoid signaling misleading project vitality.


\section{Lessons Learned}
\label{sec:lessons}

\Ali{@Marcelo, please suggest the overall structure for this section.}
\Den{What should developers, researchers, and tool builders keep in mind when thinking about portability issues in Python? Here are some suggestions:}
% - Practical guidelines distilled from study
% - Recommendations for writing portable Python code
% - Recommendations for tool designers and researchers
% - General lessons beyond Python (cross-platform SE insights)

% Not in a philosophical way, but in a practical way, what are the main takeaways from our study? 

% \begin{itemize}
%   \item \textbf{Portability is not automatic.} Even though Python is designed to be cross-platform, real projects still often break when run on different operating systems. Portability depends on how code and environments are actually used, not just what the language promises.
%   \item \textbf{Problems fall into repeatable categories.} The failures we found were not random. They fall into a small set of categories (like paths, encodings, missing APIs). This means we can focus research and tool support on the most common categories instead of treating each bug as unique.
% \end{itemize}



\section{Threats to Validity}
\label{sec:threats}
% - Internal validity: correctness of rerun framework
% - External validity: generalization beyond studied projects
% - Construct validity: definitions of “portability” and categorization
% - Conclusion validity: statistical soundness, possible biases


\section{Related Work}
\label{sec:related}
% - Prior work on cross-platform failures, portability in SE
% - Studies on testing across operating systems
% - Tool support for detecting environment-specific issues
% - Positioning of this work relative to literature

\Fix{Talk about} \cite{vahabzadeh2015empirical} here. Others: \cite{rasley2015detecting, ghandorh2020systematic}

Maybe: \cite{sun2016bear}. Mobile apps testing: \cite{boushehrinejadmoradi2015testing}

\section{Conclusions}
\label{sec:conclusions}
% - Recap of findings for each RQ
% - Key contributions
% - Long-term impact on research and practice
% - Future directions

%% \section*{Data Availability}

%% The artifacts -- including datasets and scripts -- are publicly available~\cite{ourrepo}.

\bibliographystyle{ACM-Reference-Format}
% Allow underscores in BibTeX keys within the .bbl without math mode errors
\begingroup\catcode`\_=12 \relax
\bibliography{ref}
\endgroup

\end{document}
