% Ensure xcolor loads with table option before the class loads it.
\PassOptionsToPackage{table}{xcolor}
\documentclass[sigconf,review,anonymous]{acmart}

\usepackage{tabularx}   % flexible X column
\usepackage{booktabs}   % rules
\usepackage{array}      % for m{} column type

\input{macros}

\begin{document}

\fancyfoot{}

\title{\PaperTitle}

\begin{abstract}
Python is widely regarded as cross-platform, yet our cross-OS re-execution of real project tests reveals frequent portability gaps. We study \NumProjects{} repositories (currently an estimated 500 analyzed), execute 440{,}728 tests across operating systems, and observe 9{,}508 tests with divergent outcomes; 2{,}421 differences have been analyzed so far. We identify 11 actionable categories (e.g., unavailable \texttt{os.*} methods on Windows, terminal/GUI capability gaps in CI, path/line-ending mismatches, dynamic loading issues) and curate concrete repair patterns (guards and fallbacks, cross-platform APIs, normalization). These results provide early empirical evidence of the prevalence, causes, and fix patterns of Python portability issues and establish baselines for future tools and guidelines.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011074.10011099</concept_id>
       <concept_desc>Software and its engineering~Software verification and validation</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10011007.10010940.10010992.10010993.10010994</concept_id>
       <concept_desc>Software and its engineering~Functionality</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software verification and validation}
\ccsdesc[500]{Software and its engineering~Functionality}

\keywords{Mining Software Repositories, Python, Cross-Platform, Portability, Test Re-execution, Repair Patterns}

\maketitle

\section{Introduction}
\label{sec:intro}
Python is widely regarded as cross-platform, powering diverse applications from data science pipelines to web applications across computing environments, and consistently ranking as the most popular programming language in surveys~\cite{stackoverflow2025,githuboctoverse2024,pypl2025}. Yet this compatibility often breaks down in practice when code encounters different operating systems and environments~\cite{mince2023}. The consequences include failed deployments, broken continuous integration, increased maintenance overhead, and broader implications like uneven progress in software development as developers handle platform-specific edge cases~\cite{mince2023,kiuwancodeportability}.

While Python's design philosophy emphasizes portability through standardized interfaces in modules like \texttt{os}~\cite{pymotwos,geeksos2025}, real-world codebases routinely contain platform-specific assumptions that render them fragile across operating systems. These failures manifest in predictable ways: missing system calls on Windows (e.g., \texttt{os.geteuid()}) \cite{pythonos315}, GUI-dependent code failing in headless environments, filesystem path incompatibilities, and dynamic library loading issues that vary by platform \Fix{Cite}. What appears to be a language-level guarantee of portability dissolves under the weight of actual usage patterns and environmental dependencies~\cite{peng2024,job2024}.

Our analysis of \NumProjects{} Python projects reveals significant insights regarding containerization adoption in the open-source ecosystem. Among these projects, only 345 utilize Docker through either Dockerfiles or Docker Compose configurations, representing approximately 16.9\% of the total dataset. This relatively low adoption rate highlights a critical concern: the vast majority of Python projects (83.1\%) remain potentially vulnerable to portability issues. Projects without containerization face inherent challenges in maintaining consistent execution environments across different systems, dependency management complications, and deployment reproducibility problems. The absence of Docker containerization means that these projects rely heavily on local environment configurations, Python version compatibility, and system-specific dependencies, which can lead to the infamous ``it works on my machine'' syndrome. This portability gap becomes particularly problematic when projects need to be deployed across different operating systems, cloud platforms, or when team members work with varying development setups, ultimately hindering collaboration and increasing the likelihood of environment-related bugs and deployment failures.

This fragility is not merely a theoretical concern. Modern software development increasingly relies on cross-platform deployment, from containerized applications that must run consistently across different host systems to open-source projects that serve diverse user communities. When portability breaks, it creates cascading problems: delayed releases, increased maintenance burden, and reduced software accessibility.

Our analysis reveals that these portability failures, while widespread, are not chaotic or unpredictable. Instead, they cluster around a small set of well-defined categories—path handling, character encoding, platform-specific APIs, and environment assumptions. This systematic nature suggests that targeted solutions are possible, transforming what appears to be an intractable problem into a manageable engineering challenge.

This paper contributes an empirical study that (i) re-executes tests across operating systems for a large set of open-source projects, (ii) categorizes the root causes behind cross-OS differences, and (iii) derives concrete repair patterns.

\paragraph{Contributions.}
\begin{itemize}
  \item Cross-OS re-execution of Python tests across a large sample (\NumProjects{} repositories, with results reported so far from an estimated 500).
  \item A taxonomy of 11 actionable portability categories (Table~\ref{tab:categories}) with ready-to-apply repair patterns.
\end{itemize}


\label{sec:sources}
We define an \emph{OS portability issue} as a difference in test outcome across operating systems attributable to platform-specific APIs, environment/terminal capabilities, filesystem semantics, encodings, or native libraries. 

\section{Methodology}
\label{sec:methodology}
\textbf{Dataset.} We sampled \NumProjects{} Python repositories; the analyses reported here reflect the first $\sim$500 analyzed.  



\begin{table}[H]

\centering
\caption{Stats on \NumProjects{} evaluated projects: no. of tests (\#Tests), size (SLOC), no. of commits (\#Sha), age in years (Age), no. of GitHub stars ($\star$).\label{tab:project-stats}}
\begin{tabular}{lrrrrr}
\hline
Statistic & \#Tests & SLOC & \#Sha & Age & $\star$ \\
\hline
Mean & 583.94 & 42,467.26 & 2,883.25 & 6.24 & 6,147.88 \\
Median & 72 & 7,478 & 426.50 & 5.87 & 83.50 \\
Min & 1 & 2 & 2 & 0.3 & 2 \\
Max & 36,588 & 3,237,895 & 194,153 & 17.27 & 366,472 \\
Sum & 1,169,056 & 85,019,457 & - & - & - \\
\hline
\end{tabular}
\end{table}


Table~\ref{tab:project-stats} summarizes key characteristics of the \NumProjects{} evaluated projects. 
On average, each project comprises approximately 584 tests and 42K lines of Python code (SLOC), with a median of 72 tests and 7.5K SLOC, indicating a distribution skewed by a few very large systems. Regarding repository activity, projects have on average 2.9K commits, a median of 427, and span a mean lifetime of 6.2 years. The number of GitHub stars varies widely, with an average of 6.1K, a median of only 84, and a maximum exceeding 366K, reflecting the presence of both niche and highly popular projects. 

Overall, the dataset covers a diverse range of software systems, from small-scale repositories with only 
a handful of commits to large, long-lived, and widely adopted projects.




\textbf{Cross-OS re-execution.} For each project we run the test suite on multiple operating systems and record divergent outcomes. We then triage failures via logs and small code inspections, mapping each instance to a concrete category.  

\textbf{Mining portability-related issues.} To complement test-based findings, we also mine GitHub issues and pull requests. We built a “candidate finder” combining multiple keyword axes:  
(a) OS/platform indicators (e.g., Windows, Linux, macOS, specific distros/architectures),  
(b) failure/fix language (e.g., fails, error, bug, fix, workaround),  
(c) testing/CI context (pytest, CI, GitHub Actions), and  
(d) common portability causes (e.g., path separators, chmod/permissions, encodings/UTF-8, dynamic libraries like \texttt{.dll}/\texttt{.so}).  

We applied AND across axes and OR within each axis, so matches typically required an OS reference plus a failure/fix cue, optionally reinforced by test or cause terms. We searched titles, bodies, and comments, with extra weight to title and close-proximity matches. Light triage (brief summaries and negative filters for off-topic mentions) kept the set precise. Candidate issues were normalized into consistent records (project, link, date, summary, compact tags like OS=, FIX=, TEST=, CAUSE=), duplicates removed, and full text (title, description, comments) archived for analysis.  

\section{Evaluation}
\label{sec:evaluation}
% This section presents the empirical results of our study,
% organized around the research questions (RQ1–RQ4.2).
% For each RQ, we restate the rationale, present the findings,
% and provide a concise quick answer.

\subsection{RQ1 — Prevalence}
\label{rq1}
\textbf{Rationale.} With this question, we aim to measure how often portability problems actually occur in practice. Although Python is advertised as a cross-platform language, our preliminary results suggest that many tests fail when run on different operating systems. By quantifying the frequency of such issues across a representative set of projects and applying statistical tests, we can provide stronger evidence on whether portability is a widespread concern in the Python ecosystem or only affects a small subset of projects.  

% add here: statistics, tables, and plots showing the frequency and distribution of portability issues
% for example: Table 1 (Projects analyzed, tests executed, failures observed), Figure 1 (Distribution of failures by OS)

\textbf{Research Question.} How prevalent are portability issues in Python tests when executed across different operating systems?  

\textbf{Snapshot.}
\begin{table}[h]
\centering
\caption{Dataset snapshot (in progress).}
\label{tab:dataset}
\begin{tabular}{l r}
\toprule
Projects analyzed (of \NumProjects{}) & $\sim$500 \\
Total tests executed & 440{,}728 \\
Tests with cross-OS differences & 9{,}508 \\
Differences analyzed/categorized & 2{,}421 \\
\bottomrule
\end{tabular}
\end{table}

\noindent\textbf{Quick Answer.} Portability issues are widespread across Python projects, affecting a significant fraction of test suites.

\subsection{RQ2 — Causes and Fixes}
\subsubsection{RQ2.1 — Causes}
\label{rq2-1}
\textbf{Research Question.} What are the typical causes of portability problems?  
\textbf{Rationale.} This question seeks to identify and categorize the underlying reasons why portability issues arise. Our observations indicate that differences in modules such as os, file system conventions, path handling, and environment-specific assumptions may contribute to failures. In addition, some dependencies work properly on one operating system but not on another, further aggravating the problem. Understanding these root causes is essential not only to characterize the problem but also to guide the development of best practices and tools to prevent such issues in the future.   

% Add here: categories of causes with illustrative examples (code snippets, error messages)
% For example: Figure 2 (Breakdown of causes), Table 2 (Categories and frequency)

\textbf{Findings.} Table~\ref{tab:categories} reports categories with sub-categories (bulleted). Numbers are aggregated per category (tests, issues, and totals).  

% ========================= CATEGORIES TABLE =========================
\begin{table*}[t]
    \centering
    \caption{Each row lists a portability category with representative sub-categories. Columns report: (i) distinct projects per sub-category observed in mined issues and in test outcomes; (ii) per-category totals of distinct projects from issues and from tests (sums across the sub-categories in that row); and (iii) the overall total (issues + tests).}
    \label{tab:categories}
    \scriptsize
    \setlength{\tabcolsep}{3pt}
    \renewcommand{\arraystretch}{1.15}
    \begin{tabularx}{\textwidth}{
    >{\centering\arraybackslash}m{0.21\textwidth}
    >{\raggedright\arraybackslash}X
    >{\centering\arraybackslash}m{0.12\textwidth}
    >{\centering\arraybackslash}m{0.12\textwidth}
    >{\centering\arraybackslash}m{0.08\textwidth}
    >{\centering\arraybackslash}m{0.08\textwidth}
    >{\centering\arraybackslash}m{0.06\textwidth}
}
    \toprule
    \textbf{Category} & \textbf{Sub-categories (examples)} & \textbf{Sub-category projects (tests)} & \textbf{Category projects (tests)} & \textbf{Sub-category projects (issues)} & \textbf{Category projects (issues)} & \textbf{Total projects} \\
    \midrule
    
    File/path representation mismatch &
    \begin{tabular}[c]{@{}l@{}}
    OS-specific pathing \\
    Line ending mismatch (LF vs.\ CRLF) \\
    Filename case insensitive
    \end{tabular}
    & \begin{tabular}[c]{@{}r@{}}
    27 \\
    5 \\
    0
    \end{tabular}
    & \begin{tabular}[c]{@{}c@{}}32\end{tabular}
    & \begin{tabular}[c]{@{}r@{}}
    7 \\
    1 \\
    1
    \end{tabular}
    & \begin{tabular}[c]{@{}c@{}}9\end{tabular} & \textbf{41} \\
    \hline
    
    Process execution / signal mismatch &
    \begin{tabular}[c]{@{}l@{}}
    Command execution (\texttt{os.system}, \texttt{subprocess}) \\
    Unix-only signals (SIGKILL, SIGHUP) \\
    Address already in use
    \end{tabular}
    & \begin{tabular}[c]{@{}r@{}}
    14 \\
    2 \\
    0
    \end{tabular}
    & \begin{tabular}[c]{@{}c@{}}16\end{tabular}
    & \begin{tabular}[c]{@{}r@{}}
    5 \\
    3 \\
    1
    \end{tabular}
    & \begin{tabular}[c]{@{}c@{}}9\end{tabular} & \textbf{25} \\
    \hline
    
    Library availability / dynamic loading mismatch &
    \begin{tabular}[c]{@{}l@{}}
    lib \texttt{fcntl} not on Windows \\
    Missing native library \\
    Dynamic library loading mismatch
    \end{tabular}
    & \begin{tabular}[c]{@{}r@{}}
    1 \\
    11 \\
    5
    \end{tabular}
    & \begin{tabular}[c]{@{}c@{}}17\end{tabular}
    & \begin{tabular}[c]{@{}r@{}}
    2 \\
    2 \\
    0
    \end{tabular}
    & \begin{tabular}[c]{@{}c@{}}4\end{tabular} & \textbf{21} \\
    \hline
    
    \begin{tabular}[c]{@{}c@{}}Environment / terminal capability mismatch\end{tabular} &
    \begin{tabular}[c]{@{}l@{}}
    No display in CI \\
    GUI/window-manager differences \\
    \texttt{curses} not available on Windows \\
    ANSI/color capability mismatch
    \end{tabular}
    & \begin{tabular}[c]{@{}r@{}}
    2 \\
    4 \\
    0 \\
    2
    \end{tabular}
    & \begin{tabular}[c]{@{}c@{}}8\end{tabular}
    & \begin{tabular}[c]{@{}r@{}}
    3 \\
    0 \\
    1 \\
    2
    \end{tabular}
    & \begin{tabular}[c]{@{}c@{}}6\end{tabular} & \textbf{14} \\
    \hline
    
    Opened file locking &
    \begin{tabular}[c]{@{}l@{}}Advisory vs mandatory file locks\end{tabular}
    & \begin{tabular}[t]{@{}r@{}}4\end{tabular}
    & \begin{tabular}[c]{@{}c@{}}4\end{tabular}
    & \begin{tabular}[t]{@{}r@{}}5\end{tabular}
    & \begin{tabular}[c]{@{}c@{}}5\end{tabular} & \textbf{9} \\
    \hline
    
    Encoding mismatch &
    \begin{tabular}[c]{@{}l@{}}UTF-8 vs cp1252 inconsistencies\end{tabular}
    & \begin{tabular}[t]{@{}r@{}}7\end{tabular}
    & \begin{tabular}[c]{@{}c@{}}7\end{tabular}
    & \begin{tabular}[t]{@{}r@{}}2\end{tabular}
    & \begin{tabular}[c]{@{}c@{}}2\end{tabular} & \textbf{9} \\
    \hline
    
    Method or module unavailable &
    \begin{tabular}[c]{@{}l@{}}
    os.uname() \\
    os.geteuid() \\
    os.getuid() \\
    os.getpgid() \\
    Module: readline \\
    Module: resource
    \end{tabular}
    & \begin{tabular}[c]{@{}r@{}}
    1 \\
    0 \\
    0 \\
    1 \\
    1 \\
    1
    \end{tabular}
    & \begin{tabular}[c]{@{}c@{}}4\end{tabular}
    & \begin{tabular}[c]{@{}r@{}}
    2 \\
    1 \\
    1 \\
    0 \\
    0 \\
    0
    \end{tabular}
    & \begin{tabular}[c]{@{}c@{}}4\end{tabular} & \textbf{8} \\
    \hline
    
    System permission / limits mismatch &
    \begin{tabular}[c]{@{}l@{}}
    Permission mismatch \\
    File descriptor limits \\
    Symlink privilege restrictions
    \end{tabular}
    & \begin{tabular}[c]{@{}r@{}}
    1 \\
    1 \\
    1
    \end{tabular}
    & \begin{tabular}[c]{@{}c@{}}3\end{tabular}
    & \begin{tabular}[c]{@{}r@{}}
    3 \\
    0 \\
    1
    \end{tabular}
    & \begin{tabular}[c]{@{}c@{}}4\end{tabular} & \textbf{7} \\
    \hline
    
    Binary wheel architecture mismatch &
    \begin{tabular}[c]{@{}l@{}}Pre-built wheels missing on some OS/arch\end{tabular}
    & \begin{tabular}[t]{@{}r@{}}3\end{tabular}
    & \begin{tabular}[c]{@{}c@{}}3\end{tabular}
    & \begin{tabular}[t]{@{}r@{}}0\end{tabular}
    & \begin{tabular}[c]{@{}c@{}}0\end{tabular} & \textbf{3} \\
    \hline
    
    System info source mismatch &
    \begin{tabular}[c]{@{}l@{}}
    /proc unavailable on macOS \\
    Timezone database missing on Windows
    \end{tabular}
    & \begin{tabular}[c]{@{}r@{}}
    0 \\
    0
    \end{tabular}
    & \begin{tabular}[c]{@{}c@{}}0\end{tabular}
    & \begin{tabular}[t]{@{}r@{}}
    1 \\
    1
    \end{tabular}
    & \begin{tabular}[c]{@{}c@{}}2\end{tabular} & \textbf{2} \\
    \hline
    
    File system block size mismatch &
    \begin{tabular}[c]{@{}l@{}}Different default block sizes across OSes\end{tabular}
    & \begin{tabular}[t]{@{}r@{}}1\end{tabular}
    & \begin{tabular}[c]{@{}c@{}}1\end{tabular}
    & \begin{tabular}[t]{@{}r@{}}0\end{tabular}
    & \begin{tabular}[c]{@{}c@{}}0\end{tabular} & \textbf{1} \\
    \bottomrule
    \end{tabularx}
    \end{table*}
    
% ====================================================================

\noindent\textbf{Symptom signatures.} For subcategories with concrete signals, we list verbatim exceptions or OS-specific error texts that reliably identify the underlying cause in test/CI logs. Not every subcategory has a unique signature; we include only those with specific, reproducible messages (e.g., \texttt{AttributeError: module 'os' has no attribute 'uname'}, \texttt{UnicodeDecodeError: 'utf-8' codec can't decode byte 0x..}). These signatures are meant to speed triage and help developers and tools quickly recognize portability issues; they are not exhaustive of all possible manifestations (see Table~\ref{tab:symptoms}).  

\begin{table*}[t]
    \centering
    \caption{Diagnostic symptom signatures for selected portability subcategories. Only concrete, reproducible errors are listed.}
    \label{tab:symptoms}
    \scriptsize
    \setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{1.15}
    \begin{tabularx}{\textwidth}{
        >{\raggedright\arraybackslash}m{0.22\textwidth}
        >{\raggedright\arraybackslash}m{0.30\textwidth}
        >{\raggedright\arraybackslash}X
    }
    \toprule
    \textbf{Category} & \textbf{Subcategory} & \textbf{Symptom (verbatim error / message)} \\
    \midrule
    \textbf{Method or module unavailable} &
    method does not exist: os.uname() &
    \texttt{AttributeError: module 'os' has no attribute 'uname'} \\
    & method does not exist: os.geteuid() & \texttt{AttributeError: module 'os' has no attribute 'geteuid'} \\
    & method does not exist: os.getuid() & \texttt{AttributeError: module 'os' has no attribute 'getuid'} \\
    & Method does not exist: os.getpgid() & \texttt{AttributeError: module 'os' has no attribute 'getpgid'} \\
    & module does not exist: readline & \texttt{ModuleNotFoundError: No module named 'readline'} \\
    & module does not exist: resource & \texttt{ModuleNotFoundError: No module named 'resource'} \\
    \midrule
    \textbf{System permission/limits mismatch} &
    File descriptor limits & \texttt{OSError: [Errno 24] Too many open files} \\
    & Symlink privilege restrictions & \texttt{OSError: symbolic link privilege not held: 'src' -> 'dst'} \\
    \midrule
    \textbf{Environment / terminal capability mismatch} &
    Curses maybe not avaliable Win & \texttt{ModuleNotFoundError: No module named '\_curses'} \\
    \midrule
    \textbf{System info source mismatch} &
    /proc not available on macOS & \texttt{FileNotFoundError: [Errno 2] No such file or directory: '/proc/...'} \\
    & Timezone database missing on Windows & \texttt{zoneinfo.\_common.ZoneInfoNotFoundError: 'No time zone found with key ...'} \\
    \midrule
    \textbf{Process execution / signal mismatch} &
    Signal handling (e.g., SIGKILL, SIGHUP) & \texttt{AttributeError: module 'signal' has no attribute 'SIGHUP'} \\
    & Address already in use & \texttt{OSError: [Errno 98] Address already in use} (Linux) \; / \; \texttt{OSError: [WinError 10048] Only one usage of each socket address (protocol/network address/port) is normally permitted} (Windows) \\
    \midrule
    \textbf{File/path representation mismatch} &
    Line ending mismatch & \texttt{AssertionError: '\textbackslash r\textbackslash n' != '\textbackslash n'} \\
    & Filename case insensitivite & \texttt{FileNotFoundError: [Errno 2] No such file or directory: 'Readme.md'} \\
    \midrule
    \textbf{Library availability / dynamic loading mismatch} &
    lib fcntl dont works in windows & \texttt{ModuleNotFoundError: No module named 'fcntl'} \\
    & Dynamic library loading mismatch & \texttt{OSError: [WinError 126] The specified module could not be found} \\
    \midrule
    \textbf{Opened file locking} &
    - & \texttt{PermissionError: [Errno 13] Permission denied: 'filename'} \\
    \midrule
    \textbf{Encoding mismatch} &
    - & \texttt{UnicodeDecodeError: 'utf-8' codec can't decode byte 0x\{XX\} in position N: invalid start byte} \\
    \bottomrule
    \end{tabularx}
    \end{table*}   

\noindent\textbf{OS-specific pathing.}
Differences in how operating systems handle file paths are a common source of portability issues. Code that hardcodes separators (e.g., \texttt{/} in Unix or \texttt{\textbackslash} in Windows), assumes case-insensitive filesystems, or relies on drive letters often behaves inconsistently, leading to broken comparisons, globbing, or even basic file discovery failures. Such problems frequently remain unnoticed because development and testing usually take place in a single environment, where these assumptions appear valid, and because standard utilities such as \texttt{os.path} tend to hide subtle discrepancies. A more reliable practice is to rely on \texttt{pathlib.Path} for path composition and traversal, normalize paths before comparison, and prefer temporary directories or relative paths over embedded absolute ones. When comparing file names across platforms, developers should also explicitly account for case sensitivity.


\noindent\textbf{Unavailable methods.}
Portability problems also arise when platform-specific functions are invoked without safeguards. A typical example is \texttt{os.uname()}, which is widely available on Unix-like systems but not implemented on Windows. If called unguarded, it raises an \texttt{AttributeError}, immediately breaking execution. These errors often escape detection because the code is authored and tested exclusively on Unix-like platforms, without continuous integration that includes Windows. Robust solutions involve feature detection (e.g., checking whether the attribute exists or catching exceptions) and, whenever possible, adopting portable APIs such as \texttt{platform.uname()}, which provide similar information consistently across systems.


\noindent\textbf{Command execution differences.}
Executing external commands is another area where cross-platform discrepancies are pronounced. Shell syntax, quoting rules, environment resolution, and executable extensions differ substantially, so scripts that assume a POSIX shell—such as those relying on pipelines, redirection, or calls via \texttt{sh -c}—typically fail on Windows. Even when commands execute, signal handling and exit-code semantics may diverge. These issues are often masked in developer environments, where required tools are already installed, or in tests that mock subprocess calls. A recommended approach is to use \texttt{subprocess.run} with explicit argument lists and \texttt{shell=False}, thereby avoiding shell-specific parsing. When possible, external calls should be replaced by equivalent Python libraries. If invoking external tools is unavoidable, paths should be resolved with \texttt{shutil.which}, encoding specified explicitly, and OS-specific branches introduced only when strictly necessary.

\subsubsection{RQ2.2 — Fixes}
\label{rq2-2}
\textbf{Research Question.} What are the typical fixes to address portability issues in Python?  
\textbf{Rationale.} This question focuses on how developers address portability issues once identified. Strategies include avoiding direct use of OS-specific APIs by relying on higher-level cross-platform libraries, normalizing filesystem paths with utilities such as pathlib, and handling encoding explicitly. Dependency-related problems are often mitigated by using containerization (e.g., Docker) or virtualization to ensure reproducible environments. Clear documentation of system-specific requirements also plays a key role. By analyzing these practices, we can highlight patterns that effectively reduce portability problems and help inform guidelines for the broader community.  

\textbf{Quick Answer.} Common fixes include relying on cross-platform libraries, normalizing paths, explicitly handling encodings, using containerized or virtualized environments, and documenting system requirements.  

% Add here: examples of fixes found in commits/PRs
% For example: Table 3 (Fix strategies with representative examples)

\begin{itemize}
  \item \textbf{Guards and fallbacks:} e.g., \texttt{hasattr(os,"uname")}, try/except around \texttt{ImportError}.  
  \item \textbf{Cross-platform APIs:} prefer \texttt{pathlib}, avoid Unix-only \texttt{fcntl}.  
  \item \textbf{Normalization:} line endings, encodings, ANSI escape stripping.  
  \item \textbf{Environment-aware tests:} skip GUI/\texttt{curses} on Windows or headless CI.  
  \item \textbf{Robust subprocess use:} avoid shell-only features, handle port-binding races.  
\end{itemize}

\subsection{RQ3 — Tool Effectiveness}
\label{rq3}
\textbf{Research Question.} How good are existing tools at detecting portability issues in Python projects?  
\textbf{Rationale.} This question investigates to what extent existing tools help identify portability problems. Static tools, including linters, type checkers, CodeQL queries, and even large language models, can flag certain categories of issues, such as OS-dependent APIs or unsafe path handling. However, many portability issues only manifest at runtime, for example due to environment assumptions, platform-specific encodings, or dependency incompatibilities. To complement static checks, we employ a dynamic strategy—our “rerun” approach—that executes tests across multiple operating systems to reveal failures that static analysis cannot anticipate. By comparing the coverage of static and dynamic techniques, we can assess their effectiveness and identify opportunities for improving tool support in cross-platform Python development.  

\textbf{Quick Answer.} Static tools (linters, CodeQL, LLM prompts) can detect some issues but miss runtime failures. Dynamic re-execution across OSes (our approach) provides broader coverage.


% Add here: quantitative comparison (coverage, precision, recall)
% For example: Figure 3 (Venn diagram of issues detected by static vs dynamic approaches)

\subsection{RQ4. How Do Developers Respond to Reported Portability Issues?}
\label{sec:pr-discussion}
\subsubsection{RQ4.1 — Applied Fixes}
\label{rq4-1}
\textbf{Research Question.} What types of fixes are most commonly applied to resolve portability issues, and what solution patterns can be identified?  
textbf{Rationale.} This question investigates the concrete changes developers make when addressing portability problems. For example, replacing operating-system-specific API calls, adopting cross-platform libraries, or adding conditional code that adapts to the environment. By categorizing and analyzing these changes, we can uncover recurring solution patterns that may be generalized into practical guidelines for writing more portable Python software.  

% Add here: commit/PR analysis, with representative examples of code changes
% For example: Table 4 (Common fix patterns with frequency and examples)


\textbf{Quick Answer.} Fixes often involve replacing OS-specific APIs, adding conditional checks, or adopting cross-platform libraries.  

\subsubsection{RQ4.2 — Community Reaction}
\label{rq4-2}
\textbf{Research Question.} How do open-source developers and project communities respond to reported portability issues (issues, pull requests, or discussions)?  
\textbf{Rationale.} This question examines how the OSS community treats portability fixes by analyzing issue trackers, pull requests, and related discussions. We investigate whether such problems are seen as important, how quickly they are addressed, and the engagement level (comments, reviews, merges) these fixes receive, helping us understand real-world awareness and prioritization of portability.  

% Add here: time-to-fix statistics, issue/PR comment analysis
% For example: Figure 4 (Distribution of time-to-fix), Table 5 (Engagement metrics per project)

To validate our findings and contribute back to the open-source ecosystem, we submitted pull requests addressing the portability issues uncovered in our study. Our contributions focused on libraries where our analysis revealed systematic portability challenges, proposing fixes to improve cross-platform compatibility and robustness.

Table~\ref{tab:prs} summarizes the results of this effort. We submitted a total of \TotalPROpened{} pull requests (PRs) spanning 8 categories of portability issues, of which \TotalPRAccepted{} had been accepted at the time of writing. The overall acceptance rate of 27\%, along with a zero rejection rate, indicates that the issues we identified correspond to genuine portability problems recognized by library maintainers. Notably, ``Method or module unavailable'' issues were the most frequent (11 PRs), reflecting substantial API inconsistencies across platforms. Conversely, ``Process execution / signal mismatch'' issues exhibited the highest acceptance rate (100\%), likely due to their critical impact on application functionality.

\begin{table}[h!]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{2pt}
    \caption{Summary of Pull Requests (PRs) by issue type, showing opened, accepted, and rejected. The last row shows totals.}
    \label{tab:prs}
    % Creating a compact tabular environment
    \begin{tabular}{lccc}
        \toprule
        \textbf{Category} & \textbf{Opened} & \textbf{Accepted} & \textbf{Rejected} \\
        \midrule
        Method or module unavailable & 11 & 0 & 0 \\
        Opened file locking & 4 & 2 & 0 \\
        File/path representation mismatch & 8 & 3 & 0 \\
        Encoding mismatch & 4 & 1 & 0 \\
        Environment / terminal capability mismatch & 1 & 0 & 0 \\
        Process execution / signal mismatch & 2 & 2 & 0 \\
        System info source mismatch & 2 & 1 & 0 \\
        Library availability / dynamic loading mismatch & 1 & 0 & 0 \\
        \midrule
        \textbf{Total} & \textbf{\TotalPROpened} & \textbf{\TotalPRAccepted} & \textbf{\TotalPRRejected} \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Quick Answer.} Developers typically acknowledge portability problems but often give them lower priority than functional bugs, resulting in varied resolution times.  



\Den{Please review.}
To better understand how open-source developers and project communities react when portability issues are reported, we submitted pull requests (PRs) across different projects. The responses we received illustrate a spectrum of engagement patterns, from prompt acceptance to rejection or non-merge due to project inactivity.

% \cite{breds_pr_40}
In one case, the proposed change addressed a test portability issue arising from platform-specific file handling. The original test used Python's \texttt{tempfile.NamedTemporaryFile} without disabling automatic deletion, which led to failures on Windows due to how temporary files are closed and reopened. Our fix stored the path before closing the file and explicitly removed it after the test execution. The maintainer accepted the contribution, commenting: \emph{``Thanks for this.''}, 
in another PR, the maintainer expressed appreciation for the contribution and promptly merged the fix, stating: \emph{``Thanks a lot for this contribution! Good catch.''}
% \cite{python_doit_api_pr_9}. 
These responses highlight a positive and collaborative stance, where developers value external input and promptly integrate fixes that improve cross-platform compatibility.

% \cite{frequenz_pr_1261}
A different outcome emerged in a PR where we attempted to resolve a dependency portability issue. Specifically, the tests relied on the \texttt{zoneinfo} and \texttt{tzdata} modules, which are not consistently available across platforms. Our proposed patch included conditional installation of the dependency when running on Windows. However, the maintainer argued that the better solution was to avoid reliance on the library altogether, regardless of platform. As a result, the PR was reduced to removing the dependency and related tests. This case illustrates that communities sometimes prefer simplifying the codebase rather than maintaining platform-specific workarounds.

Finally, in another project, the response was shaped less by technical considerations and more by the project's state of maintenance. The maintainer acknowledged the validity of the patch but declined to merge it, noting: \emph{``I do somewhat view that project as archived and inactive... My concern about merging is that an updated last commit date would make the project seem more active than it really is.''} This case illustrates how project activity levels and community sustainability strongly influence whether portability improvements are accepted, even if the technical changes are sound.

Overall, these three cases demonstrate that responses to portability contributions vary significantly: active projects may readily adopt fixes, maintainers may prefer design simplifications over platform-specific adjustments, and inactive projects may resist merging altogether to avoid signaling misleading project vitality.


\section{Lessons Learned}
\label{sec:lessons}

\Ali{@Marcelo, please suggest the overall structure for this section.}
\Den{What should developers, researchers, and tool builders keep in mind when thinking about portability issues in Python? Here are some suggestions:}
% - Practical guidelines distilled from study
% - Recommendations for writing portable Python code
% - Recommendations for tool designers and researchers
% - General lessons beyond Python (cross-platform SE insights)

% Not in a philosophical way, but in a practical way, what are the main takeaways from our study? 

% \begin{itemize}
%   \item \textbf{Portability is not automatic.} Even though Python is designed to be cross-platform, real projects still often break when run on different operating systems. Portability depends on how code and environments are actually used, not just what the language promises.
%   \item \textbf{Problems fall into repeatable categories.} The failures we found were not random. They fall into a small set of categories (like paths, encodings, missing APIs). This means we can focus research and tool support on the most common categories instead of treating each bug as unique.
% \end{itemize}



\section{Threats to Validity}
\label{sec:threats}
% - Internal validity: correctness of rerun framework
% - External validity: generalization beyond studied projects
% - Construct validity: definitions of “portability” and categorization
% - Conclusion validity: statistical soundness, possible biases


\section{Related Work}
\label{sec:related}
% - Prior work on cross-platform failures, portability in SE
% - Studies on testing across operating systems
% - Tool support for detecting environment-specific issues
% - Positioning of this work relative to literature

\Fix{Talk about} \cite{vahabzadeh2015empirical} here. Others: \cite{rasley2015detecting, ghandorh2020systematic}

Maybe: \cite{sun2016bear}. Mobile apps testing: \cite{boushehrinejadmoradi2015testing}

\section{Conclusions}
\label{sec:conclusions}
% - Recap of findings for each RQ
% - Key contributions
% - Long-term impact on research and practice
% - Future directions

%% \section*{Data Availability}

%% The artifacts -- including datasets and scripts -- are publicly available~\cite{ourrepo}.

\bibliographystyle{ACM-Reference-Format}
% Allow underscores in BibTeX keys within the .bbl without math mode errors
\begingroup\catcode`\_=12 \relax
\bibliography{ref}
\endgroup

\end{document}
