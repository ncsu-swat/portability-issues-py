% \documentclass[acmsmall,screen,review]{acmart} % OOPSLA
\documentclass[sigconf,review,anonymous]{acmart} % MSR

%%%%%
%% We will need to add this in camera-ready
%%%%%
%% \setcopyright{none} % to remove the copyright notice
%% \settopmatter{printacmref=false} % to remove the ACM Reference Format
%% \renewcommand\footnotetextcopyrightpermission[1]{} % to remove copyright box
\usepackage{comment}

\usepackage{lipsum}
\input{macros}

%% add page numbers
\pagestyle{plain}

\begin{document}

\title{\PaperTitle{}}

\begin{abstract}
  \sloppy
Python is broadly used across operating systems, but our cross-OS test re-execution reveals persistent portability issues. We study \NumProjects{} repositories (currently \NumProjectsAnalyzed{} projects analyzed), execute 440{,}728 tests across operating systems, and observe 9{,}508 tests with divergent outcomes; 2{,}421 differences have been analyzed so far. We identify 7 actionable root-cause categories (e.g., unavailable \texttt{os.*} methods on Windows, terminal/GUI capability gaps in CI, path/line-ending mismatches, dynamic loading issues) and curate concrete repair patterns (e.g., guards and fallbacks, cross-platform APIs, normalization). These results provide early empirical evidence of the prevalence, causes, and fix patterns of Python portability issues and establish baselines for future tools and guidelines.  
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011074.10011099</concept_id>
       <concept_desc>Software and its engineering~Software verification and validation</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10011007.10010940.10010992.10010993.10010994</concept_id>
       <concept_desc>Software and its engineering~Functionality</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software verification and validation}
\ccsdesc[500]{Software and its engineering~Functionality}

\keywords{Mining Software Repositories, Python, Cross-Platform, Portability, Test Re-execution, Repair Patterns}

\maketitle

\section{Introduction}
\label{sec:intro}

\section{Introduction}
\label{sec:intro}
Python is widely regarded as cross-platform, powering applications
from diverse domains (e.g., data science pipelines, web applications,
etc.) and consistently ranks among the most popular programming
languages of
today~\cite{stackoverflow2025,githuboctoverse2024,pypl2025}. Yet,
platform portability is not a guarantee. For example, Mince \etal{}
found that 40\% of ML library functionality does not perform
consistently across different hardware (\eg{}, CPU and
TPU)~\cite{mince2023}. Similar problem exists at the OS level.
%% While
%% Python's design philosophy emphasizes portability through standardized
%% interfaces in modules like \texttt{os}~\cite{pymotwos,geeksos2025},
%% real-world codebases routinely contain platform-specific assumptions
%% that render them fragile across operating systems.
For example, the Python function \texttt{os.geteuid()} is unavailable
on Python distributions for Windows. Overall, non-portable code
results in failed deployments and disrupts develpment
productivity~\cite{mince2023,kiuwancodeportability,peng2024,job2024}. Container
technology (e.g., Docker and Singularity) can mitigate this problem
but we find that most open source projects are not containerized,
i.e., they lack Dockerfile or Singularity recipe to create
images. Only 19.9\% of \NumProjects{} \github projects in Python we
analyze contain Dockerfile or Docker compose
configurations.\Mar{Ali/Denini, please check/revise!}

%% (e.g., broken
%% continuous integration, increased maintenance overhead, and broader
%% implications like 
%% These failures manifest in
%% predictable ways: missing system calls on Windows
%% (e.g., \texttt{os.geteuid()}) \cite{pythonos315}, GUI-dependent code
%% failing in headless environments, filesystem path incompatibilities,
%% and dynamic library loading issues that vary by
%% platform \Fix{Cite}. What appears to be a language-level guarantee of
%% portability dissolves under the weight of actual usage patterns and
%% environmental dependencies~\cite{peng2024,job2024}.


%%% This is way too long (distracting) to add in an introduction.
%%%
%% Our analysis of \NumProjects{} Python projects reveals significant insights regarding containerization adoption in the open-source ecosystem. Among these projects, only 345 utilize Docker through either Dockerfiles or Docker Compose configurations, representing approximately 16.9\% of the total dataset. This relatively low adoption rate highlights a critical concern: the vast majority of Python projects (83.1\%) remain potentially vulnerable to portability issues. Projects without containerization face inherent challenges in maintaining consistent execution environments across different systems, dependency management complications, and deployment reproducibility problems. The absence of Docker containerization means that these projects rely heavily on local environment configurations, Python version compatibility, and system-specific dependencies, which can lead to the infamous ``it works on my machine'' syndrome. This portability gap becomes particularly problematic when projects need to be deployed across different operating systems, cloud platforms, or when team members work with varying development setups, ultimately hindering collaboration and increasing the likelihood of environment-related bugs and deployment failures.

%% This fragility is not merely a theoretical concern. Modern software
%% development increasingly relies on cross-platform deployment, from
%% containerized applications that must run consistently across
%% different host systems to open-source projects that serve diverse
%% user communities. When portability breaks, it creates cascading
%% problems: delayed releases, increased maintenance burden, and
%% reduced software accessibility.

This paper reports on a study about portability issues in Python
tests\Mar{is it tests or code? considering the text above...why
tests?}. More precisely, we analyze prevalence, root causes of
failures, their fixes, and the reaction of developers toward the pull
requests we opened to fix portability issues in their code. We
conclude with a set of lessons we learn, including \Mar{try to
integrate with next paragraph...}

Our analysis reveals that these portability failures, while widespread, are not chaotic or unpredictable. Instead, they cluster around a small set of well-defined categories—path handling, character encoding, platform-specific APIs, and environment assumptions. This systematic nature suggests that targeted solutions are possible, transforming what appears to be an intractable problem into a manageable engineering challenge.

This paper contributes an empirical study that (i) re-executes tests across operating systems for a large set of open-source projects, (ii) categorizes the root causes behind cross-OS differences, and (iii) derives concrete repair patterns.

\paragraph{Contributions.}
\begin{itemize}
  \item Cross-OS re-execution of Python tests across a large sample (\NumProjects{} repositories, with results reported so far from \NumProjectsAnalyzed{}).
  \item A taxonomy of 7 actionable root-cause categories (Table~\ref{tab:categories}) with ready-to-apply repair patterns.
\end{itemize}


\label{sec:sources}
We define an \emph{OS portability issue} as a difference in test outcome across operating systems attributable to platform-specific APIs, environment/terminal capabilities, filesystem semantics, encodings, or native libraries. 

\section{Methods and Objects of analysis}
\label{sec:methodology}

This section describes the projects we used (\ref{sec:projects}), the
setup of our experiments (\ref{sec:setup}), and the research questions
we posed (\ref{sec:questions}).

\subsection{Projects}
\label{sec:projects}

\textbf{Dataset.} We sampled \NumProjects{} Python repositories; the analyses reported here reflect the first \NumProjectsAnalyzed{} analyzed.  
\textbf{Selection criteria.} We selected projects using \github{}'s REST API, focusing on repositories primarily implemented in Python that show evidence of testing activity (e.g., use of pytest or unittest), have at least one star, and contain at least one test case that can be executed successfully. This filtering ensures that the analyzed projects are active, testable, and representative of real-world Python software.
% i) \textbf{Language:} Python as primary language; ii) \textbf{Activity:} at least 100 commits and 10 contributors; iii) \textbf{Testing:} presence of an automated test suite (e.g., pytest, unittest); iv) \textbf{Popularity:} at least 50 stars on \github{}; v) \textbf{Diversity:} a mix of application domains (web, data science, utilities, etc.); vi) \textbf{Non-containerized:} absence of Dockerfile or Singularity recipe to create images.

\begin{table}[H]

\centering
\caption{Stats on \NumProjects{} evaluated projects: no. of tests (\#Tests), size (SLOC), no. of commits (\#Sha), age in years (Age), no. of \github{} stars ($\star$).\label{tab:project-stats}}
\begin{tabular}{lrrrrr}
\hline
Statistic & \#Tests & SLOC & \#Sha & Age & $\star$ \\
\hline
Mean & 583.94 & 42,467.26 & 2,883.25 & 6.24 & 6,147.88 \\
Median & 72 & 7,478 & 426.50 & 5.87 & 83.50 \\
Min & 1 & 2 & 2 & 0.3 & 2 \\
Max & 36,588 & 3,237,895 & 194,153 & 17.27 & 366,472 \\
Sum & 1,169,056 & 85,019,457 & - & - & - \\
\hline
\end{tabular}
\end{table}


Table~\ref{tab:project-stats} summarizes key characteristics of the \NumProjects{} evaluated projects. 
On average, each project comprises approximately 584 tests and 42K lines of Python code (SLOC), with a median of 72 tests and 7.5K SLOC, indicating a distribution skewed by a few very large systems. Regarding repository activity, projects have on average 2.9K commits, a median of 427, and span a mean lifetime of 6.2 years. The number of \github{} stars varies widely, with an average of 6.1K, a median of only 84, and a maximum exceeding 366K, reflecting the presence of both niche and highly popular projects. 

Overall, the dataset covers a diverse range of software systems, from small-scale repositories with only 
a handful of commits to large, long-lived, and widely adopted projects.




\subsection{Methods}

We use two complementary methods to identify portability issues:

\textbf{Cross-OS re-execution.} For each project we run the test suite on multiple operating systems and record divergent outcomes. We then triage failures via logs and small code inspections, mapping each instance to a concrete category.\Fix{Explain what is a category.}  

\textbf{Issues Mining.} To complement test-based findings, we also mine \github{} issues and pull requests. We built a “candidate finder” combining multiple keyword axes:  
(a) OS/platform indicators (e.g., Windows, Linux, macOS, specific distros/architectures),  
(b) failure/fix language (e.g., fails, error, bug, fix, workaround),  
(c) testing/CI context (pytest, CI, \github{} Actions), and  
(d) common portability causes (e.g., path separators, chmod/permissions, encodings/UTF-8, dynamic libraries like \texttt{.dll}/\texttt{.so}).  

We applied AND across axes and OR within each axis, so matches typically required an OS reference plus a failure/fix cue, optionally reinforced by test or cause terms. We searched titles, bodies, and comments, with extra weight to title and close-proximity matches. Light triage (brief summaries and negative filters for off-topic mentions) kept the set precise. Candidate issues were normalized into consistent records (project, link, date, summary, compact tags like OS=, FIX=, TEST=, CAUSE=), duplicates removed, and full text (title, description, comments) archived for analysis.  

\subsection{Setup}
\label{sec:setup}

% \Mar{HEY, this is not what you are doing. You need to explain how you
% used \github{} Actions/Cloud to create VMs for different OSes.}

\Den{Updated: Please check/revise!}


All experiments were executed on ephemeral virtual machines provided by GitHub-hosted runners~\cite{github_actions_runners}. Each virtual machine is automatically provisioned with 4~CPUs and 16~GB of RAM. We implemented a single GitHub Actions workflow (YAML) that uses a matrix strategy to run the test-suite on Ubuntu 24.04 LTS, macOS 15, and Windows Server 2025 (each job runs in a fresh VM image). Each job installs dependencies, runs the Python tests with \texttt{pytest}~\cite{pytest_tool} and the \texttt{pytest-json-report} plugin~\cite{pytest_json_report} to create JSON reports, and finally uploads the JSON files as workflow artifacts for later analysis. This approach ensures reproducible, parallel cross-platform testing while GitHub manages VM provisioning and maintenance.



% \Den{I don't know if it's necessary and I also don't know if the text is clear enough. Please check/revise!}
% All experiments were conducted across three operating systems to ensure compatibility. The Linux setup used an AMD EPYC 7763 64-Core Processor with 4 CPUs, 16 GB RAM, and Ubuntu 24.04 LTS.

% The macOS setup ran on an Apple M2 Pro chip (Arm64) with 5 CPU cores, 8 GPU cores, 16 GB RAM, and macOS 15. The Windows setup used an AMD EPYC 7763 64-Core Processor with 4 CPUs, 16 GB RAM, and Windows Server 2025.

\subsection{Research Questions and Methodology}
\label{sec:questions}

We aim to answer the following key research questions:

\noindent\textit{\RQOne{}}

\textbf{Rationale.} With this question, we aim to measure how often portability problems actually occur in practice. Although Python is advertised as a cross-platform language, our preliminary results suggest that many tests fail when run on different operating systems. By quantifying the frequency of such issues across a representative set of projects and applying statistical tests, we can provide stronger evidence on whether portability is a widespread concern in the Python ecosystem or only affects a small subset of projects.  


% \textbf{Methodology.} \Fix{Describe here the methodology to answer RQ1.}
\par

\textit{\RQTwo{}}

\textbf{Rationale.} This question is important to justify further investigation on portability problems in Python software. If we find that these problems are rare or have no consistent causes or manifestations, then further investigation is meaningless. To understand how and why portability issues occur in Python programs, and how they can be effectively addressed, we aim to answer three sub-questions:



\noindent\textit{\RQTwoOne{}}
% \textbf{Rationale.} This question seeks to identify and categorize the underlying reasons why portability issues arise. Our observations indicate that differences in modules such as os, file system conventions, path handling, and environment-specific assumptions may contribute to failures. In addition, some dependencies work properly on one operating system but not on another, further aggravating the problem. Understanding these root causes is essential not only to characterize the problem but also to guide the development of best practices and tools to prevent such issues in the future.  

% \textbf{Rationale.} Recognizing diagnostic signatures helps developers quickly triage failures and identify the underlying portability issue from test logs or CI output. While not all issues produce unique error messages, many exhibit specific, reproducible patterns that enable rapid diagnosis.

\noindent\textit{\RQTwoTwo{}}
\noindent\textit{\RQTwoThree{}}

% \textbf{Methodology.} \Fix{Describe here the methodology to answer RQ2}

\noindent\textit{\RQThree{}}

\textbf{Rationale.} This question investigates to what extent existing tools help identify portability problems. Static tools, including linters, type checkers, CodeQL queries, and even large language models, can flag certain categories of issues, such as OS-dependent APIs or unsafe path handling. However, many portability issues only manifest at runtime, for example due to environment assumptions, platform-specific encodings, or dependency incompatibilities. To complement static checks, we employ a dynamic strategy—our ``rerun'' approach—that executes tests across multiple operating systems to reveal failures that static analysis cannot anticipate. By comparing the coverage of static and dynamic techniques, we can assess their effectiveness and identify opportunities for improving tool support in cross-platform Python development. 

% \textbf{Methodology.} \Fix{Describe here the methodology to answer RQ3}


\noindent\textit{\RQFour{}}

\textbf{Rationale.} This question is important to justify further investigation on how developers handle portability issues in real projects. If developers rarely fix such problems or do not discuss them actively, further investigation is meaningless. To understand both the technical and social dimensions of how portability issues are addressed, we aim to answer two questions: 

\noindent\textit{\RQFourOne{}}

\noindent\textit{\RQFourTwo{}}

% \textbf{Methodology.} \Fix{Describe here the methodology to answer RQ4}



\section{Results}
\label{sec:results}
% This section presents the empirical results of our study,
% organized around the research questions (RQ1–RQ4.2).
% For each RQ, we restate the rationale, present the findings,
% and provide a concise quick answer.

This section provides answering to the posed research questions.

\subsection{\textbf{Answering} \RQOne{}}
\label{rq1}

% add here: statistics, tables, and plots showing the frequency and distribution of portability issues
% for example: Table 1 (Projects analyzed, tests executed, failures observed), Figure 1 (Distribution of failures by OS)

\begin{table}[t!]
  \centering
  \caption{\label{tab:dataset}Prevalence of portability problems.}  
  \begin{subtable}[t]{\linewidth}
    \centering
    \caption{\label{tab:dataset-a}Detected problems with test re-execution.}
    \begin{tabular}{lr}
    \toprule
    Total number of projects mined & \NumProjects{} \\
    Total number of projects selected & \NumProjectsAnalyzed{} \\
    \hspace{2ex}Total number of projects with OS differences & \NumProjectsWithOSDiffInTests{} \\    
    Total number of tests executed & \NumTestsExecuted{} \\
    \hspace{2ex}Total number of test runs with OS differences & \NumTestRunsWithOSDiff{} \\
    %Total number of differences analyzed & 2,421 \\
    \bottomrule
    \end{tabular}
  \end{subtable}
  \vspace{1ex} % space between subtables
  \begin{subtable}[t]{\linewidth}
    \centering
    \caption{\label{tab:dataset-b}Detected problems with manual inspection.}
    \begin{tabular}{lc}
      \toprule
      Total number of issues mined & \NumIssuesMined{} \\
      Total number of issues analyzed & \NumIssuesAnalyzed{} \\
      Total number of issues related & \NumIssuesRelated{}\\
      \hspace{2ex}Total number of projects with OS differences & \NumProjectsWithOSDiffInIssues{} \\
      \bottomrule
    \end{tabular}
  \end{subtable}
\end{table}

Table~\ref{tab:dataset} reports on the prevalence of portability
problems in \github{} Python projects. Table~\ref{tab:dataset-a}
details the results we obtain when re-executing test cases whereas
Table~\ref{tab:dataset-b} details the results we obtain when
inspecting issues from projects. 

Table~\ref{tab:dataset-a} shows that
we mined a total of \NumProjects{} but re-executed test suites of
nearly 25\% of them, \ie{}, \NumProjectsAnalyzed{} projects. Recall
from Section~\ref{sec:setup} that each test suite is executed on three
different virtual machines and the outcomes of each test cases is
compared for discrepancy. 

The \NumProjectsAnalyzed{} projects whose
test suites were executed included a total of \NumTestsExecuted{} test cases. Of
those, we found discrepant outcomes (\ie{}, there was no agreement
across all three OSes that we considered) in \NumTestRunsWithOSDiff{} cases, which
corresponds to \PercentDiscrepantTests{}. It is worth noting
that test execution method is precise, \ie{}, we found no cases of
false positives. All discrepancies we find indicate with this method
indicate real portability issues. In contrast to the test re-execution
method, the issue mining method is imprecise due to the inherent
innacuracy of text mining. As such, we require to manually analyze the
discussion threads of the issues to understand if they indicate real
portability issues. 

Table~\ref{tab:dataset-b} shows the results from 
mining and analyzing \github{} issues. We mined a total of 
\NumIssuesMined{} candidate issues using our keyword-based search 
strategy described in Section~\ref{sec:methodology}. From this set, we 
randomly sampled \NumIssuesAnalyzed{} of the most recent issues for 
manual inspection. 

Through careful analysis of issue titles, descriptions, 
discussion threads, and the associated code, we confirmed that \NumIssuesRelated{} of these 
were genuine portability issues, representing a confirmation rate of 
42.5\%. While this method is less precise than test re-execution and 
requires significant manual effort, it provides complementary insights 
that test-based detection cannot capture. Specifically, issue mining 
reveals how developers discuss, diagnose, and prioritize portability 
problems in practice, exposes historical patterns across different project 
contexts, and surfaces issues in projects or code paths that may lack 
comprehensive test coverage. 

Importantly, the \NumIssuesRelated{} 
confirmed issues come from projects with no overlap with those analyzed 
in Table~\ref{tab:dataset-a}, demonstrating that portability problems 
extend beyond our test execution sample and affect a broader range of 
the Python ecosystem. \Ali{Please revise the issue mining part.}

\begin{tcolorbox}[boxrule=0pt,sharp corners,boxsep=2pt,left=2pt,right=2pt,top=2.5pt,bottom=2pt]
\textbf{RQ\textsubscript{1} (prevalence)}: We find that portability
issues are relatively prevalent. Of the \NumProjectsAnalyzed{}
projects whose tests we re-executed in different OSes, 
\PercentProjectsWithPortabilityReExecution{} have tests with portability problems.
Additionally, our issue mining approach confirmed \NumIssuesRelated{} genuine 
portability issues from a separate set of projects, demonstrating that these 
problems affect a broad range of the Python ecosystem.
\end{tcolorbox}

\subsection{\textbf{Answering} \RQTwo{}}

\Ali{Please revise the structure of RQ2.}

This section elaborates the characteristics of portability problems:
root causes, symptoms, and fixes.

\subsubsection{\textbf{Answering \RQTwoOne{}}}
\label{rq2-1}

~

\noindent Table~\ref{tab:categories} presents our taxonomy of portability issues 
organized by root causes. Each row groups related sub-categories
under a main category. The columns show: (i) the root-cause category name and acronym,
(ii) specific sub-categories within that category,
(iii) a brief description of each sub-category,
(iv) the number of distinct projects where the issue appeared in test execution,
(v) the number of distinct projects where the issue appeared in mined \github{} issues, and 
(vi) the total count across both sources. 

% ========================= CATEGORIES TABLE =========================
\begin{table*}[t]
    \centering
    \caption{Taxonomy of portability issues showing root-cause categories, subcategories, brief descriptions, and number of distinct projects observed in test execution and mined \github{} issues.}
    \label{tab:categories}
    \scriptsize
    \setlength{\tabcolsep}{3pt}
    \renewcommand{\arraystretch}{1.15}
    \begin{tabularx}{\textwidth}{
    >{\raggedright\arraybackslash}p{0.18\textwidth}
    >{\raggedright\arraybackslash}X
    >{\raggedright\arraybackslash}p{0.23\textwidth}
    >{\centering\arraybackslash}m{0.08\textwidth}
    >{\centering\arraybackslash}m{0.08\textwidth}
    >{\centering\arraybackslash}m{0.07\textwidth}
}
    \toprule
    \textbf{Root-cause category (Acronym)} & \textbf{Sub-categories} & \textbf{Description} & \textbf{\# of projects in tests } & \textbf{\# of projects in issues} & \textbf{Total} \\
    \midrule
    
    \multirow[t]{6}{*}{File and Directories (FILE)} 
    & Path separators & Forward vs. backslash differences & 7 & 27 & \multirow{6}{*}{60} \\
    & Line endings & CRLF vs. LF mismatch & 1  & 5 &                      \\
    & Case sensitivity & Filename case handling varies & 1  & 0 &                      \\
    & File locking & Windows locks open files & 5 & 4 &                      \\
    & Encoding & UTF-8 not default everywhere & 2 & 7 &                      \\
    & Filesystem block size & Block size assumptions differ & 0 & 1 &                      \\
    \midrule
    
    \multirow[t]{3}{*}{Process and Signals (PROC)}
    & Shell command execution & Shell behavior differs across OS & 5 & 14 & \multirow{3}{*}{25} \\
    & Missing signals & Signals unavailable on Windows & 3 & 2 &                      \\
    & Address already in use & Port binding conflicts vary & 1 & 0 &                      \\
    \midrule
    
    \multirow[t]{4}{*}{Library Dependencies (LIB)}
    & Platform-specific libraries (e.g. fcntl) & Unix-only libraries missing elsewhere & 2  & 1 & \multirow{4}{*}{24} \\
    & Missing libraries & Dependencies not universally available & 2 & 11 &                      \\
    & Dynamic library loading & DLL vs. SO file issues & 0 & 5 &                      \\
    & Binary wheel mismatch & Architecture-specific wheel incompatibilities & 0 & 3 &                      \\
    \midrule
    
    \multirow[t]{4}{*}{Environment and Display (ENV)}
    & No display in github CI (linux) & Headless CI lacks display server & 3 & 2 & \multirow{4}{*}{14} \\
    & GUI differences & Window manager behavior varies & 0 & 4 &                      \\
    & Missing curses & Curses unavailable on Windows & 1 & 0 &                      \\
    & Terminal capabilities & Terminal features differ across systems & 2 & 2 &                      \\
    \midrule
    
    \multirow[t]{2}{*}{API Availability (API)}
    & Methods: os.uname(), os.geteuid(), os.getuid(), os.getpgid() & OS methods missing on Windows & 4 & 2 & \multirow{2}{*}{8} \\
    & Modules: readline, resource & Optional modules not everywhere & 0 & 2 &                      \\
    \midrule
    
    \multirow[t]{3}{*}{Permissions and Limits (PERM)}
    & Permission & Permission models differ by OS & 3 & 1 & \multirow{3}{*}{7} \\
    & File descriptor limits & Resource limits vary across systems & 0 & 1 &                     \\
    & Symlink privileges & Windows requires admin for symlinks & 1 & 1 &                     \\
    \midrule
    
    \multirow[t]{2}{*}{System Information (SYS)}
    & /proc filesystem & /proc missing on macOS/Windows & 1 & 0 & \multirow{2}{*}{2} \\
    & Timezone database & Timezone data not always installed & 1 & 0 &                      \\
    \midrule
    \textbf{Total} & - & - & \textbf{\NumProjectsWithOSDiffInTests{}} & \textbf{\NumProjectsWithOSDiffInIssues{}} & \textbf{140} \\
    \bottomrule
    \end{tabularx}
\end{table*}

We identify seven main root-cause categories that explain why portability problems occur:

\noindent\textbf{File and Directories (FILE):} The most prevalent category (60 projects) stems from OS differences in file operations. Key issues include path separator differences (backslash vs. forward slash), line ending mismatches (\CodeIn{CRLF} vs. \CodeIn{LF}), file locking semantics, and text encoding defaults.

\noindent\textbf{Process and Signals (PROC):} This category (25 projects) captures failures due to OS process management differences. For example, shell execution varies across platforms (\CodeIn{cmd.exe} vs. \CodeIn{bash}), Windows lacks many Unix signals (\CodeIn{SIGHUP}, \CodeIn{SIGKILL}), and port binding produces different error codes.

\noindent\textbf{Library Dependencies (LIB):} These failures (24 projects) occur when code assumes platform-specific libraries. Issues include Unix-only modules (e.g., fcntl), missing dependencies, different dynamic library extensions (\CodeIn{.dll} vs \CodeIn{.so} vs \CodeIn{.dylib}), and architecture-specific binary wheels.

\noindent\textbf{Environment and Display (ENV):} These problems (14 projects) arise from runtime environment differences. Headless CI lacks display servers for GUI testing, window managers behave differently, terminal capabilities vary, and curses may be unavailable (especially on Windows).

\noindent\textbf{API Availability (API):} This category (8 projects) covers Python APIs not universally available. Many OS module methods are Unix-specific (\CodeIn{os.uname()}, \CodeIn{os.geteuid()}, \CodeIn{os.getpgid()}) and fail on Windows, while optional modules like readline and resource may be missing.

\noindent\textbf{Permissions and Limits (PERM):} These issues (7 projects) reflect OS differences in access control. Permission models differ (Unix chmod vs. Windows ACLs), file descriptor limits vary, and symbolic link creation requires admin privileges on Windows.

\noindent\textbf{System Information (SYS):} This category (2 projects) includes failures from platform-specific system information mechanisms. The /proc filesystem exists only on Linux, and timezone databases may not be installed by default.

\begin{figure}[!ht]
\centering
% Defensive Checks
\begin{subfigure}{0.48\textwidth}
\begin{lstlisting}[caption={Defensive check for optional library}, label={lst:defensive}]
- import readline
+ try:
+     import readline
+ except ImportError:
+     readline = None  # Fallback if readline is unavailable
\end{lstlisting}
\end{subfigure}
\hfill
% Portable APIs
\begin{subfigure}{0.48\textwidth}
\begin{lstlisting}[caption={Portable APIs: Using pathlib for cross-platform paths}, label={lst:portable}]
- import os
- home_dir = os.path.expanduser('~')
+ from pathlib import Path
+ home_dir = Path.home()  # Cross-platform home directory
\end{lstlisting}
\end{subfigure}
\vspace{0.2cm}
% Normalization
\begin{subfigure}{0.48\textwidth}
\begin{lstlisting}[caption={Normalization: Normalizing file paths}, label={lst:normalization}]
- file_path = "data/file.txt"
+ import os
+ file_path = "data" + os.path.sep + "file.txt"  # Consistent path
\end{lstlisting}
\end{subfigure}
\hfill
% Environment Handling
\begin{subfigure}{0.48\textwidth}
\begin{lstlisting}[caption={Environment handling: Using pytest skip}, label={lst:environment}]
+ import platform
+ @pytest.mark.skipif(platform.system() == "Darwin", reason="Not supported on macOS")
def test_special_case():
    ...
\end{lstlisting}
\end{subfigure}
\caption{Representative examples of fixes for portability issues: (a) Defensive checks, (b) Portable APIs, (c) Normalization, and (d) Environment handling.}
\label{fig:patterns}
\end{figure}


\subsubsection{\textbf{Answering \RQTwoTwo{}}}

\noindent\begin{table*}[t]
    \centering
    \caption{Diagnostic symptom signatures and general fix patterns for portability sub-categories.}
    \label{tab:symptoms}
    \scriptsize
    \setlength{\tabcolsep}{3pt}
    \renewcommand{\arraystretch}{1.15}
    \begin{tabularx}{\textwidth}{
        >{\raggedright\arraybackslash}m{0.06\textwidth}
        >{\raggedright\arraybackslash}m{0.18\textwidth}
        >{\raggedright\arraybackslash}X
        >{\raggedright\arraybackslash}m{0.3\textwidth}
    }
    \toprule
    \textbf{Root-cause category} & \textbf{Sub-categories} & \textbf{Symptom (verbatim error / message)} & \textbf{General Fix Pattern} \\
    \midrule
    \textbf{FILE} &
    Path separators & No specific signature & Normalization — normalize paths with pathlib \\
    & Line endings & \texttt{AssertionError: '\textbackslash r\textbackslash n' != '\textbackslash n'} & Normalization — normalize line endings \\
    & Case sensitivity & \texttt{FileNotFoundError: [Errno 2] No such file or directory: 'Readme.md'} & Normalization — case-normalize filenames \\
    & File locking & \texttt{PermissionError: [Errno 13] Permission denied: 'filename'} & Environment handling — use context managers, explicit close \\
    & Encoding & \texttt{UnicodeDecodeError: 'utf-8' codec can't decode byte 0x\{XX\} in position N: invalid start byte} & Normalization — set explicit UTF-8 encoding \\
    & Filesystem block size & No specific signature & Normalization — normalize computations avoid assumptions \\
    \midrule
    \textbf{PROC} &
    Shell command execution & No specific signature & Portable APIs — use subprocess.run shell=False \\
    & Missing signals & \texttt{AttributeError: module 'signal' has no attribute 'SIGHUP'} & Defensive checks — guard signals per platform \\
    & Address already in use & \texttt{OSError: [Errno 98] Address already in use} (Linux) \; / \; \texttt{OSError: [WinError 10048] Only one usage of each socket address (protocol/network address/port) is normally permitted} (Windows) & Defensive checks + Environment handling — catch OS-specific errors, use port 0 \\
    \midrule
    \textbf{LIB} &
    Platform-specific libraries (e.g. \CodeIn{fcntl}) & \texttt{ModuleNotFoundError: No module named 'fcntl'} & Defensive checks — conditional import provide fallback \\
    & Missing libraries & \texttt{ModuleNotFoundError: No module named '...'} & Defensive checks — optional import with fallback \\
    & Dynamic library loading & \texttt{OSError: [WinError 126] The specified module could not be found} & Defensive checks — guard loading prefer pure-Python \\
    & Binary wheel mismatch & No specific signature & Environment handling — install platform-appropriate wheel \\
    \midrule
    \textbf{ENV} &
    No display in github CI (linux) & \texttt{TclError: no display name and no \$DISPLAY environment variable} & Environment handling — skip GUI tests in CI \\
    & GUI differences & No specific signature & Environment handling — skip or mock GUI features \\
    & Missing curses & \texttt{ModuleNotFoundError: No module named '\_curses'} & Defensive checks — conditional import with stub fallback \\
    & Terminal capabilities & No specific signature & Environment handling — detect capabilities degrade gracefully \\
    \midrule
    \textbf{API} &
    Methods: os.uname(), os.geteuid(), os.getuid(), os.getpgid() &
    \texttt{AttributeError: module 'os' has no attribute 'uname'} (example) & Portable APIs + Defensive checks — use platform.uname, hasattr guards \\
    & Modules: readline, resource & \texttt{ModuleNotFoundError: No module named 'readline'} (example) & Defensive checks — guard import use alternatives \\
    \midrule
    \textbf{PERM} &
    Permission & \texttt{PermissionError: [Errno 13] Permission denied: '...'} & Defensive checks + Environment handling — check permissions, skip restricted tests \\
    & File descriptor limits & \texttt{OSError: [Errno 24] Too many open files} & Environment handling — close files promptly, raise limits in CI \\
    & Symlink privileges & \texttt{OSError: symbolic link privilege not held: 'src' -> 'dst'} & Environment handling — detect support and use copies \\
    \midrule
    \textbf{SYS} &
    /proc filesystem & \texttt{FileNotFoundError: [Errno 2] No such file or directory: '/proc/...'} & Portable APIs — use psutil or platform APIs \\
    & Timezone database & \texttt{zoneinfo.\_common.ZoneInfoNotFoundError: 'No time zone found with key ...'} & Normalization — install tzdata or fallback \\
    \bottomrule
    \end{tabularx}
    \end{table*}   

\noindent Table~\ref{tab:symptoms} presents diagnostic symptom signatures mapped to portability sub-categories. Each row represents a specific sub-category within a broader root-cause category (\eg FILE). The columns show: (i) the root-cause category, (ii) the specific sub-category, (iii) verbatim error messages or symptoms when available, and (iv) the recommended general fix pattern. We find that not every sub-category produces a unique diagnostic signature; where concrete error messages exist, they reliably identify the underlying cause in test/CI logs (e.g., \texttt{AttributeError: module ``os'' has no attribute ``uname''}, \texttt{UnicodeDecodeError}). These signatures enable rapid triage and help developers quickly recognize portability issues in practice.

\subsubsection{\textbf{Answering \RQTwoThree{}}}

\noindent

Figure~\ref{fig:patterns} illustrates representative examples of fix patterns for portability issues, drawn from real-world projects. Each subfigure demonstrates a different repair strategy applied to actual code. The figure shows before (lines prefixed with \texttt{-}) and after (lines prefixed with \texttt{+}) versions of code snippets. The four patterns demonstrated are: (a) defensive checks using try/except for optional imports, (b) portable APIs replacing OS-specific constructs with cross-platform abstractions, (c) normalization using path separators correctly, and (d) environment handling using platform-aware test skipping.

Across projects, portability issues are resolved through four recurring strategies: \textbf{(i) Defensive checks} add guards, fallbacks, \texttt{try/except}, or conditional imports to handle missing APIs or modules gracefully (Listing~\ref{lst:defensive}); \textbf{(ii) Portable APIs} replace OS-specific constructs with cross-platform abstractions like \texttt{pathlib}, \texttt{platform}, or \texttt{psutil} (Listing~\ref{lst:portable}); \textbf{(iii) Normalization} standardizes encodings, line endings, paths, or filenames to prevent spurious differences (Listing~\ref{lst:normalization}); and \textbf{(iv) Environment handling} adapts code and tests to runtime conditions, such as skipping GUI tests in headless CI or managing platform-specific permissions (Listing~\ref{lst:environment}). 

Table~\ref{tab:symptoms} (in RQ2.2) maps each root-cause sub-category to its recommended fix pattern, showing how these four general strategies apply across the taxonomy. This mapping provides developers with actionable guidance: given a specific symptom or error message, the table indicates which fix pattern to apply.

\begin{tcolorbox}[boxrule=0pt,sharp corners,boxsep=2pt,left=2pt,right=2pt,top=2.5pt,bottom=2pt]
\textbf{RQ\textsubscript{2} (characteristics)}: Portability issues cluster around 7 root-cause categories, with File and Directories (FILE) being the most prevalent (60 projects). Most issues produce distinctive error signatures that enable quick diagnosis. Four general fix patterns—defensive checks, portable APIs, normalization, and environment handling—address the majority of observed portability problems. These patterns are simple, teachable, and applicable across different categories.
\end{tcolorbox}

\subsection{\textbf{Answering} \RQThree{}}
\label{rq3}


\textbf{Analysis of Static Tools.} To evaluate the effectiveness of static analysis tools in detecting Python portability issues, we systematically examined the rules and capabilities of several widely used linters and type checkers. Specifically, we considered Ruff~\cite{ruffref}, Flake8~\cite{flake8}, Pylint~\cite{pylint}, MyPy~\cite{mypy}, Bandit~\cite{bandit}, and Pyright~\cite{pyright}.

Our analysis revealed that, among these tools, only Ruff provides a rule that directly addresses potential portability concerns, namely the \texttt{unspecified-encoding} check~\cite{encruff} that maps to the ``Encoding'' subcategory within the ``File and Directories'' category. This rule helps identify cases where files are opened without explicitly specifying an encoding, which may lead to cross-platform inconsistencies. The remaining tools focus primarily on style, type correctness, security, and general code quality, without offering rules specifically designed to detect operating-system-dependent behaviors or other portability issues.

\textbf{Motivation for LLM Evaluation.}

Given the limited support for portability checks in existing static tools, we extended our evaluation to large language models (LLMs). We designed an experiment in which LLMs were presented with real Python code snippets exhibiting three distinct behaviors: (i)~non-portable code containing OS-specific constructs, (ii)~portable code that had been corrected to work across platforms, and (iii)~code unrelated to portability.
% 
Unrelated code was treated as portable for classification purposes, since such snippets do not exhibit any portability problems. Consequently, the dataset comprised 60 portable samples (30 corrected + 30 unrelated) and 30 non-portable samples. 
% 
This configuration reflects a realistic distribution of code seen in the wild, where most scripts are implicitly portable, and portability violations are less frequent but critical when present.


By comparing the models' predictions against the known classification of each snippet, we quantitatively assessed their ability to identify cross-platform issues. Specifically, we computed precision, recall, F1-score, and accuracy for each model. 
% 
These metrics provide an objective measure of how effectively LLMs can detect or reason about portability issues—an area where traditional static tools often fall short. This setup allows us to explore whether LLMs can complement static analyzers by offering probabilistic insights into potential cross-platform hazards. Table~\ref{tab:llm_performance} summarizes the performance of three evaluated LLMs: \texttt{llama-3.3}~\cite{llama3}, \texttt{Grok-4-Fast}~\cite{grok4}, and \texttt{GPT-4o-Mini}~\cite{gpt4o}. We used the API access provided by openrouter.ai~\cite{openrouter} to interact with the models. Each model was prompted with a consistent template that included the code snippet and a question asking whether the code was portable across the OSes. The models' responses were parsed to extract their classification decisions.

\begin{table}[h!]
\centering
\caption{Performance metrics for each evaluated LLM in detecting Python portability issues (90 code samples: 60 portable and 30 non-portable).}
\label{tab:llm_performance}
\begin{tabular}{lrrrrr}
\toprule
Model & Precision & Recall & F1-Score & Accuracy \\
\midrule
\texttt{llama-3.3} & 0.714 & 0.167 & 0.270 & 0.400 \\
\texttt{grok-4-fast} & 0.815 & 0.883 & 0.848 & 0.789 \\
\texttt{gpt-4o-mini} & 0.955 & 0.350 & 0.512 & 0.556 \\
\bottomrule
\end{tabular}
\end{table}

% \begin{table}[h!]
% \centering
% \caption{\label{tab:confusion_matrix_llama}Confusion matrix -- \texttt{llama-3.3}}
% \begin{tabular}{lcc}
% \toprule
% \textbf{Actual / Predicted} & \textbf{Port} & \textbf{NonPort} \\
% \midrule
% \textbf{Port}     & 10 & 50 \\
% \textbf{NonPort}  & 4  & 26 \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[h!]
% \centering

% \caption{\label{tab:confusion_matrix_grok}Confusion matrix -- \texttt{grok-4-fast}}
% \begin{tabular}{lcc}
% \toprule
% \textbf{Actual / Predicted} & \textbf{Port} & \textbf{NonPort} \\
% \midrule
% \textbf{Port}     & 53 & 7  \\
% \textbf{NonPort}  & 12 & 18 \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[h!]
% \centering

% \caption{\label{tab:confusion_matrix_gpt4o}Confusion matrix -- \texttt{gpt-4o-mini}}
% \begin{tabular}{lcc}
% \toprule
% \textbf{Actual / Predicted} & \textbf{Port} & \textbf{NonPort} \\
% \midrule
% \textbf{Port}     & 21 & 39 \\
% \textbf{NonPort}  & 1  & 29 \\
% \bottomrule
% \end{tabular}
% \end{table}

\begin{table*}[h!]
\centering
\caption{\label{tab:confusion_matrices_combined}Confusion matrices for all models}
\begin{tabular}{lccc|ccc|ccc}
\toprule
& \multicolumn{3}{c|}{\texttt{llama-3.3}} & \multicolumn{3}{c|}{\texttt{grok-4-fast}} & \multicolumn{3}{c}{\texttt{gpt-4o-mini}} \\
\textbf{Actual / Predicted} & Port & NonPort & & Port & NonPort & & Port & NonPort & \\
\midrule
\textbf{Port}     & 10 & 50 & & 53 & 7  & & 21 & 39 & \\
\textbf{NonPort}  & 4  & 26 & & 12 & 18 & & 1  & 29 & \\
\bottomrule
\end{tabular}
\end{table*}


The extended evaluation provides new insight into how different LLMs reason about software portability under a more diverse and balanced dataset. The inclusion of ``unrelated'' yet valid Python code introduces a subtle but important challenge: distinguishing the absence of portability issues from the presence of clear portability signals. This distinction is conceptually close to the notion of false alarms in static analysis and serves as a more realistic proxy for practical development scenarios.

The \texttt{llama-3.3} model achieved the lowest recall (0.167) and F1-score (0.270), despite maintaining moderate precision (0.714). This means that while Llama was occasionally correct when flagging code as non-portable, it failed to detect the majority of true portability-related cases. Its confusion matrix (Table~\ref{tab:confusion_matrices_combined}) reveals a clear conservative bias: most portability-related code—whether portable or not—was classified as non-portable. This under-detection behavior suggests that Llama relies heavily on surface-level heuristics, such as the appearance of system or file I/O APIs, rather than a deeper understanding of their context or cross-platform semantics. Such an approach limits its usefulness for nuanced code auditing, as it tends to overestimate risk and overlook safe but platform-agnostic constructs.

In contrast, \texttt{Grok-4-Fast} exhibited the most balanced and effective performance, achieving the highest recall (0.883), F1-score (0.848), and accuracy (0.789). Its confusion matrix (Table~\ref{tab:confusion_matrices_combined}) shows that Grok correctly classified the vast majority of both portable and non-portable samples, suggesting a strong generalization ability across different code types. However, its precision (0.815) indicates a mild tendency to over-classify snippets as portable, occasionally mislabeling risky constructs. This reflects an inclusive reasoning pattern—Grok tends to assume code is portable unless clear OS-specific cues are present. Such behavior may result from broader contextual generalization learned during instruction tuning, leading the model to associate “ordinary Python” with portability unless explicit evidence of platform dependence is detected.

The \texttt{GPT-4o-Mini} model achieved the highest precision (0.955), showing that its positive classifications were almost always correct. Nevertheless, its recall (0.350) and F1-score (0.512) reveal that it missed a large portion of portability-related cases. GPT-4o (confusion matrix \ref{tab:confusion_matrices_combined}) thus demonstrates an opposite inferential style to Grok: it classifies conservatively, only asserting portability when the evidence is unambiguous. This behavior mirrors risk-averse reasoning, where the model avoids false positives even at the cost of missing true positives.

In practical contexts, such cautious bias could be advantageous in high-stakes environments—e.g., when automated fixes or portability warnings must be highly reliable—but less useful for exploratory detection tasks aimed at coverage.

Overall, the three models exhibit distinct reasoning paradigms:
\begin{enumerate}
  \item \texttt{llama-3.3}: Conservative and shallow reasoning: prioritizes safety but lacks sensitivity, leading to under-detection.
  \item \texttt{Grok-4-Fast}: Inclusive and generalizing reasoning: prioritizes coverage and context recognition, achieving the best balance.
  \item \texttt{GPT-4o-Mini}: Selective and risk-averse reasoning: prioritizes correctness at the expense of recall.
\end{enumerate}


From a broader research standpoint, the results highlight both promise and limitation. On one hand, all models demonstrate a capacity for recognizing key portability indicators, confirming that they capture meaningful cross-platform cues such as file I/O encodings, path separators, and system-dependent API calls. On the other hand, none exhibit consistent sensitivity to more subtle forms of non-portability—like locale-dependent behavior, subprocess invocation nuances, or reliance on external command availability. These gaps reflect the probabilistic and context-limited nature of LLM reasoning in software domains.

\textbf{Answer.} \Fix{turn this into a box anwser:}Static tools (linters, CodeQL, LLM prompts) can detect some issues but miss runtime failures. Dynamic re-execution across OSes (our approach) provides broader coverage.

\textbf{LLM-Based Fixes.}

Following the classification experiment, we conducted a complementary evaluation focused on the corrective capabilities of LLMs. In this setting, we used the 30 non-portable Python snippets identified earlier and prompted each model with a generic instruction explicitly stating that the code exhibited portability issues and requesting a corrected version. The goal was to assess whether the models could not only recognize but also repair cross-platform defects by producing functionally portable code. We then manually verified whether the generated fixes effectively resolved the underlying portability problem without altering the intended semantics of the original program.

Each model thus attempted to repair 30 faulty code samples, yielding a total of 90 attempted fixes across all LLMs. Table~\ref{tab:llm_fix_results} summarizes the quantitative results. Overall, out of the 90 non-portable snippets, only 38 were correctly repaired, corresponding to an overall success rate of 42.22\%. This moderate performance highlights the challenge of producing accurate cross-platform corrections even when the model is explicitly informed that the input code is non-portable.

\begin{table}[h!]
\centering
\caption{Performance of LLMs in generating correct portability fixes.}
\label{tab:llm_fix_results}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{Correct} & \textbf{Incorrect} & \textbf{Accuracy} \\
\midrule
\texttt{grok-4-fast} & 14 & 16 & 0.47 \\
\texttt{gpt-4o-mini} & 13 & 17 & 0.43 \\
\texttt{llama-3.3} & 11 & 19 & 0.37 \\
\bottomrule
\end{tabular}
\end{table}

Among the evaluated models, \texttt{grok-4-fast} achieved the highest accuracy at 46.67\%, successfully repairing 14 of the 30 faulty snippets. Its results indicate that the model can often identify the source of non-portability and propose plausible corrections, such as replacing hard-coded path separators or adjusting file-opening modes for cross-platform compatibility. However, several fixes were only partially correct—resolving one issue while inadvertently introducing another—revealing that Grok's repair strategies remain heuristic rather than systematic.

\texttt{gpt-4o-mini} followed closely with an accuracy of 43.33\%. Its fixes tended to be more conservative, often preserving the structure of the original code while addressing specific portability issues. In some cases, GPT-4o correctly identified the problematic API usage but failed to modify it comprehensively, producing fixes that improved portability without fully resolving the problem. This pattern suggests a reasoning style that prioritizes minimal and precise edits, consistent with its high precision observed in the previous classification experiment.

The \texttt{llama-3.3} model obtained the lowest accuracy, correctly repairing 36.67\% of the snippets. In many instances, it produced code that appeared plausible but did not execute correctly on all platforms. The errors often involved overlooking subtle environment-dependent aspects, such as socket reuse semantics or filesystem path normalization. This indicates that while Llama demonstrates surface-level understanding of cross-platform issues, its capacity to generate effective end-to-end corrections is limited.

Overall, the results reveal that current LLMs can sometimes generate valid portability fixes but struggle to do so consistently. The best-performing model achieved success in fewer than half of the cases, underscoring that awareness of the issue (as expressed in the prompt) does not guarantee successful code repair. 
% 
The models appear to rely on pattern-based generalizations rather than reasoning through the operational implications of their modifications. These findings emphasize the need for prompt calibration, repair-specific fine-tuning, or hybrid approaches combining LLM reasoning with static verification tools to achieve reliable automated portability correction.

From a broader perspective, this experiment complements the earlier classification results by demonstrating that identifying portability problems and repairing them are distinct capabilities. 
% 
While classification relies on recognition of problematic patterns, effective repair requires the synthesis of correct, executable, and platform-agnostic alternatives—a substantially more demanding cognitive task for current LLMs. 
Nevertheless, the observed partial success confirms that LLMs possess a meaningful, if incomplete, grasp of cross-platform reasoning, marking an important step toward their integration into automated software maintenance workflows.



% Add here: quantitative comparison (coverage, precision, recall)
% For example: Figure 3 (Venn diagram of issues detected by static vs dynamic approaches)

\subsection{\textbf{Answering} \RQFour{}}
\label{sec:pr-discussion}
\subsubsection{\textbf{Answering} \RQFourOne{}}
\label{rq4-1}
 

% Add here: commit/PR analysis, with representative examples of code changes
% For example: Table 4 (Common fix patterns with frequency and examples)


\textbf{Quick Answer.} Fixes often involve replacing OS-specific APIs, adding conditional checks, or adopting cross-platform libraries.  

\subsubsection{\textbf{Answering} \RQFourTwo{}}
\label{rq4-2}


% Add here: time-to-fix statistics, issue/PR comment analysis
% For example: Figure 4 (Distribution of time-to-fix), Table 5 (Engagement metrics per project)

To validate our findings and contribute back to the open-source ecosystem, we submitted pull requests addressing the portability issues uncovered in our study. Our contributions focused on libraries where our analysis revealed systematic portability challenges, proposing fixes to improve cross-platform compatibility and robustness.

Table~\ref{tab:prs} summarizes the results of this effort. We submitted a total of \TotalPROpened{} pull requests (PRs) spanning 6 categories of portability issues, of which \TotalPRAccepted{} had been accepted at the time of writing. The overall acceptance rate of 33\%, along with a zero rejection rate, indicates that the issues we identified correspond to genuine portability problems recognized by library maintainers. Notably, ``File and Directories'' issues were the most frequent (18 PRs), reflecting the prevalence of path, encoding, and file handling inconsistencies across platforms. Conversely, ``Process and Signals'' issues exhibited the highest acceptance rate (100\%), likely due to their critical impact on application functionality.


From this number of PRs, we observe that the fix was applied in \TotalPRInTestsCode{} cases to the test code only, in \TotalPRInProgramCode{} cases to the program code only, and in \TotalPRInBothCode{} cases to both the test and program code. This distribution suggests that while some portability issues can be resolved by adjusting tests to be more environment-aware, many require changes to the application logic itself to ensure consistent behavior across platforms.

\begin{table*}[h!]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{2pt}
    \caption{\Fix{Ali, please update this table with the latest results.} Summary of Pull Requests (PRs) by issue type, showing opened, accepted, and rejected. The last row shows totals.}
    \label{tab:prs}
    % Creating a compact tabular environment
    \begin{tabular}{lccc}
        \toprule
        \textbf{Category} & \textbf{Opened} & \textbf{Accepted} & \textbf{Rejected} \\
        \midrule
        File and Directories & 18 & 6 & 0 \\
        API Availability & 12 & 2 & 0 \\
        Process and Signals & 2 & 2 & 0 \\
        System Information & 2 & 1 & 0 \\
        Library Dependencies & 1 & 1 & 0 \\
        Environment and Display & 1 & 0 & 0 \\
        \midrule
        \textbf{Total} & \textbf{\TotalPROpened} & \textbf{\TotalPRAccepted} & \textbf{\TotalPRRejected} \\
        \bottomrule
    \end{tabular}
\end{table*}

\textbf{Quick Answer.} Developers typically acknowledge portability problems but often give them lower priority than functional bugs, resulting in varied resolution times.  



\Den{Please review.}
To better understand how open-source developers and project communities react when portability issues are reported, we submitted pull requests (PRs) across different projects. The responses we received illustrate a spectrum of engagement patterns, from prompt acceptance to rejection or non-merge due to project inactivity.

% \cite{breds_pr_40}
In one case, the proposed change addressed a test portability issue arising from platform-specific file handling. The original test used Python's \texttt{tempfile.NamedTemporaryFile} without disabling automatic deletion, which led to failures on Windows due to how temporary files are closed and reopened. Our fix stored the path before closing the file and explicitly removed it after the test execution. The maintainer accepted the contribution, commenting: \emph{``Thanks for this.''}, 
in another PR, the maintainer expressed appreciation for the contribution and promptly merged the fix, stating: \emph{``Thanks a lot for this contribution! Good catch.''}
% \cite{python_doit_api_pr_9}. 
These responses highlight a positive and collaborative stance, where developers value external input and promptly integrate fixes that improve cross-platform compatibility.

% \cite{frequenz_pr_1261}
A different outcome emerged in a PR where we attempted to resolve a dependency portability issue. Specifically, the tests relied on the \texttt{zoneinfo} and \texttt{tzdata} modules, which are not consistently available across platforms. Our proposed patch included conditional installation of the dependency when running on Windows. However, the maintainer argued that the better solution was to avoid reliance on the library altogether, regardless of platform. As a result, the PR was reduced to removing the dependency and related tests. This case illustrates that communities sometimes prefer simplifying the codebase rather than maintaining platform-specific workarounds.

Finally, in another project, the response was shaped less by technical considerations and more by the project's state of maintenance. The maintainer acknowledged the validity of the patch but declined to merge it, noting: \emph{``I do somewhat view that project as archived and inactive... My concern about merging is that an updated last commit date would make the project seem more active than it really is.''} This case illustrates how project activity levels and community sustainability strongly influence whether portability improvements are accepted, even if the technical changes are sound.

Overall, these three cases demonstrate that responses to portability
contributions vary significantly: active projects may readily adopt
fixes, maintainers may prefer design simplifications over
platform-specific adjustments, and inactive projects may resist
merging altogether to avoid signaling misleading project vitality.

\section{Lessons Learned}
\label{sec:lessons}

\Den{What should developers, researchers, and tool builders keep in mind when thinking about portability issues in Python? Here are some suggestions:}
% - Practical guidelines distilled from study
% - Recommendations for writing portable Python code
% - Recommendations for tool designers and researchers
% - General lessons beyond Python (cross-platform SE insights)

% Not in a philosophical way, but in a practical way, what are the main takeaways from our study? 

% \begin{itemize}
%   \item \textbf{Portability is not automatic.} Even though Python is designed to be cross-platform, real projects still often break when run on different operating systems. Portability depends on how code and environments are actually used, not just what the language promises.
%   \item \textbf{Problems fall into repeatable categories.} The failures we found were not random. They fall into a small set of categories (like paths, encodings, missing APIs). This means we can focus research and tool support on the most common categories instead of treating each bug as unique.
% \end{itemize}

This section distills practical insights from our study for developers and researchers working with cross-platform Python code.

\noindent\textbf{Prioritize file and directory handling.}
File-related issues (FILE) were both the most common category in our taxonomy (59 instances) and the most frequent target of our pull requests (18 PRs). Path separators, line endings, encoding defaults, and file locking behavior vary significantly across platforms. Use cross-platform abstractions such as \texttt{pathlib} for paths and explicitly specify encoding when opening files. Avoid hardcoding path separators or making assumptions about filesystem case sensitivity.

\noindent\textbf{Process and signal issues are rare but critical.}
While process and signal-related problems (PROC) accounted for only 2
pull requests, they achieved a 100\% acceptance rate from
maintainers. When such issues occur, they typically have severe
functional impact—crashes, hangs, or incorrect behavior. Developers
should test subprocess invocation, signal handling, and process
management across platforms early in development, as these issues are
immediately recognized as critical by the community.

\section{Threats to Validity}
\label{sec:threats}
% - Internal validity: correctness of rerun framework
% - External validity: generalization beyond studied projects
% - Construct validity: definitions of “portability” and categorization
% - Conclusion validity: statistical soundness, possible biases

\Fix{one thing we remember is that we do not explore other versions of OSs}


\section{Related Work}
\label{sec:related}
% - Prior work on cross-platform failures, portability in SE
% - Studies on testing across operating systems
% - Tool support for detecting environment-specific issues
% - Positioning of this work relative to literature

\Fix{Talk about} \cite{vahabzadeh2015empirical} here. Others: \cite{rasley2015detecting, ghandorh2020systematic}

Maybe: \cite{sun2016bear}. Mobile apps testing: \cite{boushehrinejadmoradi2015testing}

\section{Conclusions}
\label{sec:conclusions}
% - Recap of findings for each RQ
% - Key contributions
% - Long-term impact on research and practice
% - Future directions

%% \section*{Data Availability}

%% The artifacts -- including datasets and scripts -- are publicly available~\cite{ourrepo}.

\onecolumn \begin{multicols}{2}
%\balance
\bibliographystyle{ACM-Reference-Format}
%% \bibliography{references}
\bibliography{ref}
\end{multicols}

\end{document}
