% \documentclass[acmsmall,screen,review]{acmart} % OOPSLA
% \documentclass[sigconf,review,anonymous]{acmart} % MSR
\documentclass[sigconf]{acmart} % MSR

% --- Author 1 (NC State) ---
\author{Author One}
\affiliation{%
  \institution{North Carolina State University}
  \city{Raleigh}
  \state{NC}
  \country{USA}
}
\email{author1@ncsu.edu}

% --- Author 2 (NC State) ---
\author{Author Two}
\affiliation{%
  \institution{North Carolina State University}
  \city{Raleigh}
  \state{NC}
  \country{USA}
}
\email{author2@ncsu.edu}

% --- Author 3 (UFPE, Brazil) ---
\author{Author Three}
\affiliation{%
  \institution{Federal University of Pernambuco}
  \city{Recife}
  \state{PE}
  \country{Brazil}
}
\email{author3@ufpe.br}


%%%%%
%% We will need to add this in camera-ready
%%%%%
%% \setcopyright{none} % to remove the copyright notice
%% \settopmatter{printacmref=false} % to remove the ACM Reference Format
%% \renewcommand\footnotetextcopyrightpermission[1]{} % to remove copyright box
\usepackage{comment}

\usepackage{lipsum}
\input{macros}

%%%%%%%%%%%%%%
%% remove copyright, acm reference, 
%% add page numbers
%%
%% comment for camera ready -Marcelo
%%%%%%%%%%%%%%
\setcopyright{none}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\begin{document}

\title{\PaperTitle{}}

\begin{abstract}
  \sloppy
  While Python is designed as a cross-platform language,
real-world applications encounter portability failures
when deployed across different operating systems.
We present the first large-scale empirical study of
cross-OS portability issues in Python, analyzing \NumProjectsTotal{}
open-source repositories using two complementary approaches:
systematic cross-OS test re-execution and manual analysis
of GitHub issues. Our cross-platform testing of
\NumProjectsReExecution{} projects reveals that
\PercentProjectsWithPortabilityReExecution{} exhibit
OS-dependent test failures.
%% , with \NumTestRunsWithOSDiff{}
%% divergent outcomes across \NumTestsExecuted{} test executions.
Through systematic analysis of \NumIssuesAnalyzed{} GitHub issues,
we confirm \NumIssuesRelated{} genuine portability problems
spanning \NumProjectsWithOSDiffInIssues{} additional projects.
We develop a comprehensive taxonomy identifying
\NumRootCauseCategories{} primary failure categories—with
file/directory operations, process management, and library dependencies
being most prevalent—along with \NumRootCauseSubcategories{} 
distinct sub-categories, \NumSymptomSignatures{} diagnostic signatures, 
and \NumFixPatterns{} systematic repair patterns. Our evaluation reveals
that existing static analysis tools provide minimal support for
portability detection, while large language models achieve 40--79\% 
accuracy in identifying issues and 50--77\% success in generating fixes
when provided with structured guidance. Through \TotalPROpened{} contributed pull requests,
we demonstrate practical applicability and developer acceptance 
(\TotalPRAccepted{} merged, zero rejected) of our findings. This work establishes
the first comprehensive baseline for understanding and addressing
cross-OS portability issues in Python, providing actionable insights
for developers, tool designers, and the broader research community. 
\end{abstract}


%%%%%%%
%%% Uncomment for camera ready -Marcelo
%%%%%%%

%% \begin{CCSXML}
%% <ccs2012>
%%    <concept>
%%        <concept_id>10011007.10011074.10011099</concept_id>
%%        <concept_desc>Software and its engineering~Software verification and validation</concept_desc>
%%        <concept_significance>500</concept_significance>
%%    </concept>
%%    <concept>
%%        <concept_id>10011007.10010940.10010992.10010993.10010994</concept_id>
%%        <concept_desc>Software and its engineering~Functionality</concept_desc>
%%        <concept_significance>500</concept_significance>
%%    </concept>
%% </ccs2012>
%% \end{CCSXML}

%% \ccsdesc[500]{Software and its engineering~Software verification and validation}
%% \ccsdesc[500]{Software and its engineering~Functionality}

%% \keywords{Mining Software Repositories, Python, Cross-Platform, Portability, Test Re-execution, Repair Patterns}

\maketitle


\section{Introduction}
\label{sec:intro}
\label{sec:sources}


A \emph{portability issue} refers to an observable difference in a
program’s behavior that arises from variations in the underlying
platform—for example, inconsistencies in API implementations across
platforms.
%or platform-specific functionality.
Python is widely regarded as a cross-platform language, powering
applications across diverse domains such as data science, web
development, and automation. It consistently ranks among the most
popular programming languages
today~\cite{stackoverflow2025,githuboctoverse2024,pypl2025}. However,
platform portability is not guaranteed. Mince et al.~\cite{mince2023},
for instance, reported that roughly 40\% of the functionality in
popular machine learning libraries (\eg{},
PyTorch~\cite{paszke2019pytorch}) behaves inconsistently across
hardware platforms such as CPUs and TPUs. Similar challenges occur at
the operating-system level; for example, the function
\CodeIn{os.geteuid()} is unavailable in Windows distributions of
Python~\cite{pythongeteuid}.


Portability issues in software projects often arise from dependencies
on platform-specific APIs, system libraries, and environment
assumptions. These issues commonly lead to deployment failures when
software is executed in environments differing from those used during
development or testing. Consequently, developers must spend
substantial time diagnosing and adapting code, which negatively
impacts productivity and delays releases. Prior work has shown that
the availability and usage of platform-specific APIs can directly
affect the portability and reliability of software
systems~\cite{job2024}. Industry reports further emphasize that
ensuring code portability requires deliberate cross-platform design,
testing, and maintenance
practices~\cite{kiuwancodeportability}. Systematic investigations
indicate that defining and quantifying portability remains a
persistent challenge in software engineering, with no widely adopted
metrics or automated measurement
frameworks~\cite{ghandorh2020systematic}. Moreover, empirical studies
have connected portability-related inconsistencies to unstable or
flaky test behavior, complicating continuous integration pipelines and
further burdening
developers~\cite{vahabzadeh2015empirical,flaky_tests_empirical}.
%
Container technologies such as Docker~\cite{docker} and Singularity
(now Apptainer)~\cite{apptainer} can address software portability
challenges by encapsulating code, dependencies, and execution
environments into reproducible units. They minimize configuration
discrepancies across platforms and facilitate consistent testing and
deployment. Nevertheless, despite the clear benefits of
containerization, most open-source Python projects remain
non-containerized. In our analysis of \NumProjectsTotal{}
\github\ repositories, we find that only 19.9\% included a Dockerfile
or Docker Compose configuration.

%% Portability issues in software projects leads to failed deployments
%% and reduces developer's
%% productivity~\cite{vahabzadeh2015empirical,sun2016bear,ghandorh2020systematic,kiuwancodeportability,job2024}.

%% A \emph{portability issue} is an observable difference in the
%% execution of a project caused by the underlying platform, \eg{},
%% differences in API implementations that are API specific. Python is
%% widely regarded as cross-platform, powering applications from diverse
%% domains (e.g., data science pipelines, web applications, etc.) and
%% consistently ranks among the most popular programming languages of
%% today~\cite{stackoverflow2025,githuboctoverse2024,pypl2025}. Yet,
%% platform portability is not a guarantee. For example, Mince
%% \etal{}~\cite{mince2023} found that 40\% of the functionality from ML
%% Python libraries (\eg{}, \jax, \torch, and \tf) does not perform
%% consistently across different hardware (\eg{}, CPU and TPU). Similar
%% problem exists at the OS level.  For example, the Python function
%% \texttt{os.geteuid()} is unavailable on Python distributions for
%% Windows. Non-portable code is an important problem; it results in
%% failed deployments and disrupts develpment
%% productivity. Container
%% technology (e.g., Docker and Singularity/Apptainer) can mitigate this
%% problem but we find that most open source projects are not
%% containerized, i.e., they lack Dockerfiles or Singularity recipes to
%% create images. Only 19.9\% of \NumProjects{} \github projects in
%% Python we analyze contain Dockerfile or Docker compose configurations.

%% While
%% Python's design philosophy emphasizes portability through standardized
%% interfaces in modules like \texttt{os}~\cite{pymotwos,geeksos2025},
%% real-world codebases routinely contain platform-specific assumptions
%% that render them fragile across operating systems.
%% (e.g., broken
%% continuous integration, increased maintenance overhead, and broader
%% implications like 
%% These failures manifest in
%% predictable ways: missing system calls on Windows
%% (e.g., \texttt{os.geteuid()}) \cite{pythonos315}, GUI-dependent code
%% failing in headless environments, filesystem path incompatibilities,
%% and dynamic library loading issues that vary by
%% platform \Fix{Cite}. What appears to be a language-level guarantee of
%% portability dissolves under the weight of actual usage patterns and
%% environmental dependencies~\cite{peng2024,job2024}.


%%% This is way too long (distracting) to add in an introduction.
%%%
%% Our analysis of \NumProjects{} Python projects reveals significant insights regarding containerization adoption in the open-source ecosystem. Among these projects, only 345 utilize Docker through either Dockerfiles or Docker Compose configurations, representing approximately 16.9\% of the total dataset. This relatively low adoption rate highlights a critical concern: the vast majority of Python projects (83.1\%) remain potentially vulnerable to portability issues. Projects without containerization face inherent challenges in maintaining consistent execution environments across different systems, dependency management complications, and deployment reproducibility problems. The absence of Docker containerization means that these projects rely heavily on local environment configurations, Python version compatibility, and system-specific dependencies, which can lead to the infamous ``it works on my machine'' syndrome. This portability gap becomes particularly problematic when projects need to be deployed across different operating systems, cloud platforms, or when team members work with varying development setups, ultimately hindering collaboration and increasing the likelihood of environment-related bugs and deployment failures.

%% This fragility is not merely a theoretical concern. Modern software
%% development increasingly relies on cross-platform deployment, from
%% containerized applications that must run consistently across
%% different host systems to open-source projects that serve diverse
%% user communities. When portability breaks, it creates cascading
%% problems: delayed releases, increased maintenance burden, and
%% reduced software accessibility.


\sloppy
This paper presents a study of cross-OS portability issues in Python
projects. Specifically, we analyze \NumProjectsTotal{} repositories to assess the prevalence of such issues,
their characteristics (e.g., symptoms, root causes, and fixes), the
effectiveness of existing tools (e.g., static analyzers and large
language models) in detecting and repairing them, and developers'
reactions to the \TotalPROpened{} pull requests we submitted to address portability
issues in their code. Our analysis reveals that these
portability failures, while widespread (\PercentProjectsWithPortabilityReExecution{} of tested projects), are not chaotic or
unpredictable. Instead, they cluster around \NumRootCauseCategories{} main root-cause categories and \NumRootCauseSubcategories{} 
well-defined sub-categories—with file and directory operations, process management, and library dependencies
being most prevalent. Results indicate
that a combination of test re-execution and LLMs can complementarily 
detect issues, with LLMs achieving 40--79\% accuracy in detection and 50--77\% success in repair when provided with structured guidance.
We conclude with a set of lessons we learned.

This paper makes the following contributions:
\begin{packed_itemize}
  \item[\Contrib] A study about the prevalence of portability issues that use
    two complementary methods to identify issues: test re-execution
    and mining of issues with subsequent manual analysis;
  \item[\Contrib] A detailed categorization of symptoms, root causes, and fix
    patterns of portability issues;
  \item[\Contrib] An evaluation of the ability of static analysis and LLMs to detect and repair portability issues;
  \item[\Contrib] An evaluation about the reaction of developers about the pull
    requests we open to fix portability issues in their
    \github\ projects;
  \item[\Contrib] A dataset of portability issues across \NumProjectsTotal{}
    Python projects, including test re-execution results, GitHub issues, 
    code examples, and LLM evaluation data.
\end{packed_itemize}

Our artifacts, including code and data, are publicly available at \ourrepo{}.

\section{Projects and Questions}
\label{sec:methodology}

This section describes the projects we used (\ref{sec:projects}) the
research questions we posed (\ref{sec:questions}).

\subsection{Projects}
\label{sec:projects}

\noindent
\textbf{Dataset.} We sampled \NumProjectsTotal{} Python repositories from
\github. We selected projects using \github{}'s REST API, focusing on
repositories primarily implemented in Python that show evidence of
testing activity (e.g., use of pytest or unittest), have at least one
star, and contain at least one test case that can be executed
successfully. This filtering ensures that the analyzed projects are
active, testable, and representative of real-world Python software.

\textbf{Dataset partitioning.} To support different aspects of our study,
we partitioned the dataset into two complementary subsets. First, we
allocated \NumProjectsLearning{} projects to a \emph{learning set} used
for identifying and categorizing portability issues through detailed
analysis. This learning set was further divided based on the detection
method employed: \NumProjectsReExecution{} projects were analyzed through
cross-OS test re-execution, while \NumProjectsIssueMining{} projects were 
analyzed through systematic mining of \github{} issues and pull
requests; Section~\ref{sec:methods} describes these methods. These two subsets 
are non-overlapping, ensuring independent observations across detection 
methods. The remaining \NumProjectsApplication{} projects constitute an 
\emph{application set}, used to validate our findings by submitting pull 
requests that address identified portability issues; 
Section~\ref{sec:pr-discussion} elaborates the validation stage.

\begin{table}[H]

\centering
\caption{Stats on \NumProjectsTotal{} evaluated projects: no. of tests (\#Tests), size (SLOC), no. of commits (\#Sha), age in years (Age), no. of \github{} stars ($\star$).\label{tab:project-stats}}
\begin{tabular}{lrrrrr}
\hline
Statistic & \#Tests & SLOC & \#Sha & Age & $\star$ \\
\hline
Mean & 572.51 & 41,635.39 & 2,883.25 & 6.24 & 6,147.88 \\
Median & 72 & 7,478 & 426.50 & 5.87 & 83.50 \\
Min & 1 & 2 & 2 & 0.3 & 2 \\
Max & 36,588 & 3,237,895 & 194,153 & 17.27 & 366,472 \\
Sum & 1,169,056 & 85,019,457 & - & - & - \\
\hline
\end{tabular}
\end{table}


Table~\ref{tab:project-stats} summarizes key characteristics of the
\NumProjectsTotal{} evaluated projects.  On average, each project comprises
approximately 573 tests and 41.6K lines of Python code (SLOC), with a
median of 72 tests and 7.5K SLOC, indicating a distribution skewed by
a few very large systems. Regarding repository activity, projects have
on average 2.9K commits, a median of 427, and span a mean lifetime of
6.2 years. The number of \github{} stars varies widely, with an
average of 6.1K, a median of only 84, and a maximum exceeding 366K,
reflecting the presence of both niche and highly popular projects.

Overall, the dataset covers a diverse range of software systems, from
small-scale repositories with only a handful of commits to large,
long-lived, and widely adopted projects.


\subsection{Research Questions and Methodology}
\label{sec:questions}

We aim to answer the following key research questions:

\sloppy \MyPara{\RQOne{}}~This question aims to assess how frequently
portability problems occur in practice. If such issues are very rare,
or if developers generally disregard them, the study’s practical value
would be limited. To address this question, we analyze issue reports
and execute tests in the wild across multiple virtual machines.
%% Although
%% Python is advertised as a cross-platform language, our preliminary
%% results suggest that many tests fail when run on different operating
%% systems. By quantifying the frequency of such issues across a
%% representative set of projects and applying statistical tests, we can
%% provide stronger evidence on whether portability is a widespread
%% concern in the Python ecosystem or only affects a small subset of
%% projects.


% \textbf{Methodology.} \Fix{Describe here the methodology to answer RQ1.}

\MyPara{\RQTwo{}}~Assuming that identifying and
resolving portability issues is an important problem, it is also
essential to understand how these issues manifest (i.e., their
symptoms), what their root causes are, and how they are typically
repaired. For instance, automated solutions become more feasible when
a small number of recurring fix patterns can be observed and documented.

%% Assuming problem of finding and
%% fixing portability issues is important, it is also important to
%% understand manifestations (\ie{}, symptoms), the root-causes of
%% portability issues, and also how they are typically repaired. For
%% example, automated solutions are more viable when a small set of fix
%% patterns exists.
% \textbf{Rationale.} This question seeks to identify and categorize the underlying reasons why portability issues arise. Our observations indicate that differences in modules such as os, file system conventions, path handling, and environment-specific assumptions may contribute to failures. In addition, some dependencies work properly on one operating system but not on another, further aggravating the problem. Understanding these root-causes is essential not only to characterize the problem but also to guide the development of best practices and tools to prevent such issues in the future.  
% \textbf{Rationale.} Recognizing diagnostic signatures helps developers quickly triage failures and identify the underlying portability issue from test logs or CI output. While not all issues produce unique error messages, many exhibit specific, reproducible patterns that enable rapid diagnosis.
%% \noindent\textit{\RQTwoOne{}}
%% \noindent\textit{\RQTwoTwo{}}
%% \noindent\textit{\RQTwoThree{}}
% \textbf{Methodology.} \Fix{Describe here the methodology to answer RQ2}

\MyPara{\RQThree{}}~This question examines the extent to which
existing tools (e.g., static analyzers and LLMs) can detect and fix
portability problems. Re-executing test suites across different OSes
(e.g., through virtual machines) can reliably identify portability
issues, but it is limited by the ability of the existing tests to
exercise the affected code locations.

%% Static tools, including
%% linters, type checkers, CodeQL queries, and even large language
%% models, can flag certain categories of issues, such as OS-dependent
%% APIs or unsafe path handling. However, many portability issues only
%% manifest at runtime, for example due to environment assumptions,
%% platform-specific encodings, or dependency incompatibilities. To
%% complement static checks, we employ a dynamic strategy—our ``rerun''
%% approach—that executes tests across multiple operating systems to
%% reveal failures that static analysis cannot anticipate. By comparing
%% the coverage of static and dynamic techniques, we can assess their
%% effectiveness and identify opportunities for improving tool support in
%% cross-platform Python development.
% \textbf{Methodology.} \Fix{Describe here the methodology to answer RQ3}


\MyPara{\RQFour{}}~This question aims to further understand technical
and social dimensions of portability issues, namely, what are the
common fixes developers employ and how developers react to pull
requests we submit to fix portability issues in their projects.

%% \noindent\textit{\RQFourOne{}}
%% \noindent\textit{\RQFourTwo{}}
% \textbf{Methodology.} \Fix{Describe here the methodology to answer RQ4}



\section{Results}
\label{sec:results}
% This section presents the empirical results of our study,
% organized around the research questions (RQ1–RQ4.2).
% For each RQ, we restate the rationale, present the findings,
% and provide a concise quick answer.

This section provides answers to the posed research questions.

\subsection{\textbf{Answering} \RQOne{}}
\label{rq1}

% add here: statistics, tables, and plots showing the frequency and distribution of portability issues
% for example: Table 1 (Projects analyzed, tests executed, failures observed), Figure 1 (Distribution of failures by OS)

\begin{table}[t!]
  \centering
  \caption{\label{tab:dataset}Prevalence of portability problems.}
  \vspace{-1ex}
  \begin{subtable}[t]{\linewidth}
    \centering
    \caption{\label{tab:dataset-a}Problems detected with test re-execution.}
    \begin{tabular}{lr}
    \toprule
%     Number of projects mined & \NumProjects{} \\
     Number of projects selected & \NumProjectsReExecution{} \\
    \hspace{2ex} Number of projects with OS differences & \NumProjectsWithOSDiffInTests{} \\    
     Number of tests executed & \NumTestsExecuted{} \\
    \hspace{2ex} Number of test runs with OS differences & \NumTestRunsWithOSDiff{} \\
    % Number of differences analyzed & 2,421 \\
    \bottomrule
    \end{tabular}
  \vspace{1ex} % space between subtables    
  \end{subtable}
  \begin{subtable}[t]{\linewidth}
    \centering
    \caption{\label{tab:dataset-b}Problems detected in issues with manual inspection.}
    \begin{tabular}{lr}
      \toprule
       Number of projects selected & \NumProjectsIssueMining{} \\
      \hspace{2ex} Number of projects with OS differences & \NumProjectsWithOSDiffInIssues{} \\      
       Number of issues mined & \NumIssuesMined{} \\
      \hspace{2ex} Number of issues analyzed & \NumIssuesAnalyzed{} \\
      \hspace{4ex} Number of issues documenting OS differences & \NumIssuesRelated{}\\
      \bottomrule
    \end{tabular}
  \end{subtable}
\end{table}


%% \Ali{Please review updates: (smaller paragraphs, remove the 100
%%   percentage accuracy of re-execution method, add example of issue
%%   mining findings, ...)}

\subsubsection{Methods}
\label{sec:methods}

We use two complementary methods to identify portability issues. The 
first method, \emph{cross-OS test re-execution}, systematically reveals behavioral 
differences across platforms but it is inherently incomplete as tests may 
not cover important code paths. The second method, \emph{issue mining}, 
complements test re-execution by including developer perspectives and 
discussions, though it is costly as it requires manual analysis of 
discussion threads to rule out spurious cases. We discuss both methods 
in detail below.

\textbf{Cross-OS test re-execution.} For each project we run the test suite
on multiple operating systems and record divergent outcomes. We then
triage failures via logs and small code inspections, mapping each
instance to a concrete category (i.e., a classification of the root cause 
of the portability issue, such as file handling, missing APIs, or library 
dependencies; the full taxonomy is presented in answering \RQTwo{}). We 
execute tests on ephemeral virtual machines provided by GitHub-hosted
runners~\cite{github_actions_runners}. Each virtual machine is
automatically provisioned with 4~CPUs and 16~GB of RAM. We implemented
a single GitHub Actions workflow (YAML) that uses a matrix strategy to
run the test-suite on Ubuntu 24.04 LTS, macOS 15, and Windows Server
2025 (each job runs in a fresh VM image). Each job installs
dependencies, runs the Python tests with
\texttt{pytest}~\cite{pytest_tool} and the \texttt{pytest-json-report}
plugin~\cite{pytest_json_report} to create JSON reports, and finally
uploads the JSON files as workflow artifacts for later analysis. This
approach ensures reproducible, parallel cross-platform testing while
GitHub manages VM provisioning and maintenance.

\textbf{Issues Mining.} To complement test-based findings, we selected
\NumProjectsIssueMining{} projects from the learning set and mined
\github{} issues and pull requests from them. We wrote a ``candidate finder''
combining multiple \emph{types} of keyword:  
(a) OS/platform indicators (e.g., Windows, Linux, macOS, specific distros/architectures),  
(b) failure/fix language (e.g., fails, error, bug, fix, workaround),  
(c) testing/CI context (pytest, CI, \github{} Actions), and  
(d) common portability causes (e.g., path separators, chmod/permissions, encodings/UTF-8, dynamic libraries like \texttt{.dll}/\texttt{.so}).  
%
We combine these keywords with ``OR'' within each type and ``AND''
across types. We searched titles, bodies, and comments, with extra
weight to title and close-proximity matches. We employ a lightweight
triage (brief summaries and negative filters for off-topic mentions)
to filter spurious results. Candidate issues were normalized into
consistent records (project, link, date, summary, compact tags like
OS=, FIX=, TEST=, CAUSE=), duplicates removed, and full text (title,
description, comments) archived for analysis.

% \Mar{HEY, this is not what you are doing. You need to explain how you
% used \github{} Actions/Cloud to create VMs for different OSes.}
%\Den{Updated: Please check/revise!}

% \Den{I don't know if it's necessary and I also don't know if the text is clear enough. Please check/revise!}
% All experiments were conducted across three operating systems to ensure compatibility. The Linux setup used an AMD EPYC 7763 64-Core Processor with 4 CPUs, 16 GB RAM, and Ubuntu 24.04 LTS.

% The macOS setup ran on an Apple M2 Pro chip (Arm64) with 5 CPU
% cores, 8 GPU cores, 16 GB RAM, and macOS 15. The Windows setup used
% an AMD EPYC 7763 64-Core Processor with 4 CPUs, 16 GB RAM, and
% Windows Server 2025.

\subsubsection{Results}

We applied both methods described in Section~\ref{sec:methods} to the 
learning set. Table~\ref{tab:dataset} reports on the prevalence of 
portability problems in \github{} Python projects. 
Table~\ref{tab:dataset-a} details results from test re-execution, while
Table~\ref{tab:dataset-b} details results from issue mining. 

Table~\ref{tab:dataset-a} shows that of the \NumProjectsReExecution{} projects 
analyzed, we found discrepant behavior in \NumProjectsWithOSDiffInTests{} 
(\PercentProjectsWithPortabilityReExecution{}) of them, \ie{}, projects where 
test suites had at least one test manifesting OS differences. These projects 
included a total of \NumTestsExecuted{} test cases, of which \NumTestRunsWithOSDiff{} 
showed discrepant outcomes, corresponding to \PercentDiscrepantTests{}. 
%% , due to the 
%% resource-intensive nature of cross-platform test execution across
%% three operating systems.
%% Recall from Section~\ref{sec:methods} that each test suite is executed
%% on three different virtual machines and the outcomes of each test
%% cases is compared for discrepancy.

%% The test re-execution method
%% provides a systematic and reliable approach to detecting portability
%% issues by directly observing behavioral differences across
%% platforms.
%% . Due to the inherent variability of natural
%% language and the need to interpret context, we manually analyze the
%% discussion threads of the issues to confirm whether they indicate real
%% portability issues.

Table~\ref{tab:dataset-b} shows the results from issue mining. 
From the \NumProjectsIssueMining{} projects selected (see Section~\ref{sec:methods}), 
we mined a total of \NumIssuesMined{} candidate issues. From this set, we randomly 
sampled \NumIssuesAnalyzed{} of the most recent issues for manual inspection. 
Through careful manual analysis, we confirmed that \NumIssuesRelated{} of 
these were genuine portability issues, spanning \NumProjectsWithOSDiffInIssues{} 
distinct projects. Combined with the test re-execution results, we identified
\textbf{\TotalProjectsWithPortabilityIssues{}} Python projects with portability issues 
considering both methods.

%% \Ali{Explained where 95 distinct projects comes from.}
%% \Ali{Moved methodology discussion to RQ2 and added Table 4 showing data sources for taxonomy components.}

%\Mar{I like this discussion, but it does not belong here. The question
%is about prevalence and this text is about analysis of root causes,
%symptoms, etc. It seems to belong to the next RQs. That said, I notice
%you do not mention if the (sub)category of root cause, symptom, and
%fix originate from test re-execution or issue mining. A table showing
%(1) only re-execution, (2) only mining, (3) both with rows for the
%three cases (causes, symptom, fixes) would be ``good enough''.}


%% demonstrating that portability problems 
%% extend beyond our test execution sample and affect a broader range of 
%% the Python ecosystem.

\begin{tcolorbox}[boxrule=0pt,sharp corners,boxsep=2pt,left=2pt,right=2pt,top=2.5pt,bottom=2pt]
\textbf{RQ\textsubscript{1} (prevalence)}: We find that portability
issues are relatively prevalent. For example,
\PercentProjectsWithPortabilityReExecution{} of the
\NumProjectsReExecution{} projects we analyzed with cross-OS test
re-execution have portability problems.  Additionally, our issue
mining approach shows that \NumProjectsWithOSDiffInIssues{} of the
projects we analyzed have genuine portability issues.
\end{tcolorbox}

\subsection{\textbf{Answering} \RQTwo{}}

%% : test re-execution
%% (\NumTestRunsWithOSDiff{} test differences) and issue mining
%% (\NumIssuesRelated{} confirmed issues with a 42.5\% confirmation
%% rate). 
%% For instance, through issue mining we found cases\Mar{more
%%   than one?} where a project's tutorial failed on Windows due to
%% platform-incompatible dependencies, prompting maintainer discussions
%% about whether to make dependencies optional or create
%% platform-specific variants—decision-making processes and untested code
%% paths that test re-execution alone cannot
%% capture~\cite{pyearthtools-issue82}.\Mar{Can you show an excerpt of
%%   this in a listing/figure to make it more tangible?}

\subsubsection{Method}~We conducted a qualitative study to categorize
the root causes, symptoms, and the fixes of portability problems.

%% We
%% used two different methods reflecting the methods used to identify the
%% portability problems~(Section~\ref{sec:methods}).

We used cross-OS test re-execution and the analysis of issues to find
root causes and symptoms. For fixes, we needed to rely on the
discussion in the issue and the corresponding fix commits. In cross-OS
test re-execution, we analyzed error messages, stack traces, and the
failing test code to understand the root causes and symptoms. For
issue mining, we analyzed issue titles, descriptions, developer
discussions, and comments to understand the root causes and symptoms.
Through iterative open coding~\cite{strauss1990basics} across both
data sources, we identified recurring patterns and grouped instances
with similar underlying causes. 
The coding process was conducted independently 
by two co-authors, with regular discussion to resolve 
ambiguities and refine category definitions. To ensure reliability, we computed 
inter-coder agreement on a subset of the data, achieving 96.4\% agreement with a 
Cohen's kappa of 0.89, reflecting almost perfect inter-coder reliability. During the later phases of the coding process,
new issues no longer produced new categories, indicating that the taxonomy 
had reached theoretical saturation. 

The resulting taxonomy of root causes consists of \textbf{\NumRootCauseCategories{}}
high-level categories (e.g., FILE, PROC, LIB) with \textbf{\NumRootCauseSubcategories{}}
distinct sub-categories. For each instance, we documented observable
symptoms (error messages, behaviors) and examined corresponding fixes
in merged pull requests or commits to identify common repair
patterns. We identified \textbf{\NumSymptomSignatures{}} unique symptom signatures and 
\textbf{\NumContextSymptoms{}} context-dependent symptoms across the \NumRootCauseSubcategories{} 
sub-categories, and \textbf{\NumFixPatterns{}} general fix patterns that collectively 
address all observed portability problems. Table~\ref{tab:data_sources} summarizes the 
primary data sources used to identify each component of our taxonomy. Root-cause
categories were derived from patterns observed across both test
failures and issue discussions.  Symptom signatures were primarily
extracted from test re-execution, where concrete error messages and
stack traces provide direct evidence of portability problems. Fix
patterns were identified by examining code changes and discussions in
both test-related commits and issue-related pull requests.

\begin{table}[h!]
\centering
\caption{Data sources for portability taxonomy components.}
\label{tab:data_sources}
\begin{tabular}{lcc}
\toprule
\textbf{Component} & \textbf{Test Re-execution} & \textbf{Issue Mining} \\
\midrule
Root-cause categories & \cmark & \cmark \\
Symptoms & \cmark & \xmark \\
Fix patterns & \cmark & \cmark \\
\bottomrule
\end{tabular}
\end{table}

In the following subsections, we elaborate on the characteristics of
 portability problems: root-causes, symptoms, and fixes.

\subsubsection{\textbf{Answering \RQTwoOne{}}}
\label{rq2-1}

~

\noindent Table~\ref{tab:categories} presents our taxonomy of portability issues 
organized by root-causes. Each row groups related sub-categories
under a main category. The columns show: (i) the root-cause category name and acronym,
(ii) specific sub-categories within that category,
(iii) a brief description of each sub-category,
(iv) the number of distinct projects where the issue appeared in test execution,
(v) the number of distinct projects where the issue appeared in mined \github{} issues, and 
(vi) the total count across both sources. 

% ========================= CATEGORIES TABLE =========================
\begin{table*}[t]
    \centering
    \caption{Taxonomy of portability issues showing root-cause categories, sub-categories, brief descriptions, and number of distinct projects observed in test execution and mined \github{} issues.}
    \label{tab:categories}
    \scriptsize
    \setlength{\tabcolsep}{3pt}
    \renewcommand{\arraystretch}{1.15}
    \begin{tabularx}{\textwidth}{
    >{\raggedright\arraybackslash}p{0.18\textwidth}
    >{\raggedright\arraybackslash}X
    >{\raggedright\arraybackslash}p{0.23\textwidth}
    >{\centering\arraybackslash}m{0.08\textwidth}
    >{\centering\arraybackslash}m{0.08\textwidth}
    >{\centering\arraybackslash}m{0.07\textwidth}
}
    \toprule
    \textbf{Root-cause category (Acronym)} & \textbf{Sub-categories} & \textbf{Description} & \textbf{\# of projects in tests } & \textbf{\# of projects in issues} & \textbf{Total} \\
    \midrule
    
    \multirow[t]{6}{*}{File and Directories (FILE)} 
    & Path separators & Forward vs. backslash differences & 7 & 27 & \multirow{6}{*}{62} \\
    & Line endings & CRLF vs. LF mismatch & 1  & 5 &                      \\
    & Case sensitivity & Filename case handling varies & 1  & 0 &                      \\
    & File locking & Windows locks open files & 7 & 4 &                      \\
    & Encoding & UTF-8 not default everywhere & 2 & 7 &                      \\
    & Filesystem block size & Block size assumptions differ & 0 & 1 &                      \\
    \midrule
    
    \multirow[t]{3}{*}{Process and Signals (PROC)}
    & Shell command execution & Shell behavior differs across OS & 5 & 14 & \multirow{3}{*}{25} \\
    & Missing signals & Signals unavailable on Windows & 3 & 2 &                      \\
    & Address already in use & Port binding conflicts vary & 1 & 0 &                      \\
    \midrule
    
    \multirow[t]{4}{*}{Library Dependencies (LIB)}
    & Platform-specific libraries (e.g., fcntl) & Unix-only libraries missing elsewhere & 2  & 1 & \multirow{4}{*}{24} \\
    & Missing libraries & Dependencies not universally available & 2 & 11 &                      \\
    & Dynamic library loading & DLL vs. SO file issues & 0 & 5 &                      \\
    & Binary wheel mismatch & Architecture-specific wheel incompatibilities & 0 & 3 &                      \\
    \midrule
    
    \multirow[t]{2}{*}{API Availability (API)}
    & Methods: os.uname(), os.geteuid(), os.getuid(), os.getpgid() & OS methods missing on Windows & 13 & 2 & \multirow{2}{*}{17} \\
    & Modules: readline, resource & Optional modules not everywhere & 0 & 2 &                      \\
    \midrule
    
    \multirow[t]{4}{*}{Environment and Display (ENV)}
    & No display in github CI (linux) & Headless CI lacks display server & 3 & 2 & \multirow{4}{*}{14} \\
    & GUI differences & Window manager behavior varies & 0 & 4 &                      \\
    & Missing curses & Curses unavailable on Windows & 1 & 0 &                      \\
    & Terminal capabilities & Terminal features differ across systems & 2 & 2 &                      \\
    \midrule
    
    \multirow[t]{3}{*}{Permissions and Limits (PERM)}
    & Permission & Permission models differ by OS & 3 & 1 & \multirow{3}{*}{7} \\
    & File descriptor limits & Resource limits vary across systems & 0 & 1 &                     \\
    & Symlink privileges & Windows requires admin for symlinks & 1 & 1 &                     \\
    \midrule
    
    \multirow[t]{2}{*}{System Information (SYS)}
    & /proc filesystem & /proc missing on macOS/Windows & 1 & 0 & \multirow{2}{*}{2} \\
    & Timezone database & Timezone data not always installed & 1 & 0 &                      \\
    \midrule
    \textbf{Total} & - & - & \textbf{\NumProjectsWithOSDiffInTests{}} & \textbf{\NumProjectsWithOSDiffInIssues{}} & \textbf{\TotalProjectsWithPortabilityIssues{}} \\
    \bottomrule
    \end{tabularx}
\end{table*}

We identify seven main root-cause categories that explain why portability problems occur:

\begin{packed_enumerate}
\item \textbf{File and Directories (FILE):} The most prevalent category (62 projects) stems
 from OS differences in file operations. Key issues include path separator differences
  (backslash vs. forward slash), line ending mismatches (\CodeIn{CRLF} vs. \CodeIn{LF}),
   file locking semantics, and text encoding defaults.

\item \textbf{Process and Signals (PROC):} This category (25 projects) captures failures
 due to OS process management differences. For example, shell execution varies across platforms
  (\CodeIn{cmd.exe} vs. \CodeIn{bash}), Windows lacks many Unix signals
   (\CodeIn{SIGHUP}, \CodeIn{SIGKILL}), and port binding produces different error codes.

\item \textbf{Library Dependencies (LIB):} These failures (24 projects) occur when code
 assumes platform-specific libraries. Issues include Unix-only modules (e.g., \CodeIn{fcntl}),
  missing dependencies, different dynamic library extensions (\CodeIn{.dll} vs \CodeIn{.so} vs
   \CodeIn{.dylib}), and architecture-specific binary wheels.

\item \textbf{API Availability (API):} This category (17 projects) covers Python APIs
 not universally available. Many OS module methods are Unix-specific (\CodeIn{os.uname()},
  \CodeIn{os.geteuid()}, \CodeIn{os.getpgid()}) and fail on Windows, while optional modules
   like readline and resource may be missing.

\item \textbf{Environment and Display (ENV):} These problems (14 projects) 
arise from runtime environment differences. For example, headless CI lacks display servers 
for GUI testing, window managers behave differently, terminal capabilities vary, 
and the curses module is unavailable on Windows (not included in standard Python distributions).

\item \textbf{Permissions and Limits (PERM):} These issues (7 projects) reflect OS
 differences in access control. For example, permission models differ (Unix chmod versus Windows ACLs),
  file descriptor limits vary, and symbolic link creation requires admin privileges on Windows.

\item \textbf{System Information (SYS):} This category (2 projects) includes failures
 from platform-specific system information mechanisms. For example, the /proc filesystem exists only on
  Linux, and timezone databases may not be installed by default.
\end{packed_enumerate}

\begin{tcolorbox}[boxrule=0pt,sharp corners,boxsep=2pt,left=2pt,right=2pt,top=2.5pt,bottom=2pt]
\textbf{RQ\textsubscript{2.1} (root causes)}: We identified \NumRootCauseCategories{} root-cause categories
 with \NumRootCauseSubcategories{} sub-categories. File and Directories (FILE) is most prevalent
  (62 projects), followed by Process and Signals (25 projects) and Library Dependencies (24 projects).
\end{tcolorbox}

\begin{figure}[!ht]
\centering
% Defensive Checks
\begin{subfigure}{0.48\textwidth}
\begin{lstlisting}[caption={Defensive check for optional library}, label={lst:defensive}]
- import readline
+ try:
+     import readline
+ except ImportError:
+     readline = None  # fallback if readline is unavailable
\end{lstlisting}
\end{subfigure}
% \hfill
% Environment Handling
\begin{subfigure}{0.48\textwidth}
\begin{lstlisting}[caption={Environment handling: Using pytest skip}, label={lst:environment}]
+ import platform
+ @pytest.mark.skipif(platform.system() == "Darwin", reason="Not supported on macOS")
def test_special_case():
    ...
\end{lstlisting}
\end{subfigure}
% \hfill

% Normalization
\begin{subfigure}{0.48\textwidth}
\begin{lstlisting}[caption={Normalization: Normalizing file paths}, label={lst:normalization}]
- file_path = "data/file.txt"
+ import os
+ file_path = "data" + os.path.sep + "file.txt"  # consistent path
\end{lstlisting}
\end{subfigure}
% \hfill

% Portable APIs
% OLD
% - temp_file = "/tmp/data.txt"  # unix-only path
% + import tempfile
% + from pathlib import Path
% + temp_file = Path(tempfile.gettempdir()) / "data.txt" # cross-platform 

\begin{subfigure}{0.48\textwidth}
\begin{lstlisting}[caption={Portable APIs: Using platform module for OS detection}, label={lst:portable}]
- hostname = os.uname()[1]  # unix-only function
+ import platform
+ hostname = platform.uname().node  # cross-platform
\end{lstlisting}
\end{subfigure}
% \vspace{0.2cm}
\caption{Representative examples of fixes for portability issues: (a) Defensive checks, (b) Environment handling, (c) Normalization, and (d) Portable APIs.}
\label{fig:patterns}
\end{figure}



\subsubsection{\textbf{Answering \RQTwoTwo{}}}
\label{rq2-2}

\noindent Understanding how portability issues manifest during
execution is essential for rapid diagnosis and triaging. In this
section, we examine the observable symptoms of portability
problems—the concrete error messages, exceptions, and behavioral
differences that appear in test logs, and CI
output. Table~\ref{tab:symptoms} maps the root causes (of portability
issues) to the corresponding symptom signature and fix pattern.

\noindent\begin{table*}[t]
    \centering
    \caption{Symptom signatures and general fix patterns for portability sub-categories.
     Symptom cells are color-coded: \colorbox{UniqueSymptomColor}{unique signatures} enable immediate diagnosis;
     \colorbox{ContextSymptomColor}{context-dependent} require code inspection.
     Fix patterns are color-coded to show reuse across 
     categories: \EnvPattern{}, \DefensivePattern{}, \NormPattern{}, \PortablePattern{}.}
    \label{tab:symptoms}
    \scriptsize
    \setlength{\tabcolsep}{3pt}
    \renewcommand{\arraystretch}{1.15}
    \begin{tabularx}{\textwidth}{
        >{\raggedright\arraybackslash}m{0.06\textwidth}
        >{\raggedright\arraybackslash}m{0.13\textwidth}
        >{\raggedright\arraybackslash}X
        >{\raggedright\arraybackslash}m{0.31\textwidth}
    }
    \toprule
    \textbf{Root-cause category} & \textbf{Sub-categories} & \textbf{Symptom (verbatim error / message)} & \textbf{General Fix Pattern} \\
    \midrule
    \textbf{FILE} &
    Path separators & \ContextSymptom{Context-dependent: hardcoded paths fail silently or with generic file errors} & \NormPattern{} — normalize paths with pathlib \\
    & Line endings & \UniqueSymptom{\texttt{AssertionError: `\textbackslash r\textbackslash n' != `\textbackslash n'}} & \NormPattern{} — normalize line endings \\
    & Case sensitivity & \ContextSymptom{Context-dependent: file exists but name differs only in case} & \NormPattern{} — case-normalize filenames \\
    & File locking & \UniqueSymptom{\texttt{PermissionError: [WinError 32] The process cannot access the file because it is being used by another process} (Windows)} & \EnvPattern{} — use context managers, explicit close \\
    & Encoding & \UniqueSymptom{\texttt{UnicodeDecodeError: `charmap' codec can't decode byte 0x\{XX\} in position N}} & \NormPattern{} — set explicit UTF-8 encoding \\
    & Filesystem block size & \ContextSymptom{Context-dependent: tests assert exact byte counts that vary by OS} & \NormPattern{} — normalize computations avoid assumptions \\
    \midrule
    \textbf{PROC} &
    Shell command execution & \ContextSymptom{Context-dependent: command syntax or exit codes differ across shells} & \PortablePattern{} — use subprocess.run shell=False \\
    & Missing signals & \UniqueSymptom{\texttt{AttributeError: module `signal' has no attribute `SIGHUP'} (or SIGKILL, SIGTERM variants)} & \DefensivePattern{} — guard signals per platform \\
    & Address already in use & \UniqueSymptom{\texttt{OSError: [Errno 98] Address already in use} (Linux) / \texttt{[WinError 10048]} (Windows)} & \DefensivePattern{} + \EnvPattern{} — catch OS-specific errors, use port 0 \\
    \midrule
    \textbf{LIB} &
%%    \MySpace{ (e.g., fcntl)} 
    Platform-specific libraries& \UniqueSymptom{\texttt{ModuleNotFoundError: No module named `fcntl'} (Windows)} & \DefensivePattern{} — conditional import provide fallback \\
    & Missing libraries & \UniqueSymptom{\texttt{ModuleNotFoundError: No module named `\{library\_name\}'}} & \DefensivePattern{} — optional import with fallback \\
    & Dynamic library loading & \UniqueSymptom{\texttt{OSError: [WinError 126] The specified module could not be found} (Windows)} & \DefensivePattern{} — guard loading prefer pure-Python \\
    & Binary wheel mismatch & \ContextSymptom{Context-dependent: installation succeeds but runtime fails with ABI errors} & \EnvPattern{} — install platform-appropriate wheel \\
    \midrule
    \textbf{API} &
    Methods (e.g., os.uname) & %: os.uname(), os.geteuid(), os.getuid(), os.getpgid()
    \UniqueSymptom{\texttt{AttributeError: module `os' has no attribute `\{method\}'} (Windows)} & \PortablePattern{} + \DefensivePattern{} + \EnvPattern{} — use platform.uname, hasattr guards, OS detection \\
    & Modules (e.g., readline)  & % : readline, resource
    \UniqueSymptom{\texttt{ModuleNotFoundError: No module named `\{module\}'} (Windows)} & \DefensivePattern{} — guard import use alternatives \\
    \midrule
    \textbf{ENV} &
    No display in github CI & \UniqueSymptom{\texttt{TclError: no
        display name and no \$DISPLAY environment variable} (Linux)} & \EnvPattern{} — skip GUI tests in CI \\
    & GUI differences & \ContextSymptom{Context-dependent: widgets render or behave differently} & \EnvPattern{} — skip or mock GUI features \\
    & Missing curses & \UniqueSymptom{\texttt{ModuleNotFoundError: No module named `\_curses'} (Windows)} & \DefensivePattern{} — conditional import with stub fallback \\
    & Terminal capabilities & \ContextSymptom{Context-dependent: color codes appear as text or formatting ignored} & \EnvPattern{} — detect capabilities degrade gracefully \\
    \midrule
    \textbf{PERM} &
    Permission & \ContextSymptom{Context-dependent: depends on operation (chmod, file access, etc.)} & \DefensivePattern{} + \EnvPattern{} — check permissions, skip restricted tests \\
    & File descriptor limits & \UniqueSymptom{\texttt{OSError: [Errno 24] Too many open files}} & \EnvPattern{} — close files promptly, raise limits in CI \\
    & Symlink privileges & \UniqueSymptom{\texttt{OSError: [WinError 1314] A required privilege is not held by the client} (Windows)} & \EnvPattern{} — detect support and use copies \\
    \midrule
    \textbf{SYS} &
    /proc filesystem & \ContextSymptom{Context-dependent: \texttt{/proc}    paths fail on macOS/Windows with generic FileNotFoundError} & \PortablePattern{} — use psutil or platform APIs \\
    & Timezone database & \UniqueSymptom{\texttt{zoneinfo.ZoneInfoNotFoundError: `No time zone found with key \{...\}'}} & \NormPattern{} — install tzdata or fallback \\
    \bottomrule
    \end{tabularx}
    \end{table*}   

\MyPara{Symptom categories}~We categorize symptoms into two types based on 
their diagnostic value (color-coded in Table~\ref{tab:symptoms}): 
\textbf{(i) Unique signatures} are error messages 
that unambiguously identify a specific portability sub-category, enabling 
immediate diagnosis from logs alone; and \textbf{(ii) Context-dependent symptoms} 
are manifestations that require additional code inspection or contextual understanding 
to distinguish from other failure modes.

\MyPara{Unique signatures}~The majority of sub-categories (15 out of 24, 62.5\%) 
produce unique, actionable error signatures. These include missing API methods 
(\eg{}, \texttt{AttributeError: module `os' has no attribute `geteuid'}), 
unavailable modules (\eg{}, \texttt{ModuleNotFoundError: No module named `fcntl'}), 
encoding errors (\texttt{UnicodeDecodeError: `charmap' codec...}), 
specialized exceptions like \texttt{TclError} for display issues or 
\texttt{ZoneInfoNotFoundError} for timezone problems, and platform-specific errors 
like \texttt{OSError: [Errno 24] Too many open files} for file descriptor limits.
These signatures facilitate more direct diagnosis from logs, reducing the need for 
extensive code inspection required by context-dependent symptoms.

\MyPara{Context-dependent symptoms}~The remaining 9 sub-categories (37.5\%) 
do not produce unique error signatures, requiring developers to inspect the code 
context to diagnose the portability issue. These include cases where the same error 
can arise from multiple root causes, where failures are silent or produce generic errors, 
or where the symptom manifests as behavioral differences rather than exceptions. 
For example, path separator issues often manifest as generic \texttt{FileNotFoundError} 
or silent failures where hardcoded paths like \texttt{"/tmp/file.txt"} work on Unix but 
fail on Windows. Similarly, case sensitivity issues may produce \texttt{FileNotFoundError} 
indistinguishable from actual missing files unless one examines the filesystem. 
The /proc filesystem is another case where generic \texttt{FileNotFoundError} occurs on 
macOS/Windows, requiring code inspection to determine if the issue stems from missing /proc 
paths rather than other file access problems. Shell command execution issues rarely throw 
exceptions but instead manifest as incorrect exit codes or output differences when commands 
are interpreted by different shells (\texttt{cmd.exe} vs. \texttt{bash}). GUI differences 
show as visual rendering variations or behavioral inconsistencies rather than explicit errors, 
and terminal capability mismatches may cause ANSI color codes to appear as literal text. 
Permission issues vary depending on the specific operation (chmod, file access, etc.), 
filesystem block size problems appear as test assertions on exact byte counts that differ 
across platforms, and binary wheel mismatches often succeed during installation but fail at 
runtime with ABI compatibility errors that vary by architecture.

\MyPara{Implications for diagnosis}~Our findings reveal that while
unique signatures enable rapid automated triage for 62.5\% of
portability issues, the remaining 37.5\% require more sophisticated
analysis. This suggests that detection tools must go beyond simple log
parsing and incorporate contextual hints (\eg{}, hardcoded paths,
shell command patterns, or platform-specific API usage). The presence
of context-dependent symptoms also highlights the importance of
comprehensive cross-platform testing, as these issues cannot be
reliably predicted through static analysis alone.

% \begin{tcolorbox}[boxrule=0pt,sharp corners,boxsep=2pt,left=2pt,right=2pt,top=2.5pt,bottom=2pt]
\begin{tcolorbox}[boxrule=0pt,sharp corners,boxsep=1pt,left=2pt,right=2pt,top=2pt,bottom=1pt]
\textbf{RQ\textsubscript{2.2} (symptoms)}: Of 24 sub-categories, 15 (62.5\%) produce unique error signatures enabling
 immediate diagnosis, while 9 (37.5\%) exhibit context-dependent symptoms requiring code inspection to distinguish
  from other failure modes.
\end{tcolorbox}
 
\subsubsection{\textbf{Answering \RQTwoThree{}}}
\label{rq2-3}


\noindent

\MyPara{Fix patterns}~By examining source code, test logs, issue discussions, and 
merged pull requests from both detection approaches, we identified four general fix 
patterns that address portability problems across all 24 sub-categories (mapped in 
Table~\ref{tab:symptoms}). Figure~\ref{fig:patterns} illustrates these patterns with 
concrete examples:

\begin{packed_enumerate}
\item \textbf{Environment handling} adapts code and tests to runtime conditions by 
detecting platform capabilities and adjusting behavior. Listing~\ref{lst:environment} 
shows this pattern using pytest's \texttt{skipif} decorator to conditionally skip 
tests on macOS. This allows test suites to adapt to platform-specific capabilities 
without maintaining separate test files for each operating system. This is the most 
prevalent pattern, accounting for 35.7\% of all pattern applications.

\item \textbf{Defensive checks} add guards, fallbacks, or conditional imports to handle 
missing APIs or modules gracefully. Listing~\ref{lst:defensive} demonstrates this 
pattern by wrapping the \texttt{readline} import (unavailable on Windows) in a 
\texttt{try/except} block with a fallback value, allowing the code to continue 
execution rather than crash. This pattern applies when functionality may be unavailable 
on certain platforms and accounts for 32.2\% of pattern applications.

\item \textbf{Normalization} standardizes encodings, line endings, paths, or filenames 
to prevent spurious differences across platforms. Listing~\ref{lst:normalization} 
illustrates this by constructing file paths using \texttt{os.path.sep} instead of 
hardcoded separators, ensuring correct behavior on both Unix (forward slash) and 
Windows (backslash) systems. This pattern accounts for 21.4\% of applications.

\item \textbf{Portable APIs} replace OS-specific constructs with cross-platform 
abstractions. Listing~\ref{lst:portable} demonstrates replacing the Unix-only 
\texttt{os.uname()} function with the portable \texttt{platform.uname()} function, 
ensuring the code works correctly on both Unix-like systems and Windows. While 
used least frequently (10.7\%), this pattern targets specific, well-defined problems 
where standard cross-platform abstractions exist.

\end{packed_enumerate}  


\MyPara{Pattern
  distribution}~Figure~\ref{fig:fix_pattern_distribution} shows how
these patterns distribute across all pattern applications. Some
sub-categories require multiple patterns, resulting in 28 total
pattern applications across 24 sub-categories. Environment handling is
the most prevalent strategy, accounting for 10 applications (35.7\%),
followed by Defensive checks with 9 applications (32.2\%), together
representing approximately two-thirds of all pattern applications.
Normalization accounts for 6 applications (21.4\%), primarily
addressing file and path-related issues. Portable APIs, while used
least frequently (3 applications, 10.7\%), targets specific,
well-defined problems where standard cross-platform abstractions
exist. This distribution reflects the nature of portability issues:
most problems arise from platform-environmental differences
(explaining fixes with conditional handling) rather than fundamental
API incompatibilities. The data also show that portability fixes are
typically short and localized rather than extensive or
widespread. Consequently, they tend to impose a low workload on
developers for both implementation and review.

\begin{figure}[ht]
\centering
% \vspace{-0.5em}
\begin{tikzpicture}
\pie[
    text=legend,
    radius=1.4,
    color={EnvColor!80,DefensiveColor!80,NormColor!80,PortableColor!80}
]{
    35.7/Environment handling (10),
    32.2/Defensive checks (9),
    21.4/Normalization (6),
    10.7/Portable APIs (3)
}
\end{tikzpicture}
% \vspace{-0.5em}
\caption{Distribution of fix patterns across 24 portability
  sub-categories.}
\label{fig:fix_pattern_distribution}
\end{figure}

\begin{tcolorbox}[boxrule=0pt,sharp corners,boxsep=2pt,left=2pt,right=2pt,top=2.5pt,bottom=2pt]
\textbf{RQ\textsubscript{2.3} (fixes)}: We identified \NumFixPatterns{} general fix 
patterns that collectively address all \NumRootCauseSubcategories{} portability 
sub-categories. Some sub-categories require multiple patterns, resulting in 28 total 
pattern applications. Environment handling is most prevalent (35.7\%), followed by 
Defensive checks (32.2\%), Normalization (21.4\%), and Portable APIs (10.7\%). 
These patterns represent recurring, localized code modifications that provide 
systematic guidance for resolving portability issues across diverse contexts.
\end{tcolorbox}

\subsection{\textbf{Answering} \RQThree{}}
\label{rq3}

This section evaluates performance of tools for detecting
(\textsection~\ref{sec:llm-detection}) and for fixing
(\textsection~\ref{sec:llm-fixing}) portability issues.

\begin{table}[t!]
\centering
  \caption{~\label{tab:llm_detection_results} LLMs for detection: metrics.}
  \vspace{-2ex}  
  \setlength{\tabcolsep}{1.5pt} % Sets the horizontal space before and after each column to 10pt
  \begin{subtable}[t]{0.5\textwidth}
    \centering
\caption{\label{tab:confusion_matrices_combined}Confusion matrices w/ 90 samples: 60 portable and 30 non-portable.}
\begin{tabular}{lccc|ccc|ccc}
  \toprule
  & \multicolumn{9}{c}{\textbf{Predicted}}\\
\cline{2-10}
& \multicolumn{3}{c|}{\texttt{llama-3.3}} &
\multicolumn{3}{c|}{\texttt{grok-4-fast}} &
\multicolumn{3}{c}{\texttt{gpt-4o-mini}} \\
\textbf{Actual} & \textbf{Port} & \textbf{NonPort} & & \textbf{Port} & \textbf{NonPort} & & \textbf{Port} & \textbf{NonPort} & \\
\midrule
\textbf{Port}     & 10 & 50 & & 53 & 7  & & 21 & 39 & \\
\textbf{NonPort}  & 4  & 26 & & 12 & 18 & & 1  & 29 & \\
\bottomrule
\end{tabular}
  \end{subtable}
  \vspace{1ex}
  \hfill
  \begin{subtable}[t]{0.5\textwidth}
    \centering
\caption{Performance metrics.}
\label{tab:llm_performance}
\begin{tabular}{lrrrrr}
\toprule
Model & Precision & Recall & F1-Score & Accuracy \\
\midrule
\texttt{llama-3.3} & 0.34 & 0.87 & 0.49 & 0.40 \\
\texttt{grok-4-fast} & 0.72 & 0.60 & 0.65 & 0.79 \\
\texttt{gpt-4o-mini} & 0.43 & 0.97 & 0.59 & 0.56 \\
\bottomrule
\end{tabular}
  \end{subtable}
\end{table}


\begin{table}[t!]
\centering
\caption{Prompts used for LLM evaluation.}
\label{tab:llm_prompts}

\begin{subtable}[t]{0.97\columnwidth}
\centering
\caption{Generic prompt used for portability detection and fixing.}
\label{tab:generic_prompt}
\begin{tabular}{p{0.97\columnwidth}}
\toprule
\textbf{Generic Prompt} \\
\midrule
You are a Python expert. Check the following code and answer: \\
1. Is there any operation in the code that could fail on a specific operating system (Linux, Mac, Windows)? \\
2. If yes, explain why and on which OS it might fail. If it is fully portable, finish saying ``Portable''. \\
\bottomrule
\end{tabular}
\end{subtable}

\vspace{1ex}

\begin{subtable}[t]{0.97\columnwidth}
\centering
\caption{Pattern-guided prompt template to fix portability issues.}
\label{tab:specific_prompts}
\begin{tabular}{p{0.97\columnwidth}}
\toprule
\textbf{Pattern-Guided Prompt Template} \\
\midrule
You are a Python expert. The following code has a portability problem:
\textit{<symptom-description>}. Fix the code, considering the
following fix options \textit{<list-of-possible-fixes-for-the-symptom>}. \\
\bottomrule
\end{tabular}
\end{subtable}
\end{table}




% \\
% \textbf{Concrete Examples:} \\
% \\
% \textbf{For unique symptoms (e.g., Missing signals):} \\
% ``This code has \texttt{AttributeError: module 'signal' has no attribute 'SIGHUP'}. Consider using the \texttt{Defensive checks} approach: \texttt{guard signals per platform}.'' \\
% \\
% \textbf{For context-dependent symptoms (e.g., Shell command execution):} \\
% ``This code has \texttt{context-dependent command syntax or exit codes that differ across shells}. Consider using the \texttt{Portable APIs} approach: \texttt{use subprocess.run shell=False}.'' \\
% \\
% \textbf{For combined patterns (e.g., OS methods):} \\
% ``This code has \texttt{AttributeError: module 'os' has no attribute '\{method\}'}. Consider using the \texttt{Portable APIs + Defensive checks} approach: \texttt{use platform.uname, hasattr guards}.'' \\


\subsubsection{LLMs for detection}\sloppy
\label{sec:llm-detection}
\textbf{Alternative static analysis tools.} We examine the
capabilities of various popular linters for Python, namely
Ruff~\cite{ruffref}, Flake8~\cite{flake8}, Pylint~\cite{pylint},
MyPy~\cite{mypy}, Bandit~\cite{bandit}, and Pyright~\cite{pyright}.
%
We find that only Ruff provides a rule that directly addresses
potential portability concerns, namely the
\texttt{unspecified-encoding} check~\cite{encruff} that maps to the
``Encoding'' sub-category within the ``File and Directories''
category. The remaining tools focus primarily on style, type
correctness, security, and general code quality. For that reason, we
explored the use of Large Language Models (LLMs) to detect portability
problems and repair them. We consider three LLMs of moderate model size, namely
\texttt{llama-3.3}~\cite{llama3}, \texttt{Grok-4-Fast}~\cite{grok4},
and \texttt{GPT-4o-Mini}~\cite{gpt4o}

%% This rule helps identify cases where files are opened
%% without explicitly specifying an encoding, which may lead to
%% cross-platform inconsistencies. The remaining tools focus primarily on
%% style, type correctness, security, and general code quality, without
%% offering rules specifically designed to detect
%% operating-system-dependent behaviors or other portability issues.
%% \Den{@Marcelo, everything below is new and needs to be revised!}
%% \textbf{Motivation for LLM Evaluation.}
%% Given the limited support for portability checks in existing static
%% tools, we extended our evaluation to large language models (LLMs).

\noindent\textbf{Setup.}~We sampled a total of \NumCodeUsedLLM{} code
snippets to evaluate performance of LLM: (i)~\NumCodeNonPortableByLLM{} non-portable snippets
containing OS-specific constructs, (ii)~\NumCodePortableIssue{} portable snippets that had
been fixed by developers (mined from issues), and (iii)~\NumCodePortableManual{} code we
manually categorized as portable. This distribution attempts to
reflect a more realistic division of code seen in the wild, where
portability violations are less frequent. We used the openrouter.ai
API~\cite{openrouter} to interact with the models. Each model was
prompted with a consistent template that included the code snippet and
a question asking whether the code was portable across Windows, Linux,
and Mac (see Table~\ref{tab:llm_prompts}). We parse the answers to extract 
classification decisions.
% 
%% Unrelated code was treated as portable for classification purposes, since such snippets do not exhibit any portability problems. Consequently, the dataset comprised 60 portable samples (30 corrected + 30 unrelated) and 30 non-portable samples. 
% 

\noindent\textbf{Metrics.}~We used standard metrics to measure the performance of binary classifiers (e.g., precision, recall, accuracy, etc.)~\cite{saito2015precision}. Table~\ref{tab:llm_detection_results} shows
results. Table~\ref{tab:confusion_matrices_combined} shows the
confusion matrices associated with each LLM, illustrating the
classification of the \NumCodeUsedLLM{} samples.
Table~\ref{tab:llm_performance} shows the metrics for each model.

%% By comparing the models' predictions against the known classification
%% of each snippet, we quantitatively assessed their ability to identify
%% cross-platform issues. Specifically, we computed precision, recall,
%% F1-score, and accuracy for each model.
%% % 
%% These metrics provide an objective measure of how effectively LLMs can
%% detect or reason about portability issues—an area where traditional
%% static tools often fall short. This setup allows us to explore whether
%% LLMs can complement static analyzers by offering probabilistic
%% insights into potential cross-platform
%% hazards.

\noindent\textbf{Analysis of results.}~ The results we obtain indicate
limited ability of LLMs to accurately reason about portability
issues. llama-3.3 and GPT-4o-Mini are ``pessimistic'' and report too
many alarms, most of which are false positives. In contrast,
Grok-4-Fast is more cautious in reporting but misses lots of true
positives.
%% In practical terms, these results suggest complementary reasoning profiles across models:
%% \begin{enumerate}
%%   \item \texttt{llama-3.3}: Highly cautious and overgeneralizing reasoning—maximizes detection coverage but produces many false positives.
%%   \item \texttt{Grok-4-Fast}: Balanced reasoning—achieves the best trade-off between recall, precision, and overall accuracy.
%%   \item \texttt{GPT-4o-Mini}: Over-sensitive reasoning—detects almost all non-portable cases but lacks discrimination power.
%% \end{enumerate}


%% a tendency to rely on surface-level
%% heuristics rather than deep contextual understanding of cross-platform
%% behavior. Addressing this limitation will require more explicit
%% exposure to platform-diverse examples during instruction tuning and
%% evaluation.
%% Table~\ref{tab:confusion_matrices_combined} presents the confusion matrices for all three models evaluated. The matrices reveal distinct classification patterns across models. \texttt{Grok-4-Fast} demonstrates the most balanced performance, correctly identifying 53 out of 60 portable cases and 18 out of 30 non-portable cases, yielding an overall accuracy of 79\%. In contrast, \texttt{llama-3.3} exhibits a strong bias toward the non-portable class, misclassifying 50 of the 60 portable samples, resulting in only 40\% accuracy. \texttt{GPT-4o-Mini} shows an opposite tendency: while it achieves near-perfect specificity (29 out of 30 non-portable cases correctly identified), it misses the majority of portable cases (39 out of 60), achieving 56\% accuracy. These patterns suggest fundamentally different decision boundaries and risk tolerance levels across the three models, which we analyze in detail below.

%% The extended evaluation provides new insight into how different LLMs reason about software portability under a more diverse and balanced dataset. The inclusion of ``unrelated'' yet valid Python code introduces a subtle but important challenge: distinguishing the absence of portability issues from the presence of clear portability signals. This distinction is conceptually close to the notion of false alarms in static analysis and serves as a more realistic proxy for practical development scenarios.

%% % talking about the precision, recall, f1-score, accuracy now:

%% The \texttt{llama-3.3} model achieved the lowest precision 34\% and accuracy 40\%, while maintaining the highest recall 87\%. This means that Llama correctly identified most non-portable cases but did so with a substantial number of false alarms—frequently flagging portable code as non-portable. Its confusion matrix (Table~\ref{tab:confusion_matrices_combined}) confirms this pattern: the model displays a highly cautious bias, tending to classify almost any code snippet as potentially non-portable.
%% % 
%% This behavior reflects a risk-sensitive but shallow reasoning strategy, where the model overestimates portability issues without truly distinguishing between harmless platform-specific constructs and genuine cross-platform incompatibilities. As a result, Llama achieves strong coverage at the expense of reliability, limiting its effectiveness for precise portability assessment.

%% In contrast, \texttt{Grok-4-Fast} exhibited the most balanced and effective performance, with a precision of 72\%, recall of 60\%, F1-score of 65\%, and the highest accuracy (79\%). Its confusion matrix (Table~\ref{tab:confusion_matrices_combined}) shows that Grok successfully differentiates between portable and non-portable code in most cases, striking a reasonable balance between sensitivity and specificity.
%% %
%% The model demonstrates contextual generalization, capturing both syntactic and semantic cues related to platform-dependent constructs. However, its moderate recall suggests a cautious tendency to miss some non-portable samples, favoring correctness over exhaustive detection.

%% The \texttt{GPT-4o-Mini} model achieved the highest recall (97\%), outperforming the others in its ability to detect nearly all non-portable cases. Nevertheless, its low precision (43\%) and moderate accuracy (56\%) indicate a strong over-detection bias—GPT-4o tends to classify many portable codes as problematic.

%% This pattern reveals a highly inclusive reasoning style: the model is eager to label code as non-portable, prioritizing coverage rather than precision. Such behavior, while useful for exploratory or preventive analysis, may produce excessive false positives in practical settings where diagnostic accuracy is essential.


%% From a broader research standpoint, the findings indicate that all three models possess a functional grasp of portability-related cues—such as system-dependent paths, OS-specific APIs, and encoding assumptions—but none exhibit fine-grained semantic discrimination. 
%
%% \textbf{Answer.} \Fix{turn this into a box anwser:}Static tools
%% (linters, CodeQL, LLM prompts) can detect some issues but miss
%% runtime failures. Dynamic re-execution across OSes (our approach)
%% provides broader coverage.


\subsubsection{LLMs for fixing.}
\label{sec:llm-fixing}
\noindent\textbf{Setup.}~ Following the classification experiment, we designed a 
complementary evaluation focused on the corrective capabilities of LLMs. In this 
phase, we used the \NumCodeNonPortableByLLM{} non-portable Python snippets identified 
earlier and prompted each model with two distinct repair configurations.  
In the first configuration (\textit{generic prompt}), each model received a general 
instruction stating that the code exhibited portability issues and was asked to 
produce a corrected version (see Table~\ref{tab:llm_prompts}).  
In the second configuration (\textit{pattern-guided prompt}), each non-portable 
snippet was accompanied by the specific symptom (derived from our Unique
signature symptoms in \ref{rq2-2}) and the corresponding \textit{General Fix Pattern} from 
Table~\ref{tab:symptoms}. 

The rationale for this approach is: (1) observe symptoms in the code,
(2) look up the mapping in Table~\ref{tab:symptoms} (Symptom
$\rightarrow$ General Fix Pattern), and (3) prompt the LLM with
specific fix pattern guidance. For example, when encountering code
that produces \texttt{AttributeError: module `signal' has no attribute
  `SIGHUP'}, we would provide that symptom together with corresponding
fix(es) (Table~\ref{tab:symptoms}). Table~\ref{tab:llm_prompts} shows
the template structure used for these pattern-guided prompts.  This
design evaluates whether structured repair guidance improves the
model's ability to generate functionally portable code. Each model
attempted to fix all \NumCodeNonPortableByLLM{} snippets, resulting in
90 repair attempts per configuration. We also examine repair performance 
separately for cases with explicit symptom descriptions versus cases with 
empty symptom fields to understand how input characteristics influence model effectiveness.

\noindent\textbf{Metrics.} ~We evaluate the correctness of a generated
fix with two criteria: (i) the modified code must execute successfully
without errors across all target platforms (Linux, macOS, Windows),
and (ii) the fix must preserve the intended functionality of the
original program.  Fixes that met both criteria were considered
\textit{correct}; otherwise, they were labeled as \textit{incorrect}.
Accuracy is the ratio of correct fixes to the total number of repair
attempts.

\begin{table}[t]
\centering
\caption{Performance of LLMs in generating portability fixes across both prompt configurations (\NumCodeNonPortableByLLM{} samples per model).}
\label{tab:llm_fix_results_combined}

\begin{subtable}[t]{0.47\textwidth}
\centering
\caption{Generic prompt.}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{Correct} & \textbf{Incorrect} & \textbf{Accuracy} \\
\midrule
\texttt{grok-4-fast} & 13 & 17 & 0.43 \\
\texttt{gpt-4o-mini} & 10 & 20 & 0.33 \\
\texttt{llama-3.3} & 8 & 22 & 0.27 \\
\bottomrule
\end{tabular}
\end{subtable}
\hfill
\begin{subtable}[t]{0.47\textwidth}
\centering
\caption{Pattern-guided prompt.}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{Correct} & \textbf{Incorrect} & \textbf{Accuracy} \\
\midrule
\texttt{grok-4-fast} & 23 & 7 & 0.77 \\
\texttt{gpt-4o-mini} & 21 & 9 & 0.70 \\
\texttt{llama-3.3} & 15 & 15 & 0.50 \\
\bottomrule
\end{tabular}
\end{subtable}
\end{table}

\begin{table}[t]
\centering
\caption{Pattern-guided repair performance breakdown: cases with symptom descriptions vs.\ empty symptom fields.}
\label{tab:llm_fix_results_symptom_breakdown}

\begin{subtable}[t]{0.47\textwidth}
\centering
\caption{With symptom descriptions ($n=23$).}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{Correct} & \textbf{Incorrect} & \textbf{Accuracy} \\
\midrule
\texttt{grok-4-fast} & 19 & 4 & 0.83 \\
\texttt{gpt-4o-mini} & 16 & 7 & 0.70 \\
\texttt{llama-3.3} & 12 & 11 & 0.52 \\
\bottomrule
\end{tabular}
\end{subtable}
\hfill
\begin{subtable}[t]{0.47\textwidth}
\centering
\caption{Without symptom descriptions ($n=7$).}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{Correct} & \textbf{Incorrect} & \textbf{Accuracy} \\
\midrule
\texttt{grok-4-fast} & 4 & 3 & 0.57 \\
\texttt{gpt-4o-mini} & 5 & 2 & 0.71 \\
\texttt{llama-3.3} & 3 & 4 & 0.43 \\
\bottomrule
\end{tabular}
\end{subtable}
\end{table}

\noindent\textbf{Analysis of results.}~ The results in Table~\ref{tab:llm_fix_results_combined} show that all models exhibited limited success under the generic prompt setting, with accuracies ranging from 27\% to 43\%.  
This indicates that, when provided with only minimal context, most LLMs struggle to infer the exact nature of portability issues and to produce appropriate cross-platform corrections.  
While the \texttt{grok-4-fast} model outperformed the others, its success rate remained below 50\%, suggesting that general-purpose reasoning alone is insufficient for precise code repair in this domain.

In contrast, the pattern-guided configuration substantially improved repair performance across all models.  
The inclusion of explicit problem descriptions and generalized fix patterns increased accuracy by more than 30 percentage points on average.  
The \texttt{grok-4-fast} model achieved the highest accuracy (76.67\%), demonstrating a clear ability to integrate structured repair hints into its reasoning process.  
Similarly, \texttt{gpt-4o-mini} reached 70\%, producing fixes that were more consistent with the intended patterns while preserving functional correctness.  
The \texttt{llama-3.3} model also showed moderate improvement (50\%), particularly in cases involving straightforward path and API adjustments, though it still struggled with more complex logic integration.

Table~\ref{tab:llm_fix_results_symptom_breakdown} reports on an
experiment where \Mar{...complete...}. Results indicate that the availability of symptom descriptions significantly influences LLM performance.  
Among the 23 cases with explicit symptom descriptions, \texttt{grok-4-fast} achieves notably higher accuracy (0.83), while \texttt{gpt-4o-mini} and \texttt{llama-3.3} maintain similar or slightly improved performance (0.70 and 0.52, respectively).  
In contrast, for the 7 cases where symptom descriptions were not available due to context-dependent or unstable symptoms, the prompt contained only the code snippet and general instructions to apply the relevant fix pattern. In these cases, performance generally declines: \texttt{grok-4-fast} drops to 0.57, while \texttt{gpt-4o-mini} remains robust at 0.71 and \texttt{llama-3.3} falls to 0.43.  
This pattern demonstrates that explicit symptom descriptions provide valuable context that particularly benefits more sophisticated models, whereas some cases can still be addressed even without rich contextual information.

Overall, these findings highlight the strong impact of guided contextualization on LLM-based repair.  
When supplied with explicit repair patterns, the models produced significantly more reliable and semantically consistent fixes, confirming that structured guidance can substantially enhance cross-platform reasoning capabilities in code generation tasks.


\begin{tcolorbox}[boxrule=0pt,sharp corners,boxsep=2pt,left=2pt,right=2pt,top=2.5pt,bottom=2pt]
  \textbf{RQ\textsubscript{3} (tool effectiveness):}~ Static analysis tools 
  provide minimal portability detection support. LLMs achieve moderate detection 
  accuracy (40--79\% across three models), with substantial variation in precision 
  and recall. For repair tasks, LLMs show limited success with generic prompts 
  (27--43\% accuracy) but improve significantly with pattern-guided prompts 
  (50--77\% accuracy), yielding an average improvement of 30 percentage points. 
  These results demonstrate that structured guidance using error signatures and 
  fix patterns from Table~\ref{tab:symptoms} substantially enhances LLM-based 
  repair capabilities.
%% —only Ruff includes a
%% specific rule related to encoding.
%% Traditional static analysis tools provide support to detect or fix
%% portability issues.
%% \textbf{For detection}, LLMs show potential over traditional tools. They achieved strong recall and precision, with the best model reaching approximately 70\%, suggesting that they can reliably identify portability-related problems in source code. 
%% \textbf{For repair}, however, their performance remains less limited. When prompted to fix 30 faulty snippets, success rates ranged from 33--43\% with generic prompts, increasing to 50--77\% only when explicit issue descriptions and repair patterns were provided. Overall, results indicate that while LLMs can recognize and occasionally correct portability issues, achieving reliable automated repairs still requires structured guidance or integration with static verification tools.
\end{tcolorbox}


\subsection{\textbf{Answering} \RQFour{}}

This section discusses the pull requests (PRs) we submitted.
%to open-source projects to address portability issues identified in our study.
\label{sec:pr-discussion}

%

% Add here: time-to-fix statistics, issue/PR comment analysis
% For example: Figure 4 (Distribution of time-to-fix), Table 5 (Engagement metrics per project)

\subsubsection{Method}
We use the cross-OS test re-execution method
(\textsection~\ref{sec:methods}) to detect portability problems in our
\emph{application set} (\textsection~\ref{sec:projects}). For each
identified issue, we manually apply one of the four fix patterns from our
taxonomy~(Table~\ref{tab:symptoms}). We then validate each fix by re-running 
the affected tests on all three operating systems (Linux, macOS, Windows) to 
confirm consistent behavior across platforms. Once the fix is validated, we 
submit a pull request to the project, providing a clear description of the 
portability issue, the root cause, and an explanation of how the proposed change 
resolves the problem.

% \Den{Add a Reference to your PR list in the artifact? \ourrepo{}}\Mar{No}
% \Den{but we used your personal account to create the PRs.}

\subsubsection{Results}

Table~\ref{tab:prs} summarizes the results of this effort. We
submitted a total of \TotalPROpened{} pull requests spanning 6
categories of portability issues, of which \TotalPRAccepted{} had been
accepted at the time of writing. The overall acceptance rate of \TotalPRAprovedPercent{},
along with a zero rejection rate, indicates that the issues we
identified correspond to genuine portability problems recognized by
maintainers. Notably, ``File and Directories'' issues were the
most frequent (15 PRs), reflecting the prevalence of path, encoding,
and file handling inconsistencies across platforms. Conversely,
``Process and Signals'' issues exhibited the highest acceptance rate
(100\%), likely due to their critical impact on application
functionality.
%
Considering the location of the fix, we observe that in
\TotalPRInTestsCode{} cases only the test code changes, in
\TotalPRInProgramCode{} cases only the program code changes, and in
\TotalPRInBothCode{} cases both the test and program code
changes. Note that many cases require changes to the application logic
to ensure consistent behavior across platforms.

Next, we highlight three representative pull requests that illustrate
how portability issues manifest in practice and how our identified fix
patterns address them. These examples span multiple categories and demonstrate
different sub-categories and fix patterns from our taxonomy.

%% This distribution suggests that while some portability issues
%% can be resolved by adjusting tests to be more environment-aware,


\begin{table}[t!]
    \centering
    \small
    %footnotesize
    \setlength{\tabcolsep}{2pt}
    \caption{
      % \Fix{Ali, please update this table with the latest results.} 
      Summary of Pull Requests (PRs) by issue type, showing opened, accepted, and rejected. The last row shows totals.}
    \label{tab:prs}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Category} & \textbf{Opened} & \textbf{Accepted} & \textbf{Rejected} \\
        \midrule
        File and Directories & 15 & 9 & 0 \\
        API Availability & 12 & 4 & 0 \\
        Process and Signals & 2 & 2 & 0 \\
        System Information & 2 & 1 & 0 \\
        Library Dependencies & 1 & 1 & 0 \\
        Environment and Display & 1 & 0 & 0 \\
        \midrule
        \textbf{Total} & \textbf{\TotalPROpened} & \textbf{\TotalPRAccepted} & \textbf{\TotalPRRejected} \\
        \bottomrule
    \end{tabular}
\end{table}


\subsubsection{The webassets Pull Request}
% https://github.com/miracle2k/webassets/pull/562

% We illustrate how portability issues manifest and are fixed through
% the webassets project~\cite{webassets}. Webassets is an asset
% management library for Python web development with 932 stars, used for
% bundling and processing web assets like CSS and JavaScript files.
We illustrate how portability issues manifest and are fixed using the \textit{webassets} project~\cite{webassets}, a Python library for managing and bundling web assets such as CSS and JavaScript, with 932 stars.
%
The problem we encountered falls into the \textbf{File and Directories
  (FILE)} category, specifically the ``File locking''
sub-category. The issue occurred in the test suite where
\CodeIn{tempfile.NamedTemporaryFile} was used without disabling
automatic deletion, causing failures on Windows due to
platform-specific file locking
behavior~\cite{pythonnamedtemporaryfile}. On Windows, the operating
system locks open files more aggressively than on Unix-like systems,
preventing the same file from being reopened while still in use.
%
Listing~\ref{lst:webassets_fix} shows the diff of our fix applied to
the test file. The original code created a temporary file within a
context manager that automatically deleted the file upon exiting the
\texttt{with} block. However, on Windows, attempting to reopen this
file (which the test needed to do because it's a fixture) failed
because the file was still locked by the previous file handle.
 %, language=diff

\begin{lstlisting}[caption={Approved PR for temporary file handling issue.}, label={lst:webassets_fix}]
 @pytest.fixture
 def tmp_file():
-  with tempfile.NamedTemporaryFile(mode="wt") as f:
+  with tempfile.NamedTemporaryFile(mode="wt", delete=False) as f:
     for _ in range(100):
       f.write("\n")
     f.flush()
-    yield f.name
+    tmp_path = f.name  # store path before file is closed
+  yield tmp_path
+  os.remove(tmp_path)  # clean up after the test
\end{lstlisting}

Our fix addresses this issue by modifying line 4 to disable automatic
deletion using \CodeIn{delete=False}, storing the file path on line~9
before the file handle is closed, and manually cleaning up the
temporary file on line 11 after the test completes. This pattern
exemplifies the \textbf{Environment handling} fix strategy, where platform-specific behavior is accommodated through
careful resource management.
%
This case demonstrates how seemingly simple operations like temporary
file creation can have subtle portability implications that only
manifest under specific platform conditions, highlighting the
importance of cross-platform testing for robust software development.

\subsubsection{The hosteurope-letsencrypt Pull Request}

We demonstrate another portability fix through the hosteurope-letsencrypt project~\cite{hosteuropeletsencrypt}, which has 67 stars and involves system administration functionality. 
% 
The problem falls into the \textbf{API Availability (API)} category. 
The issue occurred where \CodeIn{os.geteuid()} was used to check for 
root privileges, but this function is unavailable on Windows, causing 
\texttt{AttributeError: module `os' has no attribute `geteuid'}.
% 
\begin{lstlisting}[caption={Cross-platform admin privilege detection fix.}, label={lst:shared_fix}]
+import platform
+def is_admin():
+    try:
+        if platform.system() == "Windows":
+            import ctypes
+            return ctypes.windll.shell32.IsUserAnAdmin() != 0
+        else:
+            return os.geteuid() == 0
+    except:
+        return False

-is_root = os.geteuid() == 0
+is_root = is_admin()
\end{lstlisting}

Our fix creates a cross-platform \CodeIn{is\_admin()} function that 
replaces the Unix-only \CodeIn{os.geteuid()} call. The fix uses 
\textbf{Portable APIs} (line 4: \CodeIn{platform.system()} for OS detection), 
\textbf{Environment handling} (lines 6, 8: conditional logic that calls 
\CodeIn{ctypes.windll.shell32.IsUserAnAdmin()} for Windows or 
\CodeIn{os.geteuid()} for Unix), and \textbf{Defensive checks} 
(lines 3--9: try-except wrapper with safe fallback).


\subsubsection{The pydantic-extra-types Pull Request}
% https://github.com/pydantic/pydantic-extra-types/pull/333

We show an example from the pydantic-extra-types
project~\cite{pydantic_extra_types}, a library that provides
additional type validators for Pydantic with 290 stars, used for
extending Pydantic's~\cite{pydantic_validation} validation
capabilities with specialized data types.
%
The problem we encountered also falls into the \textbf{File and
  Directories (FILE)} category, specifically the ``Path separators''
sub-category. The issue occurred in test fixtures where
\CodeIn{os.path.relpath} was used to create relative paths from
absolute paths. On Windows, this function fails when the source and
target paths are on different drives (e.g., \texttt{C:\textbackslash}
vs \texttt{D:\textbackslash}), as Windows cannot create relative paths
between different drive letters~\cite{python-relpath}.
%
Listing~\ref{lst:pydantic_fix} shows our fix applied to test
fixtures. The original code assumed that \CodeIn{os.path.relpath}
would always succeed, but on Windows this assumption may not hold. For
example, when tests are executed from a project directory located on
drive \texttt{D:} while the system temporary directory (used to create
fixture files) resides on drive \texttt{C:}, the function call fails
because it attempts to compute a relative path across drives. Since
the location of temporary directories on Windows is determined by
environment variables and may differ from the working
drive~\cite{python-tempfile}, this behavior can lead to path
resolution errors.


\begin{lstlisting}[float, caption={Patch for cross-drive relative path issue.}, label={lst:pydantic_fix}]
 @pytest.fixture
 def relative_file_path(absolute_file_path: Path) -> Path:
+ cwd = Path.cwd()
+ if os.name == 'nt' and absolute_file_path.anchor != cwd.anchor:
+   return absolute_file_path
  return Path(os.path.relpath(absolute_file_path, os.getcwd()))
\end{lstlisting}

Our fix addresses this Windows-specific limitation by detecting when the OS is Windows (line 4) and checking whether the absolute path and current working directory have different anchors (drive letters). When this condition is met, the function returns the absolute path instead of attempting the problematic relative path conversion (line 5). This pattern exemplifies the \textbf{Environment handling} fix strategy from our taxonomy, where platform-specific limitations are accommodated through conditional logic.

This case demonstrates how path operations that work seamlessly on Unix-like systems can encounter fundamental limitations on Windows due to the multi-drive architecture, highlighting the importance of considering platform-specific filesystem constraints in cross-platform development.

% 
% OLD:
% 
% To better understand how open-source developers and project communities react when portability issues are reported, we submitted pull requests (PRs) across different projects. The responses we received illustrate a spectrum of engagement patterns, from prompt acceptance to rejection or non-merge due to project inactivity.

% \cite{breds_pr_40}
% In one case, the proposed change addressed a test portability issue arising from platform-specific file handling. The original test used Python's \texttt{tempfile.NamedTemporaryFile} without disabling automatic deletion, which led to failures on Windows due to how temporary files are closed and reopened. Our fix stored the path before closing the file and explicitly removed it after the test execution. The maintainer accepted the contribution, commenting: \emph{``Thanks for this.''}, 
% in another PR, the maintainer expressed appreciation for the contribution and promptly merged the fix, stating: \emph{``Thanks a lot for this contribution! Good catch.''}
% \cite{python_doit_api_pr_9}. 
% These responses highlight a positive and collaborative stance, where developers value external input and promptly integrate fixes that improve cross-platform compatibility.

% \cite{frequenz_pr_1261}
%%%%%%%%%%%%%
% removed because of space

% \subsubsection{The frequenz-sdk-python Pull Request}
% % https://github.com/frequenz-floss/frequenz-sdk-python/pull/1261

% Frequenz-sdk-python is an energy management SDK for Python applications.
% The problem we identified falls into the \textbf{System Information (SYS)} category from Table~\ref{tab:categories}, specifically the ``Timezone database'' sub-category.
% We encountered a different type of response when addressing a dependency
% portability issue in the \texttt{frequenz-sdk-python} project~\cite{frequenz_sdk_python}.

% The issue occurred in the test suite where tests relied on the \CodeIn{zoneinfo} and \CodeIn{tzdata} modules for timezone operations. These modules are not consistently available across all platforms—while \CodeIn{zoneinfo} is included in Python~3.9+ on most systems, the timezone data (\CodeIn{tzdata}) may not be installed by default on some Windows configurations. According to the official documentation, the \CodeIn{zoneinfo} module depends on the system's IANA time zone database or the PyPI package \CodeIn{tzdata}, and on systems such as Windows where this database is not available, users must install \CodeIn{tzdata} explicitly~\cite{python-zoneinfo, python-tzdata}. This caused test failures when the required timezone information was unavailable, manifesting as \texttt{zoneinfo.\_common.ZoneInfoNotFoundError} (see Table~\ref{tab:symptoms}).


% Our proposed solution applied the \textbf{Defensive checks} pattern from 
% Table~\ref{tab:symptoms}: conditionally installing the \CodeIn{tzdata}
% dependency specifically for Windows environments. This approach would 
% detect the platform and provide the missing dependency as a fallback, 
% consistent with how other library dependency issues are resolved in our taxonomy.

% However, during the PR discussion, the maintainer took a different perspective.
% Rather than adding platform-specific fix to accommodate the dependency,
% they chose to remove the timezone-dependent tests altogether, eliminating the
% portability issue by removing the feature that caused it. 
% % While this falls outside our documented fix patterns, 
% % it represents a pragmatic trade-off:  prioritizing long-term maintainability and code simplicity over comprehensive cross-platform test coverage.

% This case illustrates that while our taxonomy captures common repair strategies,
% maintainers may choose architectural simplification when the cost of maintaining
% platform-specific code outweighs the value of the affected functionality. The fact
% that this approach was chosen does not invalidate our fix patterns—it simply
% shows that project-specific priorities can lead to solutions beyond tactical
% code repairs.

%%%%%%%%%%%%%

% Finally, in another project, the response was shaped less by technical considerations and more by the project's state of maintenance. The maintainer acknowledged the validity of the patch but declined to merge it, noting: \emph{``I do somewhat view that project as archived and inactive... My concern about merging is that an updated last commit date would make the project seem more active than it really is.''} This case illustrates how project activity levels and community sustainability strongly influence whether portability improvements are accepted, even if the technical changes are sound.

% Overall, these three cases demonstrate that responses to portability
% contributions vary significantly: active projects may readily adopt
% fixes, maintainers may prefer design simplifications over
% platform-specific adjustments, and inactive projects may resist
% merging altogether to avoid signaling misleading project vitality.

\begin{tcolorbox}[boxrule=0pt,sharp corners,boxsep=2pt,left=2pt,right=2pt,top=2.5pt,bottom=2pt]
  %% \textbf{RQ\textsubscript{4} (community validation):}~ We submitted \TotalPROpened{} pull requests to open-source projects, achieving a \TotalPRAprovedPorcent{} acceptance rate with zero rejections. File and Directories issues were most frequent (15 PRs), while Process and Signals issues had the highest acceptance rate (100\%). The results validate that our identified issues represent genuine portability problems recognized by maintainers, with fixes typically requiring localized changes to either test code (\TotalPRInProgramPorcent{}), application logic (\TotalPRInTestsPorcent{}), or both (\TotalPRInBothPorcent{}).
   \textbf{RQ\textsubscript{4} (community validation):}~ We submitted
   \TotalPROpened{} pull requests (PRs) to open-source projects,
   achieving a \TotalPRAprovedPercent{} acceptance rate with no rejections. Issues related
   to files and directories were the most common (15 PRs), whereas
   process and signal issues had the highest acceptance rate
   (100\%). These results confirm our initial observation that fixes
   generally involve localized changes—either to test code (\TotalPRInTestsPercent{}) or
   application logic (\TotalPRInProgramPercent{})—and suggest a positive response from
   developers to the submitted PRs.
\end{tcolorbox}


\section{Lessons Learned}
\label{sec:lessons}

This section distills key insights from our study into actionable guidance 
for developers working on cross-platform Python projects.

\textbf{Porting code remains important despite the rise of
  containerization technology.} We find that a relatively small
fraction of open-source projects (19.9\%) from our dataset include
Docker configurations. The majority of projects need to handle
portability natively in code.
%% Even projects targeting containerized production often run development
%% and testing on host operating systems, where portability issues still
%% manifest.
\emph{Recommendations.}~(1)~Maintain cross-OS CI test
matrices~\cite{github2025matrix} to catch issues early, regardless of
deployment target; (2) Fine tune code
agents~\cite{microsoft2025copilot} to detect (and fix) portability
issues as code is written.

%% When writing cross-platform Python code, prefer
%% built-in portable abstractions (\eg{}, \texttt{pathlib} over hardcoded
%% separators, \texttt{tempfile} over fixed paths).
% and use our four fix patterns (Table~\ref{tab:symptoms}) to
% address the portability sub-categories in Table~\ref{tab:symptoms}. 


\textbf{Guided LLM repair using error signatures and fix patterns.} 
LLMs underperform with generic ``fix this code'' prompts (27--43\% success) but 
achieve 50--77\% success when guided with specific error messages and corresponding 
fix patterns from Table~\ref{tab:symptoms}.
\emph{Recommendations.}~Include the specific error message and suggest the 
corresponding fix pattern in your LLM prompt. This structured approach nearly 
doubles success rates compared to generic prompts.

\textbf{Portability fixes are localized, not architectural.} 
Our four fix patterns (Table~\ref{tab:symptoms}) address issues through small, 
targeted modifications—adding conditional imports, normalizing paths, or wrapping 
platform-specific calls—rather than redesigning system architecture.
\emph{Recommendations.}~Treat portability issues as localized problems. Consult 
Table~\ref{tab:symptoms} to identify the issue type and apply the corresponding 
fix pattern rather than deferring fixes as ``too invasive''.

\textbf{File and path issues dominate cross-platform failures.} 
File and Directories is the most prevalent category, affecting 62 projects (41\% 
of all portability issues). Path separators, encoding, and file locking are the 
most common sub-categories.
\emph{Recommendations.}~Use \texttt{pathlib.Path} for path construction, explicitly 
specify \texttt{encoding} when opening text files, and use context managers 
(\texttt{with} statements) to avoid Windows file locking issues.



% OLD LESSONS LEARNED CONTENT (COMMENTED OUT):
% \Den{What should developers, researchers, and tool builders keep in mind when thinking about portability issues in Python? Here are some suggestions:}
% - Practical guidelines distilled from study
% - Recommendations for writing portable Python code
% - Recommendations for tool designers and researchers
% - General lessons beyond Python (cross-platform SE insights)

% Not in a philosophical way, but in a practical way, what are the main takeaways from our study? 

% \begin{itemize}
%   \item \textbf{Portability is not automatic.} Even though Python is designed to be cross-platform, real projects still often break when run on different operating systems. Portability depends on how code and environments are actually used, not just what the language promises.
%   \item \textbf{Problems fall into repeatable categories.} The failures we found were not random. They fall into a small set of categories (like paths, encodings, missing APIs). This means we can focus research and tool support on the most common categories instead of treating each bug as unique.
% \end{itemize}

% This section distills practical insights from our study for developers and researchers working with cross-platform Python code.



\section{Threats to Validity}
\label{sec:threats}
% - Internal validity: correctness of rerun framework
% - External validity: generalization beyond studied projects
% - Construct validity: definitions of “portability” and categorization
% - Conclusion validity: statistical soundness, possible biases

% \Fix{one thing we remember is that we do not explore other versions
% of OSs}


\noindent\textbf{Internal validity.} Our cross-OS test re-execution relies on GitHub
Actions runners, which may differ from physical machines. In addition,
the manual analysis of GitHub issues involved subjective
interpretation. Mitigation: multiple researchers independently
conducted the manual analysis, and disagreements were resolved through
consensus.

\noindent\textbf{External validity.} The 2,042 GitHub projects we
analyzed may not fully represent all software domains or
domain-specific applications. Mitigation: we sampled projects of
diverse domains and sizes. Our taxonomy is derived from fundamental
operating-system differences and thus transcends specific
domains. Furthermore, the high pull-request acceptance rate supports
the generalizability of our findings across multiple developer
communities.

\noindent\textbf{Conclusion validity.} Sample sizes varied across
research questions (500, 400, and 90 snippets), which may affect the
reliability of cross-question comparisons. Mitigation: we triangulated
results using multiple independent methods, validated the taxonomy
with both detection approaches, and obtained a \TotalPRAprovedPercent{} pull-request
acceptance rate with zero rejections.

Finally, our scope is limited to Linux, macOS, and Windows, which are
representative platforms. We did not consider portability across
hardware architectures or Python versions, for instance.

\section{Related Work}
\label{sec:related}
% - Prior work on cross-platform failures, portability in SE
% - Studies on testing across operating systems
% - Tool support for detecting environment-specific issues
% - Positioning of this work relative to literature

Our work relates to prior research on software portability and cross-platform testing, but differs in scope and methodology.

\noindent
\textbf{Empirical Studies on Software Portability.}~Ghandorh \etal{}~\cite{ghandorh2020systematic} performed a systematic literature review of metrics and methodologies for assessing software portability. 
Their work surveys existing approaches for measuring portability but does not provide empirical data on real-world portability issues or concrete solutions. 
In contrast, our study provides the first large-scale empirical characterization of Python portability problems through systematic testing and issue analysis, developing both a comprehensive taxonomy and practical repair patterns. 
While their review identifies the fragmented nature of portability research, our work addresses this gap by establishing a unified framework specifically for Python cross-platform issues.

\Den{New after review:}Wang \etal{}~\cite{wang2023compatibility} present an empirical study of compatibility issues in deep learning systems, analyzing thousands of posts to categorize common problems and their causes. 
While their findings relate to cross-environment inconsistencies, the scope is broader than portability and centered on DL ecosystems. 
Our work instead focuses directly on Python cross-platform portability, providing a targeted taxonomy and concrete repair strategies.


\noindent
\textbf{Cross-Platform Testing and Defect Studies.}~Vahabzadeh \etal{}~\cite{vahabzadeh2015empirical} conducted an empirical study on test code defects, finding that approximately 18\% of test bugs stem from environmental factors, particularly Windows vs. Unix platform differences. 
While their work acknowledges environment-related testing issues, it focuses on general test code quality and defect classification rather than systematically characterizing portability problems or developing repair strategies. 
Our work differs in three key ways: (1) we provide the first comprehensive taxonomy of Python portability issues with 7 categories and 24 sub-categories, (2) we develop systematic fix patterns that address these issues, and (3) we evaluate both detection and repair approaches using modern tools (LLMs) that were not available during their study. 
% Additionally, their work treats environmental issues as one category among many test defects, while our study specifically targets cross-platform portability as the primary research focus.

% \noindent
% \textbf{Dynamic Analysis for Portability.}~Rasley \etal{}~\cite{rasley2015detecting} introduced \CodeIn{CheckAPI}, a runtime framework for detecting latent cross-platform API violations by contrasting execution traces against platform-specific interface specifications. 
% While their approach focuses on API compatibility violations through dynamic analysis, our work addresses a broader scope of portability issues including file systems, encoding, dependencies, and environment differences. 
% Additionally, CheckAPI requires pre-defined API specifications for each platform, whereas our cross-OS test re-execution approach directly observes behavioral differences without requiring prior knowledge of platform-specific interfaces. 
% Our taxonomy also extends beyond API violations to capture the full spectrum of portability problems encountered in practice.

% SHORT VERSION::
\noindent
\textbf{Dynamic Analysis for Portability.}~Rasley \etal{}~\cite{rasley2015detecting} proposed \CodeIn{CheckAPI}, a runtime framework that detects cross-platform API violations by comparing execution traces with platform-specific specifications.
Unlike CheckAPI, which requires predefined API models and targets API-level compatibility, our work observes behavioral differences directly through cross-OS re-execution and covers broader portability issues such as file systems, encodings, dependencies, and environments.

% \noindent
% \textbf{API Specification Assumptions and Non-determinism.}~Shi \etal{}~\cite{ShiETAL16NonDex} and Gyori \etal{}~\cite{gyori2016nondex} developed NonDex, a tool for detecting erroneous assumptions about deterministic implementations of non-deterministic API specifications in Java.
% NonDex systematically explores different valid behaviors of APIs (e.g., iteration order of hash-based collections) to reveal bugs that arise from deterministic assumptions about inherently non-deterministic operations.
% While NonDex addresses non-determinism within a single platform (Java/JVM), our work focuses on deterministic differences across different operating system platforms.
% However, both approaches share the insight that software often makes implicit assumptions about platform behavior that lead to failures in different environments.
% NonDex's methodology of systematically varying platform behavior to expose hidden assumptions parallels our cross-OS testing approach, though we focus on inter-platform rather than intra-platform variability.

% Short VERSION::1:
\noindent
\textbf{API Specification Assumptions and Non-determinism.}~Shi \etal{}~\cite{ShiETAL16NonDex} and Gyori \etal{}~\cite{gyori2016nondex} introduced \textit{NonDex}, a tool that systematically explores alternative valid behaviors of Java APIs to uncover bugs caused by deterministic assumptions about inherently non-deterministic specifications (e.g., iteration order in hash-based collections).
Although NonDex focuses on intra-platform variability within the JVM, our study examines deterministic divergences that emerge across different operating systems.
Both approaches highlight how subtle, often implicit, assumptions about platform behavior can compromise software reliability under diverse execution environments.

% SHORT VERSION:2:
% \noindent
% \textbf{API Specification Assumptions and Non-determinism.}~Shi \etal{}~\cite{ShiETAL16NonDex} and Gyori \etal{}~\cite{gyori2016nondex} developed NonDex, a tool that detects bugs caused by deterministic assumptions about non-deterministic Java API behaviors (e.g., iteration order in hash-based collections).
% While NonDex explores intra-platform variability within the JVM, our work targets deterministic differences across operating systems.
% Both approaches expose how implicit assumptions about platform behavior can lead to failures in different environments.

\noindent
\textbf{OS Variability and System Reliability.}~Sun \etal{}~\cite{sun2016bear} proposed \textit{Bear}, a framework that quantifies how nondeterministic OS behaviors (e.g., scheduling and I/O timing) affect application reliability. 
They show that even minor OS-level variations can propagate to failures or performance drops, revealing hidden fragility in applications that assume predictable OS behavior. 
While Bear focuses on statistical modeling of OS nondeterminism, our work addresses deterministic portability issues stemming from platform-specific APIs, file systems, and libraries, providing concrete fix patterns through cross-platform testing.

\noindent
\textbf{Static Analysis for Python.}~Popular tools such as Ruff~\cite{ruffref}, Flake8~\cite{flake8}, Pylint~\cite{pylint}, MyPy~\cite{mypy}, Bandit~\cite{bandit}, and Pyright~\cite{pyright} focus on style, typing, and security, offering minimal support for portability detection—only Ruff includes \texttt{unspecified-encoding}~\cite{encruff}.
Specialized analyzers like Pysa~\cite{pysa2021} target security (e.g., taint and injection). Our work instead addresses runtime portability issues caused by OS variability, mostly missed by existing tools.

\Den{New after review:}
\noindent
\textbf{Performance Portability.}~Complementary to OS-level portability, prior work has examined achieving portable performance across heterogeneous hardware through data-centric intermediate representations and automatic optimizations~\cite{ziogas2021productivity}. While performance portability aims at maintaining efficiency across architectures, our study instead targets OS-level portability, a distinct space focused on functional correctness.

Al Awar \etal{} present PyKokkos~\cite{al2021performance}, a Python
framework that attains performance portability by translating Python
kernels into optimized C++ Kokkos code~\cite{kokkos}. Their approach
enables near-native execution while keeping development in
Python. Like other performance-portable systems, PyKokkos addresses
architectural efficiency,\Mar{(We need space)I don't see why you need to elaborate
  all this text (previously) if they are really unrelated to us. Can
  you write a single generic sentence saying that ``prior work focused
  on cross-OS translation for performance gains...''} whereas our work focuses on OS-level
inconsistencies across operating systems.



% \noindent
% \textbf{Static Analysis for Python.}~As described in Section~\ref{sec:llm-detection}, we examined popular Python static analysis tools including Ruff~\cite{ruffref}, Flake8~\cite{flake8}, Pylint~\cite{pylint}, MyPy~\cite{mypy}, Bandit~\cite{bandit}, and Pyright~\cite{pyright}. These tools primarily focus on code style, type correctness, security vulnerabilities, and general code quality, but provide minimal support for cross-platform portability detection. Only Ruff includes a single rule (\texttt{unspecified-encoding}) that addresses potential portability concerns. 

% Other specialized frameworks like Pysa~\cite{pysa2021} (Meta's security-focused static analyzer) demonstrate the effectiveness of static analysis for Python but target different domains (e.g., taint analysis, SQL injection). In contrast, our work addresses cross-platform compatibility issues that manifest at runtime due to OS differences and are largely undetected by existing static analysis approaches.


% \Fix{Others}: \cite{sun2016bear}, Mobile apps testing:
% \cite{boushehrinejadmoradi2015testing}\Mar{@Denini, I don't understand this. Will you leave this here? It's
  % been there for a while}
% \Den{I think this others are out of scope now. We can remove them.}

\section{Conclusions and Future Work}
\label{sec:conclusions}

This paper provides the first comprehensive characterization of
cross-OS portability issues in Python code. Across 900 projects, we found that
16.8\% exhibit portability problems: 11.2\% (56/500) via cross-OS test
re-execution and 23.8\% (95/400) via issue mining. File and Directories
operations are the most prevalent category. 
Our taxonomy shows 7 root-cause categories (with 24
sub-categories) and 4 general fix patterns. LLMs achieve 40-79\%
detection accuracy and 50-77\% repair success when guided by
structured patterns. Our findings are validated through 33 pull
requests with \TotalPRAprovedPercent{} acceptance rate.
%
Future work should develop hybrid detection tools and investigate
fine-tune code agents for portability problems.

\vspace{0.7ex}
\noindent\textbf{Data Availability.}
The artifacts --including datasets, submitted PRs and scripts-- are
publicly available: \ourrepo

\begin{acks}
\label{sec:acks}
This work is partially supported by the United States National Science
Foundation (NSF) under Grant Nos. CCF-2349961 and CCF-2319472. Denini
was supported by a FACEPE fellowship.
%% We thank Google
%% for the Google Cloud Platform credits. We also thank the anonymous reviewers for
%% their valuable feedback.
\end{acks}
%% \sectio

%\section*{Acknowledgments}
%We thank the anonymous reviewers for their valuable feedback. This work was supported in part by ... \Den{To be filled}

\onecolumn \begin{multicols}{2}
%\balance
\bibliographystyle{ACM-Reference-Format}
%% \bibliography{references}
\bibliography{ref}
\end{multicols}

\end{document}
